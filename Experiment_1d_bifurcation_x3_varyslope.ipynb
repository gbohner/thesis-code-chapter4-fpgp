{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from GPDM_direct_fixedpoints.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/nhome/live/gbohner/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning:\n",
      "\n",
      "Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nbimporter\n",
    "\n",
    "# Import main functionality\n",
    "from GPDM_direct_fixedpoints import *\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "# Saving outputs and timing\n",
    "import pickle, datetime, time\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import itertools\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "# %run GPDM_direct_fixedpoints.ipynb\n",
    "# %run GPDM_Examples.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental setup\n",
    "\n",
    "1. We first generate training and test data with the same parameters.\n",
    "\n",
    "2. We then fit GP models with a fixed number of parameters, but vary the number of fixed points versus basic inducing points\n",
    "\n",
    "3. We compare the performance of the GP models against one-another as well as against linear autoregression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bifurc_trans_func(x, slope):\n",
    "    return slope*x - x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bifurc_draw_trial(T, mu_0_0, Sigma_0_0, slope, maxval,Sigma_eps, Sigma_nu, **kwargs):\n",
    "    x0 = mu_0_0 + np.sqrt(Sigma_0_0)*np.random.randn(1)\n",
    "    x = np.zeros((1,T))\n",
    "    y = np.zeros((1,T))\n",
    "    for t in range(T):\n",
    "        if t==0:\n",
    "            xprev = x0\n",
    "        else:\n",
    "            xprev = x[:,t-1]\n",
    "\n",
    "        x[:,t] =  (bifurc_trans_func(xprev, slope)\n",
    "                    + np.sqrt(Sigma_eps)*np.random.randn(1))\n",
    "        \n",
    "        y[:,t] = x[:,t] + np.sqrt(Sigma_nu)*np.random.randn(1)\n",
    "        \n",
    "    return (x,y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bifurc_params = OrderedDict()\n",
    "bifurc_params[\"rseed\"] = 1234 # Happens to creates balanced training set in the first 8 trials in y\n",
    "bifurc_params[\"Ny\"] = 500\n",
    "bifurc_params[\"T\"] = 20\n",
    "bifurc_params[\"slope\"] = 1.15\n",
    "bifurc_params['maxval'] = 1.0\n",
    "bifurc_params[\"Sigma_eps\"] = 0.2**2\n",
    "bifurc_params[\"Sigma_nu\"] = 0.05**2\n",
    "bifurc_params[\"mu_0_0\"] = 0.\n",
    "bifurc_params[\"Sigma_0_0\"] = 1e-6\n",
    "\n",
    "  \n",
    "def bifurc_create_data(bifurc_params):\n",
    "    # Generate trials and collect them into [D x T x N] array\n",
    "    np.random.seed(bifurc_params[\"rseed\"])\n",
    "    all_trials_x = []\n",
    "    all_trials_y = []\n",
    "    for n in range(bifurc_params[\"Ny\"]):\n",
    "        x,y = bifurc_draw_trial(**bifurc_params)\n",
    "\n",
    "        all_trials_x.append(x[:,:,None])\n",
    "        all_trials_y.append(y[:,:,None])\n",
    "\n",
    "    x = np.concatenate(all_trials_x, axis=2)\n",
    "    y = np.concatenate(all_trials_y, axis=2)\n",
    "\n",
    "#     # Swap around trials to ensure balanced training sets\n",
    "#     if bifurc_params[\"slope\"]*bifurc_params['maxval'] > 1.1:\n",
    "#         cur_trial = 0\n",
    "#         cur_trial_type = 1.\n",
    "#         while cur_trial < 0.8*bifurc_params[\"Ny\"]:\n",
    "#             # If current trial is not correct type, switch it with one that is\n",
    "#             if not (np.mean(y[:,(bifurc_params[\"T\"]-4):,cur_trial]) * cur_trial_type > 0.2*bifurc_params['maxval']):\n",
    "#                 switch_trial = cur_trial+1\n",
    "#                 while (not (np.mean(y[:,(bifurc_params[\"T\"]-4):,switch_trial]) * cur_trial_type > 0.2*bifurc_params['maxval'])):\n",
    "#                     switch_trial += 2\n",
    "\n",
    "#                 tmp = np.copy(y[:,:,switch_trial])\n",
    "#                 y[:,:,switch_trial] = np.copy(y[:,:,cur_trial])\n",
    "#                 y[:,:,cur_trial] = tmp\n",
    "\n",
    "#                 tmp = np.copy(x[:,:,switch_trial])\n",
    "#                 x[:,:,switch_trial] = np.copy(x[:,:,cur_trial])\n",
    "#                 x[:,:,cur_trial] = tmp\n",
    "\n",
    "#             #print np.mean(y[:,(bifurc_params[\"T\"]-4):,cur_trial]) * cur_trial_type\n",
    "#             cur_trial += 1\n",
    "#             cur_trial_type *= -1.\n",
    "    \n",
    "    return [x, y]\n",
    "\n",
    "[x, y] = bifurc_create_data(bifurc_params)\n",
    "\n",
    "plots_by_run = []\n",
    "for v in range(128):\n",
    "    plots_by_run.append(\n",
    "        plt_type.Scatter(x=np.squeeze(np.arange(bifurc_params[\"T\"])), \n",
    "                      y=np.squeeze(y[:,:,v]), \n",
    "                      mode='lines')\n",
    "    )\n",
    "    \n",
    "    \n",
    "print rmse(pred_lin_AR1(y[:,:,200:], y[:,:,:50], cutoff=None), y[:,:,200:])\n",
    "    \n",
    "    \n",
    "xstar = np.atleast_2d(np.arange(-2.5,2.5,0.05))\n",
    "true_tr_vals = bifurc_trans_func(xstar, bifurc_params['slope'])\n",
    "\n",
    "#set_trace()\n",
    "\n",
    "plt([plt_type.Scatter(x=np.squeeze(xstar), \n",
    "                  y=np.squeeze(true_tr_vals), mode='markers', name = 'True trans. f.',\n",
    "                  marker=dict(color='orange')),\n",
    " plt_type.Scatter(x=np.squeeze(xstar), \n",
    "                  y=np.squeeze(true_tr_vals+2.*np.sqrt(bifurc_params[\"Sigma_eps\"])), mode='markers', name = 'True trans. f.',\n",
    "                  marker=dict(color='orange', size=2)),\n",
    " plt_type.Scatter(x=np.squeeze(xstar), \n",
    "                  y=np.squeeze(true_tr_vals-2.*np.sqrt(bifurc_params[\"Sigma_eps\"])), mode='markers', name = 'True trans. f.',\n",
    "                  marker=dict(color='orange', size=2)),\n",
    "plt_type.Scatter(x=np.squeeze(xstar), \n",
    "                  y=np.squeeze(xstar), mode='markers', name = 'x=x',\n",
    "                  marker=dict(color='blue'))\n",
    "     ])\n",
    "\n",
    "plt(plots_by_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bifurc_callback_plot_external(pvec_partial, \n",
    "                                  opt_params, init_paramvec, transforms, dict_ind, dict_shape,\n",
    "                                  bifurc_params, trainingData=None\n",
    "                                 ):\n",
    "    \n",
    "    paramvec = replace_params(pvec_partial, opt_params, init_paramvec)\n",
    "    paramdict = vec_to_params(paramvec, dict_ind, dict_shape, transforms)\n",
    "       \n",
    "    # Unpack the usual parameters\n",
    "    (Sigma_eps, mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_u, lengthscales, kernel_variance, s, J)  = \\\n",
    "        paramdict.values()[:12]\n",
    "    \n",
    "    if np.any(np.isnan(lengthscales)):\n",
    "        set_trace()\n",
    "    \n",
    "    # Deal with the extra possible parameters\n",
    "    Sigma_s = None; Sigma_J=None;\n",
    "    if 'Sigma_s' in paramdict.keys():\n",
    "        Sigma_s = paramdict['Sigma_s']\n",
    "    if 'Sigma_J' in paramdict.keys():\n",
    "        Sigma_J = paramdict['Sigma_J']\n",
    "        \n",
    "    # Plot transition function\n",
    "    xstar = np.atleast_2d(np.arange(np.min(z)-0.5,np.max(z)+0.5,0.05))\n",
    "    \n",
    "    \n",
    "    # Debugging stuff\n",
    "#     z=z[:,:2]\n",
    "#     u=u[:,:2]\n",
    "#     Sigma_u=Sigma_u[:2,:]\n",
    "#     print Sigma_J\n",
    "    \n",
    "    L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=lengthscales, z=z, u=u, s=s, J=J, \n",
    "                                         sig_eps=Sigma_eps, sig_u=Sigma_u, sig_s=Sigma_s, sig_J = Sigma_J)\n",
    "    mu_star, sig_star, K_pred = fp_predict(xstar, L, targets, params)\n",
    "\n",
    "    # print(time_full_iter(pvec, y, dict_ind, dict_shape)[0])\n",
    "        \n",
    "    # Get true function values\n",
    "    true_tr_vals = bifurc_trans_func(xstar, bifurc_params['slope'])\n",
    "    \n",
    "    #set_trace()\n",
    "    \n",
    "    # Map Sigma_s values to the range 8-20\n",
    "    if Sigma_s.size>1:\n",
    "        FP_SIZE = -np.squeeze(np.log(Sigma_s))\n",
    "        FP_SIZE = FP_SIZE - np.min(FP_SIZE)\n",
    "        FP_SIZE = (FP_SIZE/np.max(FP_SIZE))*12. + 8.\n",
    "    else:\n",
    "        FP_SIZE = 10.\n",
    "    \n",
    "    pltArray= [plt_type.Scatter(x=np.squeeze(xstar), y=np.squeeze(mu_star), mode='markers', name='GP post. mean',\n",
    "                         marker=dict(color='blue')),\n",
    "         plt_type.Scatter(x=np.squeeze(xstar), y=np.squeeze(mu_star)+2.*np.squeeze(np.sqrt(sig_star)), mode='markers', \n",
    "                         marker=dict(size=2, color='blue')),\n",
    "         plt_type.Scatter(x=np.squeeze(xstar), y=np.squeeze(mu_star)-2.*np.squeeze(np.sqrt(sig_star)), mode='markers', \n",
    "                         marker=dict(size=2, color='blue')),      \n",
    "         plt_type.Scatter(x=np.squeeze(xstar), \n",
    "                          y=np.squeeze(true_tr_vals), mode='markers', name = 'True trans. f.',\n",
    "                          marker=dict(color='orange')),\n",
    "         plt_type.Scatter(x=np.squeeze(xstar), \n",
    "                          y=np.squeeze(true_tr_vals+2.*np.sqrt(bifurc_params[\"Sigma_eps\"])), mode='markers', name = 'True trans. f.',\n",
    "                          marker=dict(color='orange', size=2)),\n",
    "         plt_type.Scatter(x=np.squeeze(xstar), \n",
    "                          y=np.squeeze(true_tr_vals-2.*np.sqrt(bifurc_params[\"Sigma_eps\"])), mode='markers', name = 'True trans. f.',\n",
    "                          marker=dict(color='orange', size=2)),\n",
    "         plt_type.Scatter(x=np.squeeze(z), y=np.squeeze(-2.0*np.ones_like(z)), mode='markers', marker=dict(size=10),\n",
    "                         name = 'Ind point loc'),\n",
    "         plt_type.Scatter(x=np.squeeze(z), y=np.squeeze(u), mode='markers', marker=dict(size=10),\n",
    "                         name = 'Ind point val'),\n",
    "         plt_type.Scatter(x=np.atleast_1d(np.squeeze(s)), y=np.atleast_1d(np.squeeze(s)), mode='markers', marker=dict(size=FP_SIZE),\n",
    "                         name = 'Fixed point')\n",
    "        ]\n",
    "    \n",
    "    if trainingData is not None:\n",
    "        trainingData_t = np.reshape(trainingData[:,1:,:],(1,-1))\n",
    "        trainingData_t1 = np.reshape(trainingData[:,:-1,:],(1,-1))\n",
    "        pltArray.append(plt_type.Scatter(x=np.squeeze(trainingData_t1), y=np.squeeze(trainingData_t),\n",
    "                                        mode='markers', marker=dict(size=4, color='black'),\n",
    "                                         name='Training data'\n",
    "                                        ))\n",
    "    \n",
    "    plt(pltArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting GP models to varying slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bifurc_params = OrderedDict()\n",
    "bifurc_params[\"rseed\"] = 1234 # Happens to creates balanced training set in the first 8 trials in y\n",
    "bifurc_params[\"Ny\"] = 500\n",
    "bifurc_params[\"T\"] = 20\n",
    "bifurc_params[\"slope\"] = 0.25\n",
    "bifurc_params['maxval'] = 1.0\n",
    "bifurc_params[\"Sigma_eps\"] = 0.2**2\n",
    "#bifurc_params[\"Sigma_eps\"] = 0.1**2\n",
    "bifurc_params[\"Sigma_nu\"] = 0.05**2\n",
    "bifurc_params[\"mu_0_0\"] = 0.\n",
    "bifurc_params[\"Sigma_0_0\"] = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_GP(bifurc_params, num_fixed_points, num_trials, batchnum, start_time, callback_plot=False, return_init=False):\n",
    "    # Set number of parameters\n",
    "    D = 1\n",
    "    Ns = num_fixed_points\n",
    "    Nz = 16\n",
    "    \n",
    "    [x,y] = bifurc_create_data(bifurc_params)\n",
    "\n",
    "    # Fix the training data\n",
    "    y_train = y[:,:,batchnum*num_trials:(batchnum*num_trials+num_trials)]\n",
    "    x_train = x[:,:,batchnum*num_trials:(batchnum*num_trials+num_trials)]\n",
    "\n",
    "    np.random.seed(bifurc_params['rseed']*2)\n",
    "\n",
    "    #######################################################\n",
    "    # Initialise the parameters\n",
    "    paramdict = init_params(y_train, D, Nz, Ns, grad_sigma=True)\n",
    "\n",
    "    # Fix noise to true value\n",
    "    paramdict['Sigma_nu'] = bifurc_params['Sigma_nu'] * np.ones_like(paramdict['Sigma_nu'])\n",
    "\n",
    "#     # Re-initialise / add some more parameters\n",
    "#     paramdict['Sigma_s'] = 1e-2*np.ones((Ns,1))\n",
    "#     paramdict['Sigma_J'] = 1e-2*np.ones((Ns*D,1))\n",
    "    \n",
    "    # Add transformations for certain parameters \n",
    "    # (Note that the parameter indices may be changed by transforms! (for cholesky repres of matrices, \"SquareMatrix\" type))\n",
    "    transforms = OrderedDict()\n",
    "    for par in ['Sigma_0_0', 'Sigma_u', 'Sigma_s', 'Sigma_J', 'lengthscales', 'Sigma_eps', 'Sigma_nu', 'kernel_variance']:\n",
    "        transforms[par] = {}\n",
    "        transforms[par]['type'] = \"Square\"\n",
    "    \n",
    "    # Create vectorised and transformed representation\n",
    "    (init_paramvec, dict_ind, dict_shape) = params_to_vec(paramdict, transforms=transforms)\n",
    " \n",
    "    #######################################################\n",
    "    # Optimise only certain elements of paramvec (messy with indices)\n",
    "    opt_params = np.arange(init_paramvec.shape[0])\n",
    "    # opt_params = np.delete(opt_params, np.hstack([dict_ind['C'], dict_ind['Sigma_nu'], dict_ind['J']])) # All except the ones listed here\n",
    "    opt_params = np.delete(opt_params, np.hstack([dict_ind['C'], dict_ind['Sigma_nu']])) # All except the ones listed here\n",
    "    cur_pvec = init_paramvec[opt_params]\n",
    "\n",
    "    #######################################################\n",
    "    # Add bounds for parameters \n",
    "    bnds = list(((None, None),) * init_paramvec.shape[0])\n",
    "    # cur_dim = 0\n",
    "    # cur_z = 0\n",
    "    # cur_tot = 0\n",
    "    # z_mins = np.min(z, axis=1)\n",
    "    # z_maxs = np.max(z, axis=1)\n",
    "    # for i in np.concatenate([dict_ind['z'], dict_ind['s']]): # Note the idiotic python reshape order for setting bounds per dim\n",
    "    #     z_min = z_mins[cur_dim]\n",
    "    #     z_max = z_maxs[cur_dim]\n",
    "    #     bnds[i] = (z_min-0.05*(z_max-z_min), z_max+0.05*(z_max-z_min))\n",
    "    #     cur_z = cur_z+1\n",
    "    #     if cur_tot < D*Nz:\n",
    "    #         cur_z = np.mod(cur_z, Nz)\n",
    "    #     else:\n",
    "    #         cur_z = np.mod(cur_z, Ns)\n",
    "    #     cur_tot = cur_tot+1\n",
    "    #     if cur_z==0:\n",
    "    #         cur_dim = cur_dim+1\n",
    "    #     if cur_tot==D*Nz:\n",
    "    #         cur_dim = 0\n",
    "    # for i in np.concatenate([dict_ind['J']]):\n",
    "    #     bnds[i] = (-1., 1.)\n",
    "    bnds_final = []\n",
    "    for i in opt_params:\n",
    "        bnds_final.append(bnds[i])\n",
    "    bnds = tuple(bnds_final)\n",
    "\n",
    "    #######################################################\n",
    "    # Add priors (to span at least the bounds)\n",
    "    priors = []\n",
    "    \n",
    "    # Add prior to ensure inducing points AND fixed points are smooth\n",
    "    cur_prior = {}\n",
    "    cur_prior['type'] = \"InducingSmooth_and_DPP\"\n",
    "    cur_prior['metadata'] = {}\n",
    "    def unpack_dict_tmp(pdict):\n",
    "        kernelparams = {'lengthscales': pdict['lengthscales'], 'kernel_variance': pdict['kernel_variance']}\n",
    "        # Return the parameters we want in the required format (joint smoothness of (z-u) and s)\n",
    "        return (np.concatenate([pdict['u'], pdict['s']],axis=1), \n",
    "            np.concatenate([pdict['z'], pdict['s']],axis=1),\n",
    "            np.concatenate([pdict['Sigma_u'], pdict['Sigma_s']]),\n",
    "            kernelparams)\n",
    "    cur_prior['metadata']['unpack_dict'] = unpack_dict_tmp\n",
    "    cur_prior['metadata']['kernel_func'] = RBF\n",
    "    cur_prior['metadata']['prior_weight_Smooth'] = 1e0\n",
    "    cur_prior['metadata']['prior_weight_DPP'] = 1e0\n",
    "    priors.append(cur_prior)\n",
    "    \n",
    "    \n",
    "#     # Add prior to ensure inducing points are smooth\n",
    "#     cur_prior = {}\n",
    "#     cur_prior['type'] = \"InducingSmooth_and_DPP\"\n",
    "#     cur_prior['metadata'] = {}\n",
    "#     def unpack_dict_tmp(pdict):\n",
    "#         kernelparams = {'lengthscales': pdict['lengthscales'], 'kernel_variance': pdict['kernel_variance']}\n",
    "#         # Return the parameters we want in the required format (joint smoothness of (z-u) and s)\n",
    "#         return (pdict['u'], \n",
    "#                 pdict['z'],\n",
    "#                 pdict['Sigma_u'],\n",
    "#                 kernelparams)\n",
    "#     cur_prior['metadata']['unpack_dict'] = unpack_dict_tmp\n",
    "#     cur_prior['metadata']['kernel_func'] = RBF\n",
    "#     cur_prior['metadata']['prior_weight_Smooth'] = 1e0\n",
    "#     cur_prior['metadata']['prior_weight_DPP'] = 1e0\n",
    "#     priors.append(cur_prior)\n",
    "    \n",
    "#     # Add prior to ensure fixed points are smooth\n",
    "#     cur_prior = {}\n",
    "#     cur_prior['type'] = \"InducingSmooth_and_DPP\"\n",
    "#     cur_prior['metadata'] = {}\n",
    "#     def unpack_dict_tmp(pdict):\n",
    "#         kernelparams = {'lengthscales': pdict['lengthscales'], 'kernel_variance': pdict['kernel_variance']}\n",
    "#         # Return the parameters we want in the required format (joint smoothness of (z-u) and s)\n",
    "#         return (pdict['s'], \n",
    "#                 pdict['s'],\n",
    "#                 pdict['Sigma_s'],\n",
    "#                 kernelparams)\n",
    "#     cur_prior['metadata']['unpack_dict'] = unpack_dict_tmp\n",
    "#     cur_prior['metadata']['kernel_func'] = RBF\n",
    "#     cur_prior['metadata']['prior_weight_Smooth'] = 1e0\n",
    "#     cur_prior['metadata']['prior_weight_DPP'] = 1e0\n",
    "#     priors.append(cur_prior)\n",
    "\n",
    "    # # Add a strong prior to learn actual fixed points\n",
    "    # logGamma_prior = create_prior(\"LogGamma\", [2., 0.5, -6.])\n",
    "    # for i in np.concatenate([dict_ind['Sigma_s'], dict_ind['Sigma_J']]):\n",
    "    #     prior_funcs[i] = logGamma_prior\n",
    "\n",
    "    # tmp_x = np.logspace(-6.0,2,100)    \n",
    "    # plt(plt_type.Figure(data=[plt_type.Scatter(x=tmp_x, y=np.exp(-logGamma_prior(tmp_x)))], layout=plt_type.Layout(xaxis=dict(type= \"log\"))))\n",
    "    # plt(plt_type.Figure(data=[plt_type.Scatter(x=tmp_x, y=logGamma_prior(tmp_x))], layout=plt_type.Layout(xaxis=dict(type= \"log\"))))\n",
    "\n",
    "    if return_init:\n",
    "        return [cur_pvec, opt_params, init_paramvec, transforms, dict_ind, dict_shape]\n",
    "    \n",
    "    #######################################################\n",
    "#     # Prepare the optimisation\n",
    "#     tmp_func = lambda pvec_partial: (time_full_iter(replace_params(pvec_partial, opt_params, init_paramvec), \n",
    "#                                                 y_train, dict_ind, dict_shape, \n",
    "#                                                 transforms=transforms,\n",
    "#                                                 priors=priors)[0])\n",
    "#     objective_with_grad = value_and_grad(tmp_func, argnum=0)\n",
    "\n",
    "    # Prepare the optimisation\n",
    "    f_objective = lambda pvec_partial: (time_full_iter(replace_params(pvec_partial, opt_params, init_paramvec), \n",
    "                                                y_train, dict_ind, dict_shape, \n",
    "                                                transforms=transforms,\n",
    "                                                priors=priors)[0])\n",
    "    f_minibatch = lambda pvec_partial: (minibatch_iter(replace_params(pvec_partial, opt_params, init_paramvec), \n",
    "                                                y_train, dict_ind, dict_shape, \n",
    "                                                transforms=transforms,\n",
    "                                                priors=priors,\n",
    "                                                minibatch_size=4)[0])\n",
    "    f_minibatch_with_grad = grad(f_minibatch, argnum=0)\n",
    "\n",
    "\n",
    "\n",
    "    # By iterating minimize within a for cycle, we can save all intermediate results and set ending times\n",
    "    expr_fname_params = \"_varyslope_%0.3d\" % (np.int32(bifurc_params[\"slope\"]*100))\n",
    "    save_fname_params = \"_%0.2d_fix_%0.3d_trials_batch_%0.2d\" % (num_fixed_points, num_trials, batchnum)\n",
    "    save_fname = \"Experiment_1d_bifurc_results/bifurc_1d_\" + start_time + save_fname_params + expr_fname_params + \".pkl\"\n",
    "    init_time = time.time()\n",
    "    max_time = 6.0*3600 # Maximum iteration time in seconds, break if reached\n",
    "    all_results = []\n",
    "\n",
    "    for it in range(1):\n",
    "        np.random.seed(1234+it)\n",
    "        result = adamOptimize(f_objective, f_minibatch_with_grad, cur_pvec,\n",
    "                          options={'maxiter':500, 'disp':True})\n",
    "#     for it in range(50):\n",
    "#         result = scipy.optimize.minimize(objective_with_grad, cur_pvec, jac=True, method='L-BFGS-B', bounds=bnds, callback=None,\n",
    "#                               options={'maxiter':50, 'disp':True})\n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save the results\n",
    "        with open(save_fname, 'wb') as f:\n",
    "            pickle.dump([y_train, x_train, bifurc_params,\n",
    "                         all_results, \n",
    "                         init_paramvec, dict_ind, dict_shape, opt_params, \n",
    "                         bnds, transforms], f)\n",
    "        cur_pvec = result.x\n",
    "        cur_time = time.time()\n",
    "        \n",
    "        if callback_plot:\n",
    "            print([it, cur_time - init_time, result.fun])\n",
    "            bifurc_callback_plot_external(cur_pvec, \n",
    "                                  opt_params, init_paramvec, transforms, dict_ind, dict_shape,\n",
    "                                  bifurc_params\n",
    "                                 )\n",
    "        \n",
    "        \n",
    "        # Exit if maximum time is reached\n",
    "\n",
    "        if ((cur_time - init_time) > max_time):\n",
    "            print([\"Maximum iteration time reached at iter\", it])\n",
    "            break\n",
    "\n",
    "        if len(all_results)>=2:\n",
    "            if (all_results[-1].fun - all_results[-2].fun) >= (-1e-2*num_trials):\n",
    "                print([\"Update did not improve objective function, stopping\"])\n",
    "                break\n",
    "                \n",
    "#     return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bifurc_params['slope'] = 2.0\n",
    "# all_results = fit_GP(bifurc_params,5,16,0,start_time=datetime.datetime.now().strftime(\"%Y%m%dT%H%M%S\"), callback_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "\n",
    "# Noise level: eps=0.2^2\n",
    "nbatches = 1\n",
    "slope_set = np.array([0.5, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.15, 1.2, 1.25, 1.3, 1.35, 1.4, 1.45, 1.5, 1.75, 2.0, 2.25])\n",
    "ntrials_set = np.array([32])\n",
    "nfix_set = np.array([5])\n",
    "\n",
    "bifurc_params[\"Sigma_eps\"] = 0.2**2\n",
    "def fit_GP_varyslope(bifurc_params, slope, *args):\n",
    "    bifurc_params['slope'] = slope\n",
    "    fit_GP(bifurc_params, *args)\n",
    "    \n",
    "\n",
    "Parallel(n_jobs=num_cores, verbose=5)(\n",
    "    delayed(fit_GP_varyslope)(bifurc_params, slope, 5, 32, batchnum, start_time) for batchnum, slope  in itertools.product(\n",
    "        range(nbatches), slope_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Create second batches\n",
    "\n",
    "# # Noise level: eps=0.2^2\n",
    "# start_time='20180206T230638'\n",
    "# nbatches = 2\n",
    "# slope_set = np.array([0.5, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.15, 1.2, 1.25, 1.3, 1.35, 1.4, 1.45, 1.5, 1.75, 2.0, 2.25])\n",
    "# ntrials_set = np.array([32])\n",
    "# nfix_set = np.array([5])\n",
    "\n",
    "# bifurc_params[\"Sigma_eps\"] = 0.2**2\n",
    "# def fit_GP_varyslope(bifurc_params, slope, *args):\n",
    "#     bifurc_params['slope'] = slope\n",
    "#     fit_GP(bifurc_params, *args)\n",
    "    \n",
    "\n",
    "# Parallel(n_jobs=num_cores, verbose=5)(\n",
    "#     delayed(fit_GP_varyslope)(bifurc_params, slope, 5, 32, batchnum, start_time) for batchnum, slope  in itertools.product(\n",
    "#         range(1,nbatches), slope_set)\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # Noise level: eps=0.1^2\n",
    "# start_time='20180207T022517'\n",
    "# nbatches = 2\n",
    "# slope_set = np.array([0.5, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.15, 1.2, 1.25, 1.3, 1.35, 1.4, 1.45, 1.5, 1.75, 2.0, 2.25])\n",
    "# ntrials_set = np.array([32])\n",
    "# nfix_set = np.array([5])\n",
    "# bifurc_params[\"Sigma_eps\"] = 0.1**2\n",
    "\n",
    "# def fit_GP_varyslope(bifurc_params, slope, *args):\n",
    "#     bifurc_params['slope'] = slope\n",
    "#     fit_GP(bifurc_params, *args)\n",
    "    \n",
    "\n",
    "# Parallel(n_jobs=num_cores, verbose=5)(\n",
    "#     delayed(fit_GP_varyslope)(bifurc_params, slope, 5, 32, batchnum, start_time) for batchnum, slope  in itertools.product(\n",
    "#         range(1,nbatches), slope_set)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [cur_pvec, opt_params, init_paramvec, transforms, dict_ind, dict_shape] = (\n",
    "#     fit_GP(4,6,3,start_time=datetime.datetime.now().strftime(\"%Y%m%dT%H%M%S\"), return_init=True))\n",
    "\n",
    "# bifurc_callback_plot_external(cur_pvec, \n",
    "#                                   opt_params, init_paramvec, transforms, dict_ind, dict_shape,\n",
    "#                                   bifurc_params\n",
    "#                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start_time = datetime.datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "\n",
    "# nbatches = 6\n",
    "# ntrials_set = np.array([2, 4, 8, 16, 32])\n",
    "# nfix_set = np.array([0,1,2,3,4,5])\n",
    "\n",
    "# Parallel(n_jobs=num_cores, verbose=5)(\n",
    "#     delayed(fit_GP)(nfix, ntrials, batchnum, start_time) for batchnum, ntrials, nfix  in itertools.product(range(nbatches), ntrials_set, nfix_set,)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load and compare fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Noise level: eps=0.2^2, joint prior\n",
    "start_time='20180208T082207'\n",
    "nbatches = 2\n",
    "slope_set = np.array([0.5, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.15, 1.2, 1.25, 1.3, 1.35, 1.4, 1.45, 1.5, 1.75, 2.0, 2.25])\n",
    "ntrials_set = np.array([32])\n",
    "nfix_set = np.array([5])\n",
    "\n",
    "\n",
    "\n",
    "# # Noise level: eps=0.2^2\n",
    "# start_time='20180206T230638'\n",
    "# nbatches = 2\n",
    "# slope_set = np.array([0.5, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.15, 1.2, 1.25, 1.3, 1.35, 1.4, 1.45, 1.5, 1.75, 2.0, 2.25])\n",
    "# ntrials_set = np.array([32])\n",
    "# nfix_set = np.array([5])\n",
    "\n",
    "\n",
    "# # Noise level: eps=0.1^2\n",
    "# start_time='20180207T022517'\n",
    "# nbatches = 2\n",
    "# slope_set = np.array([0.5, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.15, 1.2, 1.25, 1.3, 1.35, 1.4, 1.45, 1.5, 1.75, 2.0, 2.25])\n",
    "# ntrials_set = np.array([32])\n",
    "# nfix_set = np.array([5])\n",
    "\n",
    "\n",
    "# start_time='20180206T170518'\n",
    "# nbatches = 1\n",
    "# slope_set = np.array([0.25, 0.5, 0.75, 1.0, 1.05, 1.1, 1.15, 1.2, 1.25, 1.3, 1.35, 1.4, 1.45, 1.5, 2.0, 2.25])\n",
    "# ntrials_set = np.array([16])\n",
    "# nfix_set = np.array([5])\n",
    "\n",
    "# # SGD - Slope: 2.0\n",
    "# start_time = '20180205T170604'\n",
    "# nbatches = 6\n",
    "# ntrials_set = np.array([2, 4, 8, 16, 32])\n",
    "# nfix_set = np.array([0,1,2,3,4,5])\n",
    "\n",
    "# # SGD - Slope: 2.0\n",
    "# start_time = '20180205T105514'\n",
    "# nbatches = 6\n",
    "# ntrials_set = np.array([4, 8, 16, 32])\n",
    "# nfix_set = np.array([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the saved files\n",
    "def load_GP_fit(slope_ind, ntrials_ind, nfix_ind, batchnum_ind, slope, ntrials, nfix, batchnum, start_time):\n",
    "    expr_fname_params = \"_varyslope_%0.3d\" % (np.int32(slope*100))\n",
    "    load_fname_params = \"_%0.2d_fix_%0.3d_trials_batch_%0.2d\" % (nfix, ntrials, batchnum)\n",
    "    load_fname = \"Experiment_1d_bifurc_results/bifurc_1d_\" + start_time + load_fname_params + expr_fname_params + \".pkl\"\n",
    "#     load_fname_params = \"_%0.2d_fix_%0.3d_trials_batch_%0.2d\" % (nfix, ntrials, batchnum)\n",
    "#     load_fname = \"Experiment_1d_bifurc_results/bifurc_1d_\" + start_time + load_fname_params + \".pkl\"\n",
    "    # load_fname_params = \"_%2d_fix_%3d_trials\" % (nfix, ntrials)\n",
    "    # load_fname = \"Experiment_1d_wells_results/well_1d_k2_\" + start_time + load_fname_params + \".pkl\"\n",
    "\n",
    "    try:\n",
    "        # Load and store results\n",
    "        results_file = pickle.load(open(load_fname, 'r'))\n",
    "    except:\n",
    "        results_file = []\n",
    "    \n",
    "    return results_file\n",
    "\n",
    "GP_fit_saves = Parallel(n_jobs=num_cores)(\n",
    "    delayed(load_GP_fit)(slope_ind, ntrials_ind, nfix_ind, batchnum_ind, slope, ntrials, nfix, batchnum, start_time)\n",
    "    for (slope_ind, ntrials_ind, nfix_ind, batchnum_ind), (slope, ntrials, nfix, batchnum) in (\n",
    "        itertools.izip(\n",
    "            itertools.product(range(len(slope_set)), range(len(ntrials_set)), range(len(nfix_set)), range(nbatches)),\n",
    "            itertools.product(slope_set, ntrials_set, nfix_set, range(nbatches))\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nestedlist_to_array(nested_list, max_level = 0):\n",
    "    cur_nested_list = nested_list\n",
    "    level = 0\n",
    "    array_dims = []\n",
    "    array_dims_range = []\n",
    "    while (level <= max_level) and (type(cur_nested_list)==list):\n",
    "        array_dims.append(len(cur_nested_list))\n",
    "        array_dims_range.append(range(len(cur_nested_list)))\n",
    "        cur_nested_list = cur_nested_list[0]\n",
    "        level += 1\n",
    "\n",
    "    out = np.empty(tuple(array_dims), dtype=object)\n",
    "    \n",
    "    for inds in itertools.product(*array_dims_range):\n",
    "        cur_nested_list = nested_list\n",
    "        for i in inds:\n",
    "            cur_nested_list = cur_nested_list[i]\n",
    "        out[inds] = cur_nested_list\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GP_fit_saves_rs = np.reshape(nestedlist_to_array(GP_fit_saves, max_level=0), \n",
    "                             (len(slope_set), len(ntrials_set), len(nfix_set), nbatches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fp_dynamic_threshold(curFit):\n",
    "    [y_train, x_train, bifurc_params,\n",
    "     all_results, \n",
    "     init_paramvec, dict_ind, dict_shape, opt_params, \n",
    "     bnds, transforms] = curFit\n",
    "\n",
    "    paramvec = replace_params(all_results[-1].x, opt_params, init_paramvec)\n",
    "    paramdict = vec_to_params(paramvec, dict_ind, dict_shape, transforms)\n",
    "\n",
    "    # Unpack the usual parameters\n",
    "    (Sigma_eps, mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_u, lengthscales, kernel_variance, s, J)  = \\\n",
    "        paramdict.values()[:12]\n",
    "\n",
    "    if np.any(np.isnan(lengthscales)):\n",
    "        set_trace()\n",
    "\n",
    "    # Deal with the extra possible parameters\n",
    "    Sigma_s = None; Sigma_J=None;\n",
    "    if 'Sigma_s' in paramdict.keys():\n",
    "        Sigma_s = paramdict['Sigma_s']\n",
    "    if 'Sigma_J' in paramdict.keys():\n",
    "        Sigma_J = paramdict['Sigma_J']\n",
    "\n",
    "    D = s.shape[0]\n",
    "    p_vals = np.zeros(Sigma_s.shape)\n",
    "    p_vals2 = np.zeros(Sigma_s.shape)\n",
    "    KL_div_cavity_to_full = np.zeros(Sigma_s.shape)\n",
    "    KL_div_full_to_cavity = np.zeros(Sigma_s.shape)\n",
    "    # Compute the posterior variance at each fix point given all points but that fix point \n",
    "    # to judge the influence of that fixed point on the posterior\n",
    "    \n",
    "    L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=lengthscales, z=z, u=u, s=s, J=J, \n",
    "                                         sig_eps=0.*Sigma_eps, sig_u=Sigma_u, sig_s=Sigma_s, sig_J = Sigma_J)\n",
    "    mu_star, sig_star, K_pred = fp_predict(s, L, targets, params)\n",
    "    \n",
    "    for i in range(s.shape[1]):\n",
    "        \n",
    "        p_vals[i,:] = 2*(1-scipy.stats.norm.cdf(np.sqrt(\n",
    "            np.dot(np.dot((s[:,i]-mu_star[:,i]),np.linalg.inv(sig_star[i,:,:]+1e-4*np.eye(D))), (s[:,i]-mu_star[:,i]).T)\n",
    "        )))\n",
    "        \n",
    "    \n",
    "        cur_nll_term, mu_t1_t1, Sigma_t1_t1 = update_t_t(mu_star[:,[i]], sig_star[i,:,:], np.eye(D), 1e-4*np.ones((D,1)), s[:,[i]])\n",
    "        p_vals[i,:] = np.exp(-1.*cur_nll_term)\n",
    "\n",
    "        \n",
    "#         s_cavity = np.concatenate([s[:,:i], s[:,(i+1):]], axis=1)\n",
    "#         Sigma_s_cavity = np.concatenate([Sigma_s[:i,:], Sigma_s[(i+1):,:]], axis=0)\n",
    "#         J_cavity = np.concatenate([J[:i,:,:], J[(i+1):,:,:]], axis=0)\n",
    "#         Sigma_J_cavity = np.concatenate([Sigma_J[:i,:], Sigma_J[(i+1):,:]], axis=0)\n",
    "\n",
    "#         L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=lengthscales, z=z, u=u, s=s_cavity, J=J_cavity, \n",
    "#                                              sig_eps=0.*Sigma_eps, sig_u=Sigma_u, sig_s=Sigma_s_cavity, sig_J = Sigma_J_cavity)\n",
    "#         mu_star_cavity, sig_star_cavity, K_pred = fp_predict(s[:,[i]], L, targets, params)\n",
    "        \n",
    "#         # Based on \"Half width half maximum\", to estimate what the variance should be at a point, given the cavity and the point\n",
    "#         # We then compare this expected variance to the actual one\n",
    "#         Sig_expected = (np.sqrt(np.sum((s[:,[i]]-mu_star_cavity)**2)) / (np.sqrt(2*np.log(2.))))**2\n",
    "        \n",
    "#         p_vals[i] = Sig_expected/Sigma_s[i]\n",
    "\n",
    "        # Get the p-value of the learned fixed point\n",
    "#         p_vals[i,:] = 2*(1-scipy.stats.norm.cdf(np.abs(mu_star[:,[i]]-mu_star_cavity)/np.sqrt(sig_star[i,:,:]+sig_star_cavity)))\n",
    "#         p_vals_old[i,:] = 2*(1-scipy.stats.norm.cdf(np.abs(s[:,[i]]-mu_star_cavity)/np.sqrt(Sigma_s[i,:]+sig_star_cavity)))\n",
    "\n",
    "#         p_vals[i,:] = 2*(1-scipy.stats.norm.cdf(np.sqrt(\n",
    "#             np.dot(np.dot((s[:,i]-mu_star_cavity),np.linalg.inv(sig_star_cavity+Sigma_s[i]*np.eye(D))), (s[:,i]-mu_star_cavity).T)\n",
    "#         )))\n",
    "        \n",
    "#         # KL[star_cavity || star[i]]\n",
    "#         KL_div_cavity_to_full[i,:] = 1./2.*(\n",
    "#             np.linalg.slogdet(sig_star[i,:,:])[1] - np.linalg.slogdet(sig_star_cavity)[1] \n",
    "#             + np.trace(np.dot(np.linalg.inv(sig_star[i,:,:]), sig_star_cavity))\n",
    "#             + np.dot(np.dot((mu_star[:,i]-mu_star_cavity),np.linalg.inv(sig_star[i,:,:])), (mu_star[:,i]-mu_star_cavity).T)\n",
    "#             -D\n",
    "#         )\n",
    "        \n",
    "        \n",
    "#         # KL[star[i] || star_cavity]\n",
    "#         KL_div_full_to_cavity[i,:] = 1./2.*(\n",
    "#             np.linalg.slogdet(sig_star_cavity)[1] - np.linalg.slogdet(sig_star[i,:,:])[1] \n",
    "#             + np.trace(np.dot(np.linalg.inv(sig_star_cavity), sig_star[i,:,:]))\n",
    "#             + np.dot(np.dot((mu_star_cavity - mu_star[:,i]),np.linalg.inv(sig_star_cavity)), (mu_star_cavity - mu_star[:,i]).T)\n",
    "#             -D\n",
    "#         )\n",
    "    \n",
    "        #print (Sigma_s[i], s[:,i], mu_star[:,i], p_vals[i]) #, KL_div_cavity_to_full[i], KL_div_full_to_cavity[i])\n",
    "    return p_vals #[KL_div_cavity_to_full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fp_dynamic_threshold(GP_fit_saves_rs[19,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[y_train, x_train, bifurc_params,\n",
    "     all_results, \n",
    "     init_paramvec, dict_ind, dict_shape, opt_params, \n",
    "     bnds, transforms] = GP_fit_saves_rs[17,0,0,0]\n",
    "\n",
    "paramvec = replace_params(all_results[-1].x, opt_params, init_paramvec)\n",
    "paramdict = vec_to_params(paramvec, dict_ind, dict_shape, transforms)\n",
    "    \n",
    "paramdict['J']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt_all_bifurc = []\n",
    "plt_all_conf = []\n",
    "for (slope_ind, slope, curFit) in itertools.izip(range(len(slope_set)), slope_set, GP_fit_saves_rs[:,0,0,0]):\n",
    "    [y_train, x_train, bifurc_params,\n",
    "     all_results, \n",
    "     init_paramvec, dict_ind, dict_shape, opt_params, \n",
    "     bnds, transforms] = curFit\n",
    "    \n",
    "    paramvec = replace_params(all_results[-1].x, opt_params, init_paramvec)\n",
    "    paramdict = vec_to_params(paramvec, dict_ind, dict_shape, transforms)\n",
    "    \n",
    "    fp_sort = np.squeeze(np.argsort(paramdict['s']).T)\n",
    "    \n",
    "#     print [slope, slope_ind]\n",
    "#     print np.concatenate([paramdict['s'].T[fp_sort], paramdict['Sigma_s'][fp_sort], np.squeeze(paramdict['J'])[:,None][fp_sort]], axis=1)\n",
    "#     print \n",
    "    \n",
    "    \n",
    "    # Relying on \"how much this fixed points affects posterior\"\n",
    "    fp_p_vals = fp_dynamic_threshold(curFit)\n",
    "#     fp_p_vals_orig = fp_p_vals\n",
    "    \n",
    "#     # Relying on Sigma_s\n",
    "#     fp_p_vals = -np.log(paramdict['Sigma_s'])\n",
    "\n",
    "#     thresh = 0.05\n",
    "    \n",
    "    color_arr = np.array(['red', 'blue', 'black'])\n",
    "    color_choice = np.int32(np.squeeze(np.abs(paramdict['J'])>1.))\n",
    "    #color_choice[np.squeeze(paramdict['Sigma_s'] > fp_dynamic_threshold(curFit))] = 2\n",
    "    \n",
    "    fp_size = fp_p_vals - np.min(fp_p_vals)\n",
    "    fp_size = fp_size/np.max(fp_size)\n",
    "    fp_size = np.squeeze(fp_size*16+4)\n",
    "    \n",
    "    plt_all_bifurc.append(plt_type.Scatter(\n",
    "        x=slope*np.ones((paramdict['Sigma_s'].shape[0],)),\n",
    "            y=np.squeeze(paramdict['s']),\n",
    "            mode='markers', marker=dict(symbol='.', color=color_arr[color_choice], size=fp_size)\n",
    "        ))\n",
    "    \n",
    "#     \n",
    "    \n",
    "    #fp_p_vals = paramdict['Sigma_s']\n",
    "    \n",
    "    plt_all_conf.append(plt_type.Scatter(\n",
    "            x=slope*np.ones((paramdict['Sigma_s'].shape[0],))+5e-3*np.random.randn(paramdict['Sigma_s'].shape[0],),\n",
    "            y=np.squeeze(fp_p_vals),\n",
    "            mode='markers', marker=dict(symbol='x', color=color_arr[color_choice])\n",
    "            )   \n",
    "          )\n",
    "\n",
    "    \n",
    "    \n",
    "#     color_arr = np.array(['red', 'blue', 'black'])\n",
    "#     color_choice = np.int32(np.squeeze(paramdict['J'])>1)\n",
    "#     color_choice[np.squeeze(np.log(paramdict['Sigma_s']) >-2)] = 2\n",
    "    \n",
    "#     plt_all.append(plt_type.Scatter(\n",
    "#             x=slope*np.ones((paramdict['Sigma_s'].shape[0],)),\n",
    "#             y=np.squeeze(np.log(paramdict['Sigma_s'])),\n",
    "#             mode='markers', marker=dict(symbol='x', color=color_arr[color_choice])\n",
    "#             )   \n",
    "#           )\n",
    "\n",
    "\n",
    "plt(plt_all_bifurc)\n",
    "plt(plt_all_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt_all = []\n",
    "# for (slope_ind, slope, curFit) in itertools.izip(range(len(slope_set)), slope_set, GP_fit_saves):\n",
    "#     [y_train, x_train, bifurc_params,\n",
    "#      all_results, \n",
    "#      init_paramvec, dict_ind, dict_shape, opt_params, \n",
    "#      bnds, transforms] = curFit\n",
    "    \n",
    "#     paramvec = replace_params(all_results[-1].x, opt_params, init_paramvec)\n",
    "#     paramdict = vec_to_params(paramvec, dict_ind, dict_shape, transforms)\n",
    "    \n",
    "#     fp_sort = np.squeeze(np.argsort(paramdict['s']).T)\n",
    "    \n",
    "# #     print [slope, slope_ind]\n",
    "# #     print np.concatenate([paramdict['s'].T[fp_sort], paramdict['Sigma_s'][fp_sort], np.squeeze(paramdict['J'])[:,None][fp_sort]], axis=1)\n",
    "# #     print \n",
    "    \n",
    "#     color_arr = np.array(['red', 'blue', 'black'])\n",
    "#     color_choice = np.int32(np.squeeze(paramdict['J'])>1)\n",
    "#     color_choice[np.squeeze(np.log(paramdict['Sigma_s']) >-0.5)] = 2\n",
    "    \n",
    "#     plt_all.append(plt_type.Scatter(\n",
    "#             x=slope*np.ones((paramdict['Sigma_s'].shape[0],)),\n",
    "#             y=np.squeeze(np.log(paramdict['Sigma_s'])),\n",
    "#             mode='markers', marker=dict(symbol='x', color=color_arr[color_choice])\n",
    "#             )   \n",
    "#           )\n",
    "\n",
    "# plt(plt_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GP_fit_saves[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.log(paramdict['Sigma_s']) >-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slope_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create predictions for each test set trial\n",
    "def test_GP_pred(y_test, GPfit, cutoff=None):\n",
    "    if len(GPfit)>0:\n",
    "        # Sort results into variables to work with\n",
    "        [y_train, x_train, bifurc_params,\n",
    "                     all_results, \n",
    "                     init_paramvec, dict_ind, dict_shape, opt_params, \n",
    "                     bnds, transforms] = GPfit                                   \n",
    "\n",
    "        [x, y] = bifurc_create_data(bifurc_params)\n",
    "        # Establish test dataset\n",
    "        y_test = y[:,:,350:]\n",
    "        x_test = x[:,:,350:]\n",
    "        \n",
    "        \n",
    "        # Get GP predictions on test data\n",
    "        return pred_GP(y_test, \n",
    "                       replace_params(all_results[-1].x, opt_params, init_paramvec), \n",
    "                       transforms, dict_ind, dict_shape, cutoff = cutoff)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "# Load or make and save predictions\n",
    "force_redo = 1\n",
    "cutoff = None\n",
    "preds_fname = \"Experiment_1d_bifurc_results/bifurc_1d_\" + start_time + \"_predictions.pkl\"\n",
    "try:\n",
    "    if not force_redo:\n",
    "        [GP_predictions, ntrials_set, nfix_set, nbatches] = (\n",
    "            pickle.load(open(preds_fname, 'rb')))\n",
    "    else:\n",
    "        raise Exception('Redoing GP predictions per request')\n",
    "except: # (OSError, IOError) as e:\n",
    "    GP_predictions = Parallel(n_jobs=num_cores)(\n",
    "        delayed(test_GP_pred)(y_test, GPfit, cutoff=cutoff)\n",
    "        for GPfit in GP_fit_saves\n",
    "    )\n",
    "    pickle.dump([GP_predictions, ntrials_set, nfix_set, nbatches], \n",
    "                open(preds_fname, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get errors based on predictions\n",
    "axisError = (0,1,2)\n",
    "\n",
    "AR_RMSE = []\n",
    "for GPfit in GP_fit_saves:\n",
    "    if len(GPfit)>0:\n",
    "        AR_RMSE.append(rmse(pred_lin_AR1(y_test, GPfit[0], cutoff=cutoff), y_test, axis=axisError))\n",
    "    else:\n",
    "        AR_RMSE.append(np.infty)\n",
    "\n",
    "GP_NLL = []\n",
    "GP_RMSE = []\n",
    "for GPpred in GP_predictions:\n",
    "    if len(GPpred)>0:\n",
    "        GP_NLL.append(np.sum(GPpred[5], axis=tuple(np.array(axisError)[1:]-1)))\n",
    "        GP_RMSE.append(rmse(GPpred[7], y_test, axis=axisError))\n",
    "    else:\n",
    "        GP_NLL.append(np.infty)\n",
    "        GP_RMSE.append(np.infty)\n",
    "    \n",
    "\n",
    "AR_RMSE = np.reshape(np.array(AR_RMSE), (len(ntrials_set), len(nfix_set), nbatches))\n",
    "GP_RMSE = np.reshape(np.array(GP_RMSE), (len(ntrials_set), len(nfix_set), nbatches))\n",
    "GP_NLL = np.reshape(np.array(GP_NLL), (len(ntrials_set), len(nfix_set), nbatches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "print \"AR_RMSE\"\n",
    "print np.array(np.median(AR_RMSE, axis=2))\n",
    "print \"-------------\"\n",
    "print \"GP_RMSE\"\n",
    "print np.array(np.median(GP_RMSE, axis=2))\n",
    "print \"-------------\"\n",
    "print \"GP_NLL\"\n",
    "print np.array(np.median(GP_NLL, axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot negative log likelihoods for each options\n",
    "def plot_nll_fixpoints(cur_nll_ntrial, nfix_set, nbatches):\n",
    "    cur_plts = []\n",
    "    for (nfix_ind, nfix) in itertools.izip(range(len(nfix_set)), nfix_set):\n",
    "        cur_plts.append(plt_type.Scatter(x=nfix*np.ones_like(np.squeeze(cur_nll_ntrial[nfix_ind,:])), \n",
    "                                         y=np.squeeze(cur_nll_ntrial[nfix_ind,:]), \n",
    "                                         mode='markers', \n",
    "                                         marker = dict(color=plotly.colors.DEFAULT_PLOTLY_COLORS[nfix_ind]),\n",
    "                                         name = \"nfix=%d\" % nfix, \n",
    "                                         legendgroup=\"nfix=%d\" % nfix))\n",
    "    return cur_plts\n",
    "    \n",
    "    \n",
    "from plotly import tools as plt_tools\n",
    "\n",
    "def plot_nll(cur_nll, ntrials_set, nfix_set, nbatches):\n",
    "    fig_subplts = plt_tools.make_subplots(rows=1, cols=len(ntrials_set), subplot_titles=ntrials_set, shared_yaxes=True)\n",
    "    for (ntrials_ind, ntrials) in itertools.izip(range(len(ntrials_set)), ntrials_set):        \n",
    "        tmp = plot_nll_fixpoints(cur_nll[ntrials_ind,:,:], nfix_set, nbatches)\n",
    "        for trace in tmp:\n",
    "            fig_subplts.append_trace(trace,1,ntrials_ind+1)\n",
    "            if ntrials_ind>0:\n",
    "                fig_subplts['data'][-1]['showlegend']=False\n",
    "    plt(fig_subplts)\n",
    "    return fig_subplts\n",
    "\n",
    "a = plot_nll(GP_NLL, ntrials_set, nfix_set, nbatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "print \"-------------\"\n",
    "print \"GP_NLL\"\n",
    "print np.array(np.min(GP_NLL, axis=2))\n",
    "print \"-------------\"\n",
    "print \"GP_NLL\"\n",
    "print np.array(np.argmin(GP_NLL, axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot a single fit\n",
    "GP_fit_saves_rs = np.reshape(nestedlist_to_array(GP_fit_saves, max_level=0), \n",
    "                             (len(slope_set), len(ntrials_set), len(nfix_set), nbatches))\n",
    "\n",
    "slope_ind =17\n",
    "nfix_ind = 0\n",
    "ntrials_ind = 0\n",
    "batchnum = 0\n",
    "\n",
    "[y_train, x_train, bifurc_params,\n",
    " all_results, \n",
    " init_paramvec, dict_ind, dict_shape, opt_params, \n",
    " bnds, transforms] = GP_fit_saves_rs[slope_ind, ntrials_ind, nfix_ind, batchnum]\n",
    "\n",
    "print slope_set[slope_ind]\n",
    "\n",
    "# init_paramvec[dict_ind['Sigma_J']] = 0.0\n",
    "# opt_params = opt_params[opt_params<np.min(dict_ind['Sigma_J'])]\n",
    "\n",
    "bifurc_callback_plot_external(init_paramvec[opt_params], \n",
    "        opt_params, init_paramvec, transforms, dict_ind, dict_shape,\n",
    "        bifurc_params, trainingData=y_train)\n",
    "\n",
    "# for res in all_results[:-1]:\n",
    "#     bifurc_callback_plot_external(res.x, \n",
    "#             opt_params, init_paramvec, transforms, dict_ind, dict_shape,\n",
    "#             bifurc_params)\n",
    "\n",
    "plt([plt_type.Scatter(x=np.array(all_results[-1].objs)[:,0], y = np.array(all_results[-1].objs)[:,1])])\n",
    "# for theta_ind in range(len(all_results[0].theta_hist)):\n",
    "#     if (not np.mod(theta_ind, 30)) and (theta_ind<200):\n",
    "#         print np.array(all_results[0].objs)[np.array(all_results[0].objs)[:,0]==theta_ind, :]\n",
    "#         bifurc_callback_plot_external(all_results[0].theta_hist[theta_ind], \n",
    "#             opt_params, init_paramvec, transforms, dict_ind, dict_shape,\n",
    "#             bifurc_params)\n",
    "    \n",
    "    \n",
    "# theta_no_u = replace_params(all_results[-1].x, opt_params, init_paramvec)\n",
    "# theta_no_u[dict_ind['Sigma_u']] = 1e9*theta_no_u[dict_ind['Sigma_u']]\n",
    "\n",
    "# bifurc_callback_plot_external(theta_no_u[opt_params], \n",
    "#         opt_params, init_paramvec, transforms, dict_ind, dict_shape,\n",
    "#         bifurc_params)\n",
    "\n",
    "#all_results[-1].x = all_results[-1].theta_hist[142]\n",
    "\n",
    "bifurc_callback_plot_external(all_results[-1].x,\n",
    "        opt_params, init_paramvec, transforms, dict_ind, dict_shape,\n",
    "        bifurc_params)\n",
    "\n",
    "#\n",
    "\n",
    "paramvec = replace_params(all_results[-1].x, opt_params, init_paramvec)\n",
    "paramdict = vec_to_params(paramvec, dict_ind, dict_shape, transforms)\n",
    "\n",
    "# Unpack the usual parameters\n",
    "(Sigma_eps, mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_u, lengthscales, kernel_variance, s, J, Sigma_s, Sigma_J)  = \\\n",
    "    paramdict.values()\n",
    "    \n",
    "print np.concatenate([s.T, Sigma_s, np.squeeze(J)[:,None]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Sigma_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(all_results[0].objs)[:,0]==50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not np.mod(200, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range(len(all_results[0].theta_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(all_results[-1].objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramvec = replace_params(all_results[-1].x, opt_params, init_paramvec)\n",
    "paramdict = vec_to_params(paramvec, dict_ind, dict_shape, transforms)\n",
    "\n",
    "# Unpack the usual parameters\n",
    "(Sigma_eps, mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_u, lengthscales, kernel_variance, s, J, Sigma_s, Sigma_J)  = \\\n",
    "    paramdict.values()\n",
    "    \n",
    "np.set_printoptions(precision=8)\n",
    "paramdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec_to_params(init_paramvec, dict_ind, dict_shape, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8)\n",
    "print z\n",
    "print u\n",
    "print Sigma_u\n",
    "print s\n",
    "print Sigma_s\n",
    "print Sigma_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print lengthscales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add priors (to span at least the bounds)\n",
    "priors = []\n",
    "\n",
    "# Add prior to ensure inducing points are smooth\n",
    "cur_prior = {}\n",
    "cur_prior['func'] = create_prior(prior_distribution=\"InducingSmooth\")\n",
    "cur_prior['inds'] = range(len(init_paramvec))\n",
    "cur_prior['metadata'] = {}\n",
    "def unpack_x_tmp(x):\n",
    "    # Unpack all the usual parameters\n",
    "    param_tuple = vec_to_params(x, dict_ind, dict_shape)\n",
    "    (Sigma_eps, mu_0_0, Sigma_0_0, C, Sigma_nu, \n",
    "     z, u, Sigma_u, \n",
    "     lengthscales, kernel_variance, s, J, Sigma_s, Sigma_J)  = \\\n",
    "        param_tuple\n",
    "\n",
    "    kernelparams = {'lengthscales': lengthscales, 'kernel_variance': kernel_variance}\n",
    "    # Return the ones we want in the required format\n",
    "    return (np.concatenate([u, s],axis=1), \n",
    "            np.concatenate([z, s],axis=1),\n",
    "            np.concatenate([Sigma_u, Sigma_s]),\n",
    "            kernelparams)\n",
    "cur_prior['metadata']['unpack_x'] = unpack_x_tmp\n",
    "cur_prior['metadata']['kernel_func'] = RBF\n",
    "priors.append(cur_prior)\n",
    "\n",
    "tmp_func = lambda pvec_partial: (time_full_iter(replace_params(pvec_partial, opt_params, init_paramvec), \n",
    "                                                y_train, dict_ind, dict_shape, \n",
    "                                                log_transformed=log_transformed,\n",
    "                                                priors=priors)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramvec = replace_params(all_results[1].x, opt_params, init_paramvec)\n",
    "#paramvec = log_transform_inv(paramvec, log_transformed)\n",
    "paramvec = paramvec[opt_params]\n",
    "a = grad(tmp_func)(paramvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramvec = replace_params(a, opt_params, init_paramvec)\n",
    "#paramvec = log_transform_inv(paramvec, log_transformed)\n",
    "\n",
    "# Unpack the usual parameters\n",
    "param_tuple = vec_to_params(paramvec, dict_ind, dict_shape)\n",
    "(Sigma_eps, mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_u, lengthscales, kernel_variance, s, J, Sigma_s, Sigma_J)  = \\\n",
    "    param_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8)\n",
    "print z\n",
    "print u\n",
    "print Sigma_u\n",
    "print s\n",
    "print Sigma_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8)\n",
    "print z\n",
    "print u\n",
    "print Sigma_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print s\n",
    "print Sigma_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec_to_params(init_paramvec, dict_ind, dict_shape, transforms=transforms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
