{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd.scipy as scipy\n",
    "# import numpy as np\n",
    "# import scipy\n",
    "import scipy.linalg\n",
    "import scipy.cluster\n",
    "import scipy.stats\n",
    "from autograd import grad, elementwise_grad, jacobian, hessian, value_and_grad\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "import plotly\n",
    "from plotly.offline import iplot as plt\n",
    "from plotly import graph_objs as plt_type\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_dist(x, x2=None, lengthscales=None):\n",
    "    if lengthscales is None:\n",
    "        lengthscales=np.ones((x.shape[0], 1))\n",
    "    \n",
    "    x = x / lengthscales\n",
    "    xs = np.sum(np.square(x), axis=0)\n",
    "    if x2 is None:\n",
    "        return -2 * np.dot(x.T, x) + \\\n",
    "               np.reshape(xs, (-1, 1)) + np.reshape(xs, (1, -1))\n",
    "    else:\n",
    "        x2 = x2 / lengthscales\n",
    "        x2s = np.sum(np.square(x2), axis=0)\n",
    "        return -2 * np.dot(x.T, x2) + \\\n",
    "               np.reshape(xs, (-1, 1)) + np.reshape(x2s, (1, -1))\n",
    "\n",
    "def euclid_dist(x, x2, lengthscales=None):\n",
    "    if lengthscales is None:\n",
    "        lengthscales=np.ones((x.shape[0], 1))\n",
    "    r2 = square_dist(x, x2, lengthscales)\n",
    "    return np.sqrt(r2 + 1e-12)\n",
    "\n",
    "def RBF(x, x2=None, lengthscales=None, kernel_variance=1.):\n",
    "    if x.shape[1]==0:\n",
    "        if (x2 is not None):\n",
    "            return np.zeros((0, x2.shape[1]))\n",
    "        else:\n",
    "            return np.zeros((0,0))\n",
    "    elif (x2 is not None):\n",
    "        if x2.shape[1]==0:\n",
    "            return np.zeros((x.shape[1], 0))\n",
    "    \n",
    "    if lengthscales is None:\n",
    "        lengthscales=np.ones((x.shape[0], 1))\n",
    "    \n",
    "    return kernel_variance*np.exp(-square_dist(x, x2, lengthscales=lengthscales)/2)\n",
    "\n",
    "def dRBF(x, x2=None, *args,**kwargs):\n",
    "    D = x.shape[0]\n",
    "    if x.shape[1]==0:\n",
    "        if (x2 is not None):\n",
    "            return np.zeros((0, x2.shape[1]))\n",
    "        else:\n",
    "            return np.zeros((0,0))\n",
    "    elif (x2 is not None):\n",
    "        if x2.shape[1]==0:\n",
    "            return np.zeros((D*x.shape[1], 0))\n",
    "        \n",
    "    if x2 is None:\n",
    "        x2 = x\n",
    "        \n",
    "    N_x1 = x.shape[1]\n",
    "    N_x2 = x2.shape[1]\n",
    "    \n",
    "#     # Get kernel matrix (N_x1 x N_x2)\n",
    "#     if 'Kxx' in kwargs:\n",
    "#         Kxx = kwargs['Kxx']\n",
    "#     else:\n",
    "#         Kxx = RBF(x, x2=x2, *args, **kwargs)\n",
    "    \n",
    "#     # Get pairwise distances per columns (D x N_x1 x N_x2)\n",
    "#     XminusX2_pairwise_diffs = x[:,:,None] - x2[:,:,None].swapaxes(1,2)\n",
    "    \n",
    "# #     # Elementwise version for testing\n",
    "# #     out = np.zeros((D,N_x1,N_x2))\n",
    "# #     for i in range(N_x1):\n",
    "# #         for j in range(N_x2):\n",
    "# #             out[:,i,j] = -(1./np.squeeze(lengthscales**2))*(x[:,i]-x2[:,j]) *Kxx[i,j]\n",
    "    \n",
    "#     out = - (1./(np.expand_dims(lengthscales**2,axis=2))) * ( XminusX2_pairwise_diffs * np.expand_dims(Kxx, axis=0) );\n",
    "#     # We want to stack it to 2D to have shape (D*N_x1,  N_x2)\n",
    "#     return np.reshape(out, (D*N_x1,N_x2), order='F')\n",
    "\n",
    "    ### There is some discrepency here between the autograd dRBF and the above manual one, Test later?\n",
    "    # The jacobian returns a shape\n",
    "    # (N_x1, N_x2, D, 1)\n",
    "    jRBF = jacobian(RBF)\n",
    "    gradRBF = np.concatenate([jRBF(x[:,i:(i+1)], x2, *args, **kwargs) for i in range(N_x1)], axis=0)\n",
    "    # For every x1 input point, compute a 1xN_x2xD jacobian, then stack them by point getting N_x1, N_x2, D, 1\n",
    "    \n",
    "    # We want to stack it to 2D to have shape \n",
    "    # (D*N_x1,  N_x2)\n",
    "    \n",
    "    # Here the derivative is with respect to the first argument, and it is ANTI-SYMMETRIC (Transpose -> minus sign)\n",
    "    return np.reshape(gradRBF.swapaxes(1,2).swapaxes(0,1), (D*N_x1, -1), order='F')\n",
    "    \n",
    "    \n",
    "def ddRBF(x, x2=None, *args, **kwargs):\n",
    "    D = x.shape[0]\n",
    "    if x.shape[1]==0:\n",
    "        if (x2 is not None):\n",
    "            return np.zeros((0, D*x2.shape[1]))\n",
    "        else:\n",
    "            return np.zeros((0,0))\n",
    "    elif (x2 is not None):\n",
    "        if x2.shape[1]==0:\n",
    "            return np.zeros((D*x.shape[1], 0))\n",
    "        \n",
    "    if x2 is None:\n",
    "        x2 = x\n",
    "    \n",
    "    N_x1 = x.shape[1]\n",
    "    N_x2 = x2.shape[1]\n",
    "    \n",
    "#     # Get kernel matrix (N_x1 x N_x2)\n",
    "#     if 'Kxx' in kwargs:\n",
    "#         Kxx = kwargs['Kxx']\n",
    "#     else:\n",
    "#         Kxx = RBF(x, x2=x2, *args, **kwargs)\n",
    "        \n",
    "#     # Get pairwise distances per columns (D x N_x1 x N_x2)\n",
    "#     XminusX2_pairwise_diffs = x[:,:,None] - x2[:,:,None].swapaxes(1,2)\n",
    "    \n",
    "#     XminusX2_pairwise_diffs_rescaled = (1./(lengthscales**2)[:,:,None])*XminusX2_pairwise_diffs\n",
    "    \n",
    "#     # Get the outer product of pairwise distance per instance (D x D x N_x1 x N_x2)\n",
    "#     XminusX2_pairwise_diffs_rescaled_outer = (np.expand_dims(XminusX2_pairwise_diffs_rescaled,axis=1)\n",
    "#                                         * np.expand_dims(XminusX2_pairwise_diffs_rescaled,axis=0)\n",
    "#                                      )\n",
    "    \n",
    "#     lengthscales_times_id = np.diag(np.squeeze(1./(lengthscales**2)))[:,:,None,None]\n",
    "        \n",
    "    \n",
    "    \n",
    "#     out = ((lengthscales_times_id - XminusX2_pairwise_diffs_rescaled_outer) \n",
    "#            * np.expand_dims(np.expand_dims(Kxx, axis=0),axis=0)\n",
    "#            )\n",
    "    \n",
    "#     # Out has a shape of D x D x N_x1 x N_x2\n",
    "#     # We want to stack it to 2D to have shape (D*N_x1, D*N_x2)\n",
    "#     return np.reshape(out.swapaxes(1,2), (D*N_x1, D*N_x2), order='F')\n",
    "    \n",
    "    # The hessian defined here returns a shape\n",
    "    # (D*N_x1, N_x2, D, 1)\n",
    "    hRBF = jacobian(dRBF, argnum=1)\n",
    "    \n",
    "    hessRBF = np.concatenate([hRBF(x, x2[:,j:(j+1)], *args, **kwargs) for j in range(N_x2)], axis=1)\n",
    "    \n",
    "    # We want to stack it to 2D to have shape \n",
    "    # (D*N_x1, D*N_x2)\n",
    "    \n",
    "    return np.reshape(hessRBF.swapaxes(1,2), (D*N_x1, -1), order='F')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected kernels for noisy input\n",
    "Compute various kernel expectations of the RBF kernel written above, \n",
    "given the first kernel argument is a single Dx1 vector, given by mean $\\mu$ (Dx1) and diagonal variance $\\sigma$ (Dx1)\n",
    "\n",
    "Note: Lengthscales are sqrt(gaussian_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing noisy (with little noise) vs non-noisy versions\n",
    "# mu = np.array([[0], [[0]]])\n",
    "# sigma = np.array([[1e-1], [1e-1]])\n",
    "# lengthscales = np.array([[1.0], [1.0]])\n",
    "# kernel_variance = 1.0\n",
    "# X = np.concatenate([np.arange(-5,5,0.5)[:,None].T, np.arange(-5,5,0.5)[:,None].T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF_eK(mu, sigma, X, lengthscales=None, kernel_variance=1):\n",
    "    \"\"\"\n",
    "    x ~ N(mu, sigma), Dx1\n",
    "    X is DxM\n",
    "    Return E_x [ k(x, X)], a 1 x M array\n",
    "    \"\"\"\n",
    "    if X.shape[1]==0:\n",
    "        return np.zeros((1, 0))\n",
    "    \n",
    "    if lengthscales is None:\n",
    "        lengthscales=np.ones((mu.shape[0], 1))\n",
    "    return RBF(x=mu, \n",
    "               x2=X, \n",
    "               lengthscales=np.sqrt(lengthscales**2 + sigma), \n",
    "               kernel_variance=kernel_variance*np.sqrt(np.prod(lengthscales**2)/np.prod(lengthscales**2 + sigma))\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF_exK(mu, sigma, X, lengthscales=None, kernel_variance=1, eK=None):\n",
    "    \"\"\"\n",
    "    x ~ N(mu, sigma), Dx1\n",
    "    X is DxM\n",
    "    Return E_x [ x * k(x, X)], a D x M array\n",
    "    \"\"\"\n",
    "    \n",
    "    if X.shape[1]==0:\n",
    "        return np.zeros((x.shape[0], 0))\n",
    "    \n",
    "    if lengthscales is None:\n",
    "        lengthscales=np.ones((mu.shape[0], 1))\n",
    "        \n",
    "    if eK is None:\n",
    "        eK = RBF_eK(mu, sigma, X, lengthscales=lengthscales, kernel_variance=kernel_variance)\n",
    "    \n",
    "    mean_gauss = (X/(lengthscales**2) + mu/sigma)/(1/(lengthscales**2)+(1/sigma))\n",
    "    \n",
    "    return eK*mean_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF_exK(np.array([[1.5], [[1]]]), np.array([[1e-20], [1e-2]]), X[:,11:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array([[1.5], [[1]]]) * RBF(np.array([[1.5], [[1]]]), X[:,11:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF_edK(mu, sigma, X, lengthscales=None, kernel_variance=1, eK=None, exK=None):\n",
    "    \"\"\"\n",
    "    x ~ N(mu, sigma), Dx1\n",
    "    X is DxM\n",
    "    Return E_x [ dk(x, X) ], an 1 x (D x M) array\n",
    "    We want it differentiated with respect to the second argument X -> No minus sign (minus signs cancel)\n",
    "    \"\"\"\n",
    "    if X.shape[1]==0:\n",
    "        return np.zeros((1, 0))\n",
    "    \n",
    "    if lengthscales is None:\n",
    "        lengthscales=np.ones((mu.shape[0], 1))\n",
    "        \n",
    "    if eK is None:\n",
    "        eK = RBF_eK(mu=mu, sigma=sigma, X=X, lengthscales=lengthscales, kernel_variance=kernel_variance)\n",
    "    \n",
    "    if exK is None:\n",
    "        exK = RBF_exK(mu=mu, sigma=sigma, X=X, lengthscales=lengthscales, kernel_variance=kernel_variance)\n",
    "    \n",
    "    \n",
    "    return np.reshape((exK - X * eK)/(lengthscales**2),(1,-1), order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF_eKK(mu, sigma, X, lengthscales=None, kernel_variance=1):\n",
    "    \"\"\"\n",
    "    x ~ N(mu, sigma), Dx1\n",
    "    X is DxM\n",
    "    Return E_x [k(X, x) * k(x, X) ], an M x M array\n",
    "    \"\"\"\n",
    "    if X.shape[1]==0:\n",
    "        return np.zeros((0,0))\n",
    "    \n",
    "    if lengthscales is None:\n",
    "        lengthscales=np.ones((mu.shape[0], 1))\n",
    "        \n",
    "        \n",
    "    kXX_scaled = RBF(\n",
    "                       x=X, \n",
    "                       x2=X, \n",
    "                       lengthscales=np.sqrt(2*(lengthscales**2)), \n",
    "                       kernel_variance=kernel_variance*np.sqrt(np.prod(lengthscales**2)/np.prod(2*(lengthscales**2)))\n",
    "                    )\n",
    "    \n",
    "    X_pairwise_sums = X[:,:,None] + X[:,:,None].swapaxes(1,2)\n",
    "\n",
    "    kXpX_mu = RBF(\n",
    "                       x=np.reshape(X_pairwise_sums/2,(mu.shape[0], -1), order='F'), \n",
    "                       x2=mu, \n",
    "                       lengthscales=np.sqrt((lengthscales**2)/2 + sigma), \n",
    "                       kernel_variance=kernel_variance*np.sqrt(np.prod(lengthscales**2)/np.prod((lengthscales**2)/2 + sigma))\n",
    "                    )\n",
    "    \n",
    "    out = kXX_scaled * np.reshape(kXpX_mu, (X.shape[1], X.shape[1]), order='F')\n",
    "    \n",
    "    # Due to numerical instability, this is not always symmetric, fix:\n",
    "    out = (out + out.T) / 2.\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing noisy expected kernel outer product vs noiseless \n",
    "# RBF_eKK(np.array([[0], [[0]]]), np.array([[1e-13], [1e-13]]), X[:,9:12]) - \\\n",
    "#     RBF(np.array([[0], [[0]]]), X[:,9:12]) * RBF(np.array([[0], [[0]]]), X[:,9:12]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF_exKK(mu, sigma, X, lengthscales=None, kernel_variance=1, eKK=None):\n",
    "    \"\"\"\n",
    "    x ~ N(mu, sigma), Dx1\n",
    "    X is DxM\n",
    "    Return E_x [x * k(X, x) * k(x, X) ], a D x M x M array\n",
    "    \"\"\"\n",
    "    if X.shape[1]==0:\n",
    "        return np.zeros((x.shape[0], 0,0))\n",
    "    \n",
    "    if lengthscales is None:\n",
    "        lengthscales=np.ones((mu.shape[0], 1))\n",
    "        \n",
    "    \n",
    "    # M x M array\n",
    "    if eKK is None:\n",
    "        eKK = RBF_eKK(mu=mu, sigma=sigma, X=X, lengthscales=lengthscales, kernel_variance=kernel_variance)\n",
    "    \n",
    "    X_pairwise_sums = X[:,:,None] + X[:,:,None].swapaxes(1,2)\n",
    "\n",
    "    # D x M x M array\n",
    "    mean_gauss = ((X_pairwise_sums/2)/(((lengthscales**2)/2)[:,:,None]) + (mu/sigma)[:,:,None])/(\n",
    "                                                (1/((lengthscales**2)/2)+(1/sigma))[:,:,None])\n",
    "    \n",
    "    \n",
    "    return eKK[:,:,None].swapaxes(1,2).swapaxes(0,1) * mean_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing noisy expected kernel outer product\n",
    "# RBF_exKK(np.array([[1], [[1]]]), np.array([[1e-13], [1e-13]]), X[:,9:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     RBF(np.array([[1], [[1]]]), X[:,9:12]) * RBF(np.array([[1], [[1]]]), X[:,9:12]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF_exxKK(mu, sigma, X, lengthscales=None, kernel_variance=1, eKK=None):\n",
    "    \"\"\"\n",
    "    x ~ N(mu, sigma), Dx1\n",
    "    X is DxM\n",
    "    Return E_x [ (x*x.T) * k(X, x) * k(x, X) ], a D x D x M x M array\n",
    "    \"\"\"\n",
    "    if X.shape[1]==0:\n",
    "        return np.zeros((x.shape[0], x.shape[0], 0,0))\n",
    "    \n",
    "    if lengthscales is None:\n",
    "        lengthscales=np.ones((mu.shape[0], 1))\n",
    "        \n",
    "    \n",
    "    # M x M array\n",
    "    if eKK is None:\n",
    "        eKK = RBF_eKK(mu=mu, sigma=sigma, X=X, lengthscales=lengthscales, kernel_variance=kernel_variance)\n",
    "    \n",
    "    # D x D array\n",
    "    var_gauss = 1/(1/((lengthscales**2)/2)+(1/sigma))\n",
    "    \n",
    "    \n",
    "    X_pairwise_sums = X[:,:,None] + X[:,:,None].swapaxes(1,2)\n",
    "\n",
    "    # D x M x M array\n",
    "    mean_gauss = ((X_pairwise_sums/2)/(((lengthscales**2)/2)[:,:,None]) + (mu/sigma)[:,:,None])*(var_gauss[:,:,None])\n",
    "    \n",
    "    # D x D x M x M array\n",
    "    mean_outer = np.expand_dims(mean_gauss, axis=1) * np.expand_dims(mean_gauss, axis=0)\n",
    "    \n",
    "    return np.expand_dims(np.expand_dims(eKK, axis=0), axis=0) * (var_gauss[:,:,None,None] + mean_outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing noisy expected kernel outer product\n",
    "# RBF_exxKK(np.array([[1], [[1]]]), np.array([[1e-2], [1e-2]]), X[:,11:14])[0,1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  RBF(np.array([[1], [[1]]]), X[:,11:14]) * RBF(np.array([[1], [[1]]]), X[:,11:14]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF_eKdK(mu, sigma, X, lengthscales=None, kernel_variance=1, eKK=None, exKK=None):\n",
    "    \"\"\"\n",
    "    x ~ N(mu, sigma), Dx1\n",
    "    X is DxM\n",
    "    Return E_x [  k(X, x) * dk(x, X)  ], an M x (D x M) array\n",
    "    \"\"\"\n",
    "    \n",
    "    if X.shape[1]==0:\n",
    "        return np.zeros((0,0))\n",
    "    \n",
    "    if lengthscales is None:\n",
    "        lengthscales=np.ones((mu.shape[0], 1))\n",
    "        \n",
    "        \n",
    "        \n",
    "    # m1 x m2\n",
    "    if eKK is None:\n",
    "        eKK = RBF_eKK(mu=mu, sigma=sigma, X=X, lengthscales=lengthscales, kernel_variance=kernel_variance)\n",
    "        \n",
    "    # d x m1 x m2\n",
    "    if exKK is None:\n",
    "        exKK = RBF_exKK(mu=mu, sigma=sigma, X=X, lengthscales=lengthscales, kernel_variance=kernel_variance)    \n",
    "    \n",
    "    \n",
    "    # d x m1 x m2,\n",
    "    # As exKK naturally uses the first argument and\n",
    "    # X is the second argument in the derivative kernel, we should expand it, such that we iterate along m2 dimension    \n",
    "    eKdK = (exKK - np.expand_dims(X, axis=1) * np.expand_dims(eKK, axis=0))/((lengthscales**2)[:,:,None])\n",
    "       \n",
    "    # We then finally modify the order of axis and the dimensionality to get \n",
    "    # the expected m1 - d - m2 order with M x (DM) shape\n",
    "    \n",
    "    return np.reshape(eKdK.swapaxes(0,1), (X.shape[1], -1), order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF_eKdK(np.array([[0], [[1]]]), np.array([[1e-20], [1e-20]]), X[:,13:17])[0:2,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  RBF(np.array([[0], [[1]]]), X[:,13:15]).T * np.reshape(dRBF(X[:,15:17], np.array([[0], [[1]]])),(-1), order='F').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF_edKdK(mu, sigma, X, lengthscales=None, kernel_variance=1, eKK=None, exKK=None, exxKK=None):\n",
    "    \"\"\"\n",
    "    x ~ N(mu, sigma), Dx1\n",
    "    X is DxM\n",
    "    Return E_x [  dk(X, x) * dk(x, X)  ], a (D x M) x (D x M) array\n",
    "    \"\"\"\n",
    "    if X.shape[1]==0:\n",
    "        return np.zeros((0,0))\n",
    "    \n",
    "    if lengthscales is None:\n",
    "        lengthscales=np.ones((mu.shape[0], 1))\n",
    "            \n",
    "    # m1 x m2\n",
    "    if eKK is None:\n",
    "        eKK = RBF_eKK(mu=mu, sigma=sigma, X=X, lengthscales=lengthscales, kernel_variance=kernel_variance)\n",
    "    \n",
    "    # d1 x m1 x m2\n",
    "    if exKK is None:\n",
    "        exKK = RBF_exKK(mu=mu, sigma=sigma, X=X, lengthscales=lengthscales, kernel_variance=kernel_variance,\n",
    "                       eKK = eKK)\n",
    "    \n",
    "    # d1 x d2 x m1 x m2\n",
    "    if exxKK is None:\n",
    "        exxKK = RBF_exxKK(mu=mu, sigma=sigma, X=X, lengthscales=lengthscales, kernel_variance=kernel_variance,\n",
    "                        eKK = eKK)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    edKdK = (exxKK # exKK\n",
    "        -1.0 * np.expand_dims(np.expand_dims(X, axis=2), axis=1) * np.expand_dims(exKK, axis=0)  # X[:,None,:,None]\n",
    "        -1.0 * np.expand_dims(np.expand_dims(X, axis=1), axis=0) * np.expand_dims(exKK, axis=1)  # X[None,:,None,:]\n",
    "        + np.expand_dims(np.expand_dims(X, axis=2), axis=1) * np.expand_dims(np.expand_dims(X, axis=1), axis=0) \n",
    "           * np.expand_dims(np.expand_dims(eKK, axis=0), axis=0) # X[:,None,:,None] * X[None,:,None,:] * eKK[None,None,:,:]\n",
    "    )\n",
    "    \n",
    "    # Divide with lengthscales appropriately\n",
    "    edKdK = edKdK / ((lengthscales.T**2)[:,:,None,None])\n",
    "    edKdK = edKdK / ((lengthscales**2)[:,:,None,None])\n",
    "    \n",
    "    # We then finally modify the order of axis and the dimensionality to get \n",
    "    # the expected m1 - d - m2 order with M x (DM) shape\n",
    "    \n",
    "    out = np.reshape(edKdK.swapaxes(1,2), (X.shape[0]*X.shape[1], X.shape[0]*X.shape[1]), order='F')\n",
    "    \n",
    "    # Due to numerical instability (TODO: Double check if really instab), this is not always symmetric, fix:\n",
    "    out = (out + out.T) / 2.\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF_edKdK(np.array([[0.5], [[0]]]), np.array([[1e-20], [1e-20]]), X[:,11:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.reshape(dRBF(np.array([[0.5], [[0]]]), X[:,11:14]),(-1,1), order='F') * np.reshape(dRBF(np.array([[0.5], [[0]]]), X[:,11:14]),(-1), order='F').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing expected derivative kernel vs derivative kernel in 1D\n",
    "# xstar = np.arange(-5,5,0.5)\n",
    "\n",
    "# plots_by_var = []\n",
    "# for v in [0.01,1.0,2.0,3.0]:\n",
    "#     plots_by_var.append(\n",
    "#         plt_type.Scatter(x=np.squeeze(xstar), \n",
    "#                       y=np.squeeze(RBF_edK(np.array([[0]]),np.array([[v]]), xstar)), \n",
    "#                       mode='markers')\n",
    "#     )\n",
    "    \n",
    "\n",
    "# plots_by_var.append(\n",
    "#     plt_type.Scatter(x=np.squeeze(xstar), \n",
    "#                   y=np.squeeze(dRBF(np.atleast_2d(xstar) , np.array([[0]])).T), \n",
    "#                   mode='markers')\n",
    "#     )\n",
    "    \n",
    "# plt(plots_by_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP prediction given inducing and fixed points\n",
    "\n",
    "Params:\n",
    "* Kernel type - RBF (/w dRBF, ddRBF)\n",
    "* Kernel hyper - variance (eta) and lengthscales\n",
    "* Inducing point locations (z DxL) and values (u, DxL)\n",
    "* Fixed point locations (s, DxM) and jacobians (J, MxD_outxD_in)\n",
    "* Output noise level (sig_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_get_static_K(eta, lengthscales, z, u, s, J, sig_eps, sig_u=None, sig_s=None, sig_J=None):\n",
    "    \"\"\"\n",
    "    Return the cholesky decomposition of the structured kernel matrix, \n",
    "    as well as structured targets, \n",
    "    so these only have to be computed once per parameter update\n",
    "    \"\"\"\n",
    "\n",
    "    rbf = lambda x1,x2: RBF(x1, x2, lengthscales=lengthscales, kernel_variance=eta)\n",
    "    drbf = lambda x1,x2: dRBF(x1, x2, lengthscales=lengthscales, kernel_variance=eta)\n",
    "    ddrbf = lambda x1,x2: ddRBF(x1, x2, lengthscales=lengthscales, kernel_variance=eta)\n",
    "\n",
    "    ## Diagonal blocks\n",
    "    # Inducing points\n",
    "    Ku_u = rbf(z,z)\n",
    "    if (sig_u is not None):\n",
    "        Ku_u = Ku_u + np.diag((sig_u*np.ones((Ku_u.shape[0],1))).flatten()) # Noise is due to uncertainty in trans func\n",
    "\n",
    "\n",
    "\n",
    "    if s.shape[1] > 0:\n",
    "        # Fixed points\n",
    "        Ks_s = rbf(s,s)\n",
    "        if (sig_s is not None):\n",
    "            Ks_s = Ks_s + np.diag((sig_s*np.ones((Ks_s.shape[0],1))).flatten()) # Noise is due to uncertainty in fix point loc\n",
    "\n",
    "        # Jacobians at fixed points\n",
    "        KJ_J = ddrbf(s,s) # Derivatives are not affected by noisy numerics ?        \n",
    "        if (sig_J is not None):\n",
    "            KJ_J = KJ_J + np.diag((sig_J*np.ones((KJ_J.shape[0],1))).flatten()) # Noise is due to uncertainty in fix point loc\n",
    "\n",
    "        ## Off-diagonal blocks\n",
    "        # Fixed vs inducing\n",
    "        Ks_u = rbf(s, z)\n",
    "\n",
    "        # Jac vs inducing\n",
    "        KJ_u = drbf(s, z)\n",
    "\n",
    "        # Jac vs fixed\n",
    "        KJ_s = drbf(s, s)\n",
    "\n",
    "\n",
    "        ## Stack the matrices appropriately (3 x 3 blocks) # need - signs for derivative transposes?\n",
    "\n",
    "        K_full = np.concatenate(\n",
    "                           [np.concatenate([Ku_u, Ks_u, KJ_u]), \n",
    "                           np.concatenate([Ks_u.T, Ks_s, KJ_s]), \n",
    "                           np.concatenate([KJ_u.T, KJ_s.T, KJ_J])],\n",
    "                           axis=1)\n",
    "    else:\n",
    "        K_full = Ku_u\n",
    "\n",
    "    #return K_full\n",
    "\n",
    "    L = None\n",
    "    # Ensure K_full is positive definite\n",
    "    while L is None:\n",
    "        try:\n",
    "            L = np.linalg.cholesky(K_full)\n",
    "        except:\n",
    "            K_full = K_full + 1e-2*np.min(np.linalg.svd(K_full,compute_uv=False))*np.eye(K_full.shape[0])\n",
    "            L = None\n",
    "            print(\"Matrix is not positive definite, adding smallest sv to diagonal\")\n",
    "            set_trace()\n",
    "\n",
    "\n",
    "#         \n",
    "# #         # Transform J for appropriate lengthscale (columnwise multiple)\n",
    "# #         if not (lengthscales is None):\n",
    "# #             J_scaled = J / np.broadcast_to( (lengthscales[:,:,np.newaxis]).swapaxes(0,1).swapaxes(1,2), J.shape)\n",
    "# #             print J_scaled\n",
    "# #         else:\n",
    "#         J_scaled = J\n",
    "\n",
    "#         # Reshape J to be appropriate dimensions for the derivative kernel, \n",
    "#         # D-dim feature column vectors stacked vertically for each fixed point (DxM) x D output features as number of columns\n",
    "#         J_scaled = np.reshape(J_scaled.swapaxes(0,1),(-1, J.shape[1]))\n",
    "\n",
    "    if s.shape[1]>0:\n",
    "        # J is originally M fixed points x DxD jacobians (Jij is df_i(x)/d_xj, make it into (DinxM) x Dout\n",
    "        targets = np.concatenate([u.T, s.T, np.reshape(J.swapaxes(1,2),(-1, J.shape[1]), order='C')]) \n",
    "        # Sometimes called beta, (L + M + D_inxM) x D_out, where we will use D independent GPs, one for each output dimension \n",
    "        # (output and input dimensions are both D)\n",
    "    else:\n",
    "        targets = u.T\n",
    "\n",
    "    params = {'eta': eta, 'lengthscales': lengthscales, 'z': z, 'u': u, 's': s, 'J':J, 'sig_eps': sig_eps, 'sig_u': sig_u}\n",
    "\n",
    "    return (L, targets, params)\n",
    "        \n",
    "def fp_predict(xstar, L, targets, params):\n",
    "    # Separate predictions per output dimension\n",
    "    D = targets.shape[1]\n",
    "\n",
    "    # Compute the kstar kernels:\n",
    "    Kx_u = RBF(xstar, params['z'], lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "    Kx_s = RBF(xstar, params['s'], lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "    Kx_J = dRBF(params['s'], xstar, lengthscales=params['lengthscales'], kernel_variance=params['eta']).T\n",
    "    Kx_x = RBF(xstar, xstar, lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "\n",
    "    Kx_pred = np.concatenate([Kx_u, Kx_s, Kx_J], axis=1).T\n",
    "\n",
    "    # Compute the predictive mean per dimension then concatenate them\n",
    "#         Mu_star = np.concatenate([ \\\n",
    "#                 np.dot(Kx_pred.T, \n",
    "#                        scipy.linalg.solve_triangular(\n",
    "#                             L.T, scipy.linalg.solve_triangular(L, targets[:,d:(d+1)], lower=True),\n",
    "#                             lower=False)\n",
    "#                        ) \\\n",
    "#              for d in range(D)], axis=1)\n",
    "\n",
    "#         Mu_star = Mu_star.T\n",
    "\n",
    "    # Compute the predictive variance\n",
    "#         Sig_star = Kx_x - np.dot(Kx_pred.T, \n",
    "#                                  scipy.linalg.solve_triangular(\n",
    "#                                         L.T, scipy.linalg.solve_triangular(L, Kx_pred, lower=True),\n",
    "#                                         lower=False)\n",
    "#                                  )\n",
    "\n",
    "    Kinv = np.linalg.inv(np.dot(L,L.T))\n",
    "    Mu_star = np.atleast_2d(np.squeeze(np.dot(Kx_pred.T, np.dot(Kinv, targets)).T))\n",
    "\n",
    "    # Get the expected variance at each data point\n",
    "    Sig_star = Kx_x - np.dot(Kx_pred.T, np.dot(Kinv, Kx_pred))\n",
    "    Sig_star = np.atleast_1d(np.squeeze(np.diag(Sig_star)))\n",
    "\n",
    "    # Add the learned transition noise term\n",
    "    if (params['sig_eps'].size==1):\n",
    "        Sigma_eps_out = params['sig_eps']*np.eye(D)\n",
    "    elif (params['sig_eps'].size==D):\n",
    "        Sigma_eps_out = np.diag(params['sig_eps'].flatten())\n",
    "    else:\n",
    "        Sigma_eps_out = params['sig_eps']\n",
    "\n",
    "    # Add the terms as a 3rd order tensor (NxDxD)\n",
    "    Sig_star = (np.expand_dims(np.expand_dims(Sig_star, axis=1), axis=2) * np.expand_dims(np.eye(D), axis=0) \n",
    "                + np.expand_dims(Sigma_eps_out, axis=0))\n",
    "\n",
    "    return (Mu_star, Sig_star, Kx_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?scipy.linalg.solve_triangular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering of time series\n",
    "\n",
    "We apply the \"direct method\" from McHutchon thesis (2014) Chapter 3.6 to timeseries data filtering and parameter estimation via gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank1_matrix(A):\n",
    "    u,s,v = np.linalg.svd(A, full_matrices = False, compute_uv=True)\n",
    "    return np.atleast_2d(s[0]*np.dot(u[:,0:1], v[0:1,:]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D = 2\n",
    "# s = np.zeros((D,0))\n",
    "# J = np.zeros((0,D,D))\n",
    "\n",
    "# # Fixed points\n",
    "# Ks_s = RBF(s,s)\n",
    "\n",
    "# # Jacobians at fixed points\n",
    "# KJ_J = ddRBF(s,s) # Derivatives are not affected by noisy numerics ?        \n",
    "\n",
    "# Ks_s = Ks_s + np.diag((np.ones((Ks_s.shape[0],1))).flatten()) # Noise is due to numerical stability\n",
    "\n",
    "# # if (sig_J is not None):\n",
    "# #     KJ_J = KJ_J + np.diag((sig_J*np.ones((KJ_J.shape[0],1))).flatten()) # Noise is due to numerical stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_t_t1(mu_t1_t1, Sigma_t1_t1, L, targets, kernel_variance, Sigma_eps, z, u, lengthscales, s, J):\n",
    "    \"\"\"\n",
    "        Compute mu_t_t1 and Sigma_t_t1 given mu_t1_t1 and Sigma_t1_t1, as well as the parameters \n",
    "    \"\"\"\n",
    "    \n",
    "    D = mu_t1_t1.shape[0]\n",
    "    Nz = z.shape[1]\n",
    "    Ns = s.shape[1]\n",
    "    Nzs = Nz+Ns\n",
    "    NJ = D * s.shape[1]\n",
    "    \n",
    "    # K inverse matrix (Nu+Ns+D*Ns)^2 and targets (Nu+Ns+D*Ns)*D scaled\n",
    "    #Linv = scipy.linalg.solve_triangular(L, np.eye(L.shape[0]), lower=True)\n",
    "    \n",
    "    \n",
    "#     Linv = np.linalg.inv(L) # Numpy inv is by far the fastest for moderate matrices that I expect L to be\n",
    "#     Kinv_unstab = np.dot(Linv.T, Linv)\n",
    "\n",
    "    # Numerically stable Kinv using conditional pseudo-inverse\n",
    "    Kcur = np.dot(L, L.T)\n",
    "    if np.any(np.isnan(Kcur)):\n",
    "        set_trace()\n",
    "    if np.any(np.isinf(Kcur)):\n",
    "        set_trace()\n",
    "    svdu, svds, svdv = np.linalg.svd(Kcur, full_matrices = False, compute_uv=True)\n",
    "    select_evs = ((np.log(svds) - np.max(np.log(svds)))>(-10))\n",
    "    svdu = svdu[:,select_evs]\n",
    "    svds = svds[select_evs]\n",
    "    svdv = svdv[select_evs,:]\n",
    "    # This happens a lot actually\n",
    "#     if np.prod(select_evs)==0:\n",
    "#         print(\"Warning: Observation covariance is badly conditioned, using reduced rank pseudo-inverse\")\n",
    "        \n",
    "    # Compute the (possibly pseudo-) inverse\n",
    "    Kinv = np.dot(np.dot(svdu, np.diag(1./svds)), svdv)\n",
    "    \n",
    "#     if not np.allclose(Kinv_unstab, Kinv, atol=1e-8):\n",
    "#         print(\"Warning, Kinv is not stable\")\n",
    "#         # set_trace()\n",
    "    \n",
    "    alpha = np.dot(Kinv, targets)\n",
    "    \n",
    "    \n",
    "    # Expected kernels \n",
    "    X = np.concatenate([z, s],axis=1) # Base observation locations\n",
    "    dX = s # extra derivative observation locations\n",
    "    \n",
    "    eK_zs = RBF_eK(mu=mu_t1_t1, sigma=Sigma_t1_t1, X=X, lengthscales=lengthscales, kernel_variance=kernel_variance)\n",
    "        \n",
    "    eKK_zs_zs = RBF_eKK(mu=mu_t1_t1, sigma=Sigma_t1_t1, X=X, lengthscales=lengthscales, kernel_variance=kernel_variance)\n",
    "    \n",
    "    exKK_zs_zs = RBF_exKK(mu=mu_t1_t1, sigma=Sigma_t1_t1, X=X, \n",
    "                         lengthscales=lengthscales, kernel_variance=kernel_variance,\n",
    "                         eKK = eKK_zs_zs)\n",
    "    \n",
    "    # We will only use a part of this, we compute uneccessarily the derivatives at locations z.\n",
    "    eKdK_zs_zs = RBF_eKdK(mu=mu_t1_t1, sigma=Sigma_t1_t1, X=X, \n",
    "                          lengthscales=lengthscales, kernel_variance=kernel_variance,\n",
    "                          eKK = eKK_zs_zs, exKK = exKK_zs_zs)\n",
    "    \n",
    "    # Derivatives at fixed points\n",
    "    if NJ>0:\n",
    "        edK_s = RBF_edK(mu=mu_t1_t1, sigma=Sigma_t1_t1, X=dX, \n",
    "                    lengthscales=lengthscales, kernel_variance=kernel_variance,\n",
    "                    eK=eK_zs[:,-Ns:])\n",
    "\n",
    "        edKdK_s_s = RBF_edKdK(mu=mu_t1_t1, sigma=Sigma_t1_t1, X=dX, \n",
    "                              lengthscales=lengthscales, kernel_variance=kernel_variance,\n",
    "                              eKK = eKK_zs_zs[-Ns:,-Ns:], exKK = exKK_zs_zs[:,-Ns:,-Ns:])\n",
    "    \n",
    " \n",
    "    \n",
    "    # Get blocks\n",
    "    alpha_zs = alpha[0:Nzs,:]\n",
    "    Kinv_zs = Kinv[0:Nzs, 0:Nzs]\n",
    "    \n",
    "    if NJ>0:\n",
    "        alpha_J = alpha[-NJ:,:]\n",
    "        Kinv_J = Kinv[-NJ:, -NJ:]\n",
    "        Kinv_zs_J = Kinv[0:Nzs, -NJ:]\n",
    "        eKdK_zs_s = eKdK_zs_zs[:, -NJ:]\n",
    "    else:\n",
    "        alpha_J = np.zeros((0,alpha.shape[1]))\n",
    "        Kinv_J = np.zeros((0,0))\n",
    "        Kinv_zs_J = np.zeros((Nzs, 0))\n",
    "        eKdK_zs_s = np.zeros((Nzs, 0))\n",
    "    \n",
    "    # Compute the mean\n",
    "    mu_t_t1 = np.zeros((D,1))\n",
    "    \n",
    "    mu_t_t1 = np.dot( eK_zs , alpha_zs)\n",
    "    if NJ>0:\n",
    "        mu_t_t1 = mu_t_t1 + np.dot( edK_s , alpha_J)\n",
    "    \n",
    "    mu_t_t1 = mu_t_t1.T\n",
    "    \n",
    "#     mu_t_t1_noiseless = (\n",
    "#         np.dot(RBF(mu_t1_t1, X, lengthscales=lengthscales, kernel_variance=kernel_variance), alpha_zs)\n",
    "#         + np.dot(dRBF(dX, mu_t1_t1, lengthscales=lengthscales, kernel_variance=kernel_variance).T , alpha_J))\n",
    "    \n",
    "    # Compute output variances\n",
    "    \n",
    "    # Handle the simple (no fixed point) case separately:\n",
    "    if not NJ>0:\n",
    "        e_sigma_out = (\n",
    "            RBF(x=mu_t1_t1, lengthscales=lengthscales, kernel_variance=kernel_variance)\n",
    "            - np.trace(np.dot(Kinv_zs, eKK_zs_zs))\n",
    "        )\n",
    "        \n",
    "        if e_sigma_out <= 0:\n",
    "            set_trace()\n",
    "        \n",
    "        e_mu_muT_out = np.dot(np.dot(alpha_zs.T, eKK_zs_zs), alpha_zs)\n",
    "        \n",
    "    else: # There are fixed points\n",
    "    \n",
    "        # Shared for each dimensions (diagonal variances only or every entry??? Yes, expected covariance of indep GPs = 0):\n",
    "        # Expectation of the variance\n",
    "        e_sigma_out = (\n",
    "            RBF(x=mu_t1_t1, lengthscales=lengthscales, kernel_variance=kernel_variance)\n",
    "            - np.trace(np.dot(Kinv_zs, eKK_zs_zs))\n",
    "            - 2*np.trace(np.dot(Kinv_zs_J, eKdK_zs_s.T))\n",
    "            - np.trace(np.dot(Kinv_J, edKdK_s_s))\n",
    "        )\n",
    "\n",
    "        # This should also positive, if not, probably numerical error\n",
    "        if e_sigma_out <= 0:\n",
    "            e_sigma_out = 0.\n",
    "    #         print(\"Matrix inverse is less precise than our estimate of the variance, just set it to eps\")\n",
    "    #         if np.log(np.abs(e_sigma_out))>np.min(np.log(svds) - np.max(np.log(svds))):\n",
    "    #             # Matrix inverse is less precise than our estimate of the variance, just set it to predef prec value\n",
    "    #             # print(\"Matrix inverse is less precise than our estimate of the variance, just set it to eps\")\n",
    "    #             e_sigma_out = 0.\n",
    "    #         else:\n",
    "    #             # Something more serious is wrong, debug\n",
    "    #             set_trace()\n",
    "\n",
    "        # Variance of the expectation\n",
    "        # Dimension_specific (D x D matrix)\n",
    "        e_mu_muT_out = (\n",
    "            np.dot(np.dot(alpha_zs.T, eKK_zs_zs), alpha_zs)  \n",
    "            + np.dot(np.dot(alpha_zs.T, eKdK_zs_s), alpha_J)\n",
    "            + np.dot(np.dot(alpha_zs.T, eKdK_zs_s), alpha_J).T\n",
    "            + np.dot(np.dot(alpha_J.T, edKdK_s_s), alpha_J)\n",
    "        )\n",
    "        \n",
    "    # End of if (fixed points)\n",
    "    \n",
    "\n",
    "    m_mT = np.dot(mu_t_t1, mu_t_t1.T)\n",
    "\n",
    "    # The variance of the expectation is supposed to be positive semi-definite, delete the negative eigenvalues\n",
    "    var_e_mu_out = e_mu_muT_out - m_mT\n",
    "    evs, evecs = np.linalg.eigh(var_e_mu_out)\n",
    "    var_e_mu_out = np.zeros_like(var_e_mu_out)\n",
    "    for i1 in range(evs.size):\n",
    "        if evs[i1]>0:\n",
    "            var_e_mu_out = var_e_mu_out+ evs[i1]*np.dot(evecs[:,i1][:,None], evecs[:,i1][:,None].T)\n",
    "#         if evs[i1]==0:\n",
    "#             print(\"Zero eigenvalue!\")\n",
    "#         if evs[i1]<0:\n",
    "#             print(\"Negative eigenvalue!\")\n",
    "\n",
    "    # The learned transition noise term\n",
    "    if (Sigma_eps.size==1):\n",
    "        Sigma_eps_out = Sigma_eps*np.eye(D)\n",
    "    elif (Sigma_eps.size==D):\n",
    "        Sigma_eps_out = np.diag(Sigma_eps.flatten())\n",
    "    else:\n",
    "        Sigma_eps_out = Sigma_eps\n",
    "    \n",
    "    Sigma_t_t1 = e_sigma_out * np.eye(D) + var_e_mu_out + Sigma_eps_out\n",
    "    \n",
    "    if not np.allclose(Sigma_t_t1, Sigma_t_t1.T, atol=1e-8):\n",
    "        set_trace()\n",
    "        print(\"Warning: Filtered covariance is not symmetric\")\n",
    "        \n",
    "#     if np.min(np.linalg.eigvalsh(Sigma_t_t1)) < 0:\n",
    "#         set_trace()\n",
    "#         print(\"Warning: Filtered covariance is not positive definite\")\n",
    "    \n",
    "    return (mu_t_t1, Sigma_t_t1)\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def update_t_t(mu_t_t1, Sigma_t_t1, C, Sigma_nu, y_t):\n",
    "    \"\"\"\n",
    "        Compute mu_t_t and Sigma_t_t given mu_t_t1 and Sigma_t_t1, as well as the parameters \n",
    "        \n",
    "        Computes the negative log marginal likelihood\n",
    "        - log N(y_t; C*mu_t_t1, C * Sigma_t_t1 * C.T + Sigma_nu)\n",
    "    \"\"\"\n",
    "    \n",
    "    #set_trace()\n",
    "    mu_gauss = np.dot(C, mu_t_t1) \n",
    "    var_gauss = np.dot(np.dot(C, Sigma_t_t1), C.T) + np.diag(Sigma_nu.flatten())\n",
    "    \n",
    "    # Check for condition number, if high compared to expected precision in y (Sigma_nu), use pseudo-inverse and pseudo-determinant\n",
    "    prec = 10 #np.min(np.log(1./Sigma_nu))\n",
    "    # We are doing SVD anyway, use it to compute inverse\n",
    "    svdu, svds, svdv = np.linalg.svd(var_gauss, full_matrices = False, compute_uv=True)\n",
    "    # Debug checks\n",
    "    if not np.allclose(var_gauss, var_gauss.T, atol=1e-8):\n",
    "        set_trace()\n",
    "        print(\"Warning: Observation covariance is not symmetric\")\n",
    "#     evs = np.linalg.eigvals(var_gauss)\n",
    "#     if np.min(evs)<0:\n",
    "#         print(\"Warning: Observation covariance is not positive definite\")\n",
    "    evs = np.log(np.abs(svds))\n",
    "    if (np.max(evs) - np.min(evs)) > prec:\n",
    "        # Cut the \"too small\" eigenvalues, singular values, U columns and V rows \n",
    "        # (esentially to perform reduced rank pseudo-inverse and pseudo-determinant, based on expected precision of y)\n",
    "        select_evs = ((evs - np.max(evs))>(-prec))\n",
    "        svdu = svdu[:,select_evs]\n",
    "        svds = svds[select_evs]\n",
    "        svdv = svdv[select_evs,:]\n",
    "        evs = evs[select_evs]\n",
    "        # This happens a lot actually\n",
    "        # print(\"Warning: Observation covariance is badly conditioned, using reduced rank pseudo-inverse\")\n",
    "        \n",
    "    # Compute the (possibly pseudo-) inverse and determinant\n",
    "    inv_var_gauss = np.dot(np.dot(svdu, np.diag(1./svds)), svdv)\n",
    "    inv_var_gauss = (inv_var_gauss + inv_var_gauss.T) / 2.\n",
    "    ldet = np.sum(evs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    proj_var = np.dot(C, Sigma_t_t1)\n",
    "    proj_back = np.dot(Sigma_t_t1, np.dot(C.T, inv_var_gauss))\n",
    "    \n",
    "    mu_t_t = mu_t_t1 + np.dot(proj_back, (y_t - mu_gauss))\n",
    "    \n",
    "    Sigma_t_t = Sigma_t_t1 - np.dot(proj_back, proj_var)\n",
    "    \n",
    "    # Also compute negative marginal log likelihood contribution\n",
    "    D = mu_t_t1.shape[0]*1.0\n",
    "    \n",
    "    log_marg_ll = (\n",
    "        - 1.0* D/2.0*np.log(2*np.pi)\n",
    "        - 1.0/2.0*ldet\n",
    "        - 1.0/2.0*np.dot(np.dot( (y_t.T - mu_gauss.T), inv_var_gauss), (y_t - mu_gauss) )  \n",
    "    )\n",
    "    \n",
    "    return (-1.0*log_marg_ll, mu_t_t, Sigma_t_t)\n",
    "    \n",
    "    \n",
    "                                                        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the solver algorithm pieces\n",
    "\n",
    "# def nlog_marg_ll(mu_t_t1, Sigma_t_t1, C, Sigma_nu, y_t):\n",
    "#     \"\"\"\n",
    "#         Computes the negative log marginal likelihood\n",
    "#         - log N(y_t; C*mu_t_t1, C * Sigma_t_t1 * C.T + Sigma_nu)\n",
    "#     \"\"\"\n",
    "\n",
    "#     mu_gauss = np.dot(C, mu_t_t1) \n",
    "#     var_gauss = np.dot(np.dot(C, Sigma_t_t1), C.T) + np.diag(Sigma_nu.flatten())\n",
    "    \n",
    "#     # Check for condition number, if high compared to expected precision in y (Sigma_nu), use pseudo-inverse and pseudo-determinant\n",
    "#     prec = np.mean(np.log(1./Sigma_nu))\n",
    "#     prec = prec-1\n",
    "#     # We are doing SVD anyway, use it to compute inverse\n",
    "#     svdu, svds, svdv = np.linalg.svd(var_gauss, full_matrices = False, compute_uv=True)\n",
    "#     # Debug checks\n",
    "#     if not np.allclose(var_gauss, var_gauss.T, atol=1e-8):\n",
    "#         set_trace()\n",
    "#         print(\"Warning: Observation covariance is not symmetric\")\n",
    "# #     evs = np.linalg.eigvals(var_gauss)\n",
    "# #     if np.min(evs)<0:\n",
    "# #         print(\"Warning: Observation covariance is not positive definite\")\n",
    "#     evs = np.log(np.abs(svds))\n",
    "#     if (np.max(evs) - np.min(evs)) > prec:\n",
    "#         # Cut the \"too small\" eigenvalues, singular values, U columns and V rows \n",
    "#         # (esentially to perform reduced rank pseudo-inverse and pseudo-determinant, based on expected precision of y)\n",
    "#         select_evs = ((evs - np.max(evs))>(-prec))\n",
    "#         svdu = svdu[:,select_evs]\n",
    "#         svds = svds[select_evs]\n",
    "#         svdv = svdv[select_evs,:]\n",
    "#         evs = evs[select_evs]\n",
    "#         # This happens a lot actually\n",
    "#         # print(\"Warning: Observation covariance is badly conditioned, using reduced rank pseudo-inverse\")\n",
    "        \n",
    "#     # Compute the (possibly pseudo-) inverse and determinant\n",
    "#     inv_var_gauss = np.dot(np.dot(svdu, np.diag(1./svds)), svdv)\n",
    "#     inv_var_gauss = (inv_var_gauss + inv_var_gauss.T) / 2.\n",
    "#     ldet = np.sum(evs)\n",
    "\n",
    "        \n",
    "#     # Finish the computation\n",
    "#     D = mu_t_t1.shape[1]*1.0\n",
    "    \n",
    "#     log_marg_ll = (\n",
    "#         - 1.0* D/2.0*np.log(2*np.pi)\n",
    "#         - 1.0/2.0*ldet\n",
    "#         - 1.0/2.0*np.dot(np.dot( (y_t.T - mu_gauss.T), inv_var_gauss), (y_t - mu_gauss) )  \n",
    "#     )\n",
    "    \n",
    "#     return -1.0 * log_marg_ll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the optimisation algorithm\n",
    "\n",
    "Initialise parameters, define subsets to optimise, transforms (log space), and priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dy = 2\n",
    "# T = 7\n",
    "# Ny = 20\n",
    "# D = Dy\n",
    "# Nz = 2\n",
    "# Ns = 2\n",
    "\n",
    "# np.random.seed(123)\n",
    "# y = np.sqrt(0.3)*np.random.randn(Dy,T,Ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load 2D data\n",
    "# import scipy.io\n",
    "# from collections import OrderedDict\n",
    "# data_decision = scipy.io.loadmat('../Matlab/Machens_Brody/MachensBrodySim_20180202T131506_decision.mat', squeeze_me=True)\n",
    "# simParams = OrderedDict()\n",
    "# simParams['seed'] = 1234\n",
    "# simParams['curData'] = \"decision\"  # decision or loading\n",
    "# simParams['Ntrain'] = 20\n",
    "# simParams['delta_t'] = 1\n",
    "# simParams['T_max'] = min(50, data_decision['y_python'].shape[1])\n",
    "# simParams['Sigma_y'] = 1e-2\n",
    "# simParams['rescale'] = 1e3\n",
    "\n",
    "# curData = data_decision\n",
    "\n",
    "# x = curData['y_python'][:,range(0,simParams['T_max'],simParams['delta_t']),:simParams['Ntrain']]*simParams['rescale'] # Rescaling to reduce numerical instability\n",
    "# y = x + np.sqrt(simParams['Sigma_y'])*np.random.randn(*x.shape)\n",
    "# y.shape\n",
    "\n",
    "# init_params(y, 2, 30, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load 1D data\n",
    "# import scipy.optimize\n",
    "# import pickle\n",
    "# import pickle, datetime, time\n",
    "\n",
    "# curData = pickle.load(open('Experiment_1d_wells_results/well_1d_k2_20180103T181553_ 0_fix_ 20_trials.pkl'))\n",
    "# y=curData[0]\n",
    "# init_params(y,1,36,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D = 2\n",
    "# Ns = 0\n",
    "# Nz = 36\n",
    "    \n",
    "# Dy = y.shape[0]\n",
    "    \n",
    "# # If Dy = D, then just set \n",
    "# if D==Dy:\n",
    "#     C = np.eye(D, Dy)\n",
    "# else:\n",
    "#     # Do PCA and choose the first to find initial C (todo)\n",
    "#     C = np.eye(D, Dy)\n",
    "\n",
    "# # Estimate the noisyness of y via simple autoregressive AR(1) linear regression predictability, per dim\n",
    "# y_t = y[:,1:,:]\n",
    "# y_t1 = y[:,:-1,:]\n",
    "\n",
    "# y_t_rsh = np.reshape(y_t, (Dy, -1)).T\n",
    "# y_t1_rsh = np.reshape(y_t1, (Dy, -1)).T\n",
    "\n",
    "# # Get optimal linear estimate of y_t given the y_t-1 vector    \n",
    "# y_t_hat = np.dot(y_t1_rsh, np.dot(np.dot(np.linalg.inv(np.dot(y_t1_rsh.T, y_t1_rsh)), y_t1_rsh.T), y_t_rsh))\n",
    "\n",
    "# # Set the estimated variance of y_t\n",
    "# Sigma_nu = np.var(y_t_rsh - y_t_hat, axis = 0)[:,None].T\n",
    "# Sigma_nu = np.reshape(Sigma_nu, (-1,1))\n",
    "\n",
    "# # Split it into transition and observation noise\n",
    "# Sigma_eps = Sigma_nu/2.*np.sqrt(2.) # Transition noise\n",
    "# Sigma_nu = Sigma_nu/2.*np.sqrt(2.) # Observation noise\n",
    "\n",
    "# # Init GP params\n",
    "# # First run basic GP regression from y_t-1 -> y_t to estimate kernel hyperparams (skip for now, do simpler things)\n",
    "\n",
    "# # Set inducing point locations to span y backprojected through Cinv\n",
    "# if D==Dy:\n",
    "#     Cinv = np.eye(D, Dy)\n",
    "# else:\n",
    "#     Cinv = numpy.linalg.pinv(C)\n",
    "\n",
    "# Cinvy = np.tensordot(Cinv, y, axes = ([1], [0]))\n",
    "# grid_min = np.min(Cinvy, axis=(1,2))\n",
    "# grid_max = np.max(Cinvy, axis=(1,2))\n",
    "\n",
    "# grid_num = np.floor(Nz ** (1./D)) # Do an evenly spaced D-dim grid using floor(sqrt(Nz))**D points\n",
    "# remaining_num = Nz - grid_num**D # Initialise the other points randomly\n",
    "\n",
    "# grid_axes = []\n",
    "# for d in range(D):\n",
    "#     grid_axes.append(np.linspace(grid_min[d], grid_max[d], num=grid_num))\n",
    "\n",
    "# grid_dims = np.meshgrid(*grid_axes)\n",
    "# grid_dims_flat = []\n",
    "# for d in range(D):\n",
    "#     grid_dims_flat.append(grid_dims[d].flatten())\n",
    "# gridpts = np.array(grid_dims_flat)\n",
    "\n",
    "# np.random.seed(123)\n",
    "# if remaining_num > 0:\n",
    "#     randompts = np.random.rand(D,remaining_num.astype(int))*(grid_max-grid_min)[:,None]+grid_min[:,None]\n",
    "#     z = np.concatenate([gridpts, randompts], axis=1)\n",
    "# else:\n",
    "#     z = gridpts\n",
    "\n",
    "\n",
    "# # Estimate kernel lengthscale from grid size and number of inducing points\n",
    "# kernel_variance = np.array([[1.0]])\n",
    "# lengthscales=(grid_max-grid_min)[:,None]/grid_num * (2.*np.sqrt(D*1.)/2.) # Account for dimension scaling (sqrtD) also\n",
    "# lengthscales = lengthscales\n",
    "\n",
    "\n",
    "# # Initialise an empty inducing and fixed point arrays for now\n",
    "# u = np.zeros(z.shape)\n",
    "# Sigma_u = np.zeros((z.shape[1],1))\n",
    "# s = np.zeros((D,0))\n",
    "# J = np.zeros((0,D,D))\n",
    "# Sigma_s = np.zeros((0,1))\n",
    "# Sigma_J = np.zeros((0,1))    \n",
    "\n",
    "# # Gather backprojected data for AR coefficients\n",
    "# y_t = Cinvy[:,1:,:]\n",
    "# y_t1 = Cinvy[:,:-1,:]\n",
    "\n",
    "# y_t_rsh = np.reshape(y_t, (D, -1))\n",
    "# y_t1_rsh = np.reshape(y_t1, (D, -1))\n",
    "\n",
    "# # Run cross-validation on the lengthscales\n",
    "# test_lengthscales = np.zeros((D, 7))\n",
    "# for d in range(D):\n",
    "#     test_lengthscales[d,:] = np.array([1e-1, 5e-1, 1e0, 1.5e0, 2e0, 4e0, 8e0])*lengthscales[d]\n",
    "# lengthscales_perf = np.zeros((test_lengthscales.shape[1],))\n",
    "# for cv_folds in range(4):\n",
    "#     train_inds = np.array(random.sample(range(y_t1_rsh.shape[1]), np.int32(y_t1_rsh.shape[1]*3./4.)))\n",
    "#     test_inds = np.setdiff1d(np.arange(y_t1_rsh.shape[1]), train_inds)\n",
    "\n",
    "#     for lengthscale_ind in range(test_lengthscales.shape[1]):\n",
    "#         # Learn u and Sigma_u from training data\n",
    "#         L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=test_lengthscales[:,[lengthscale_ind]], \n",
    "#                                      z=y_t1_rsh[:,train_inds], u=y_t_rsh[:,train_inds], \n",
    "#                                      s=s, J=J, \n",
    "#                                      sig_eps=Sigma_eps, sig_u = np.sqrt(np.sum(Sigma_nu**2)/D)*np.ones((y_t1_rsh[:,train_inds].shape[1],1)),\n",
    "#                                      sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "#         mu_star, sig_star, K_pred = fp_predict(z, L, targets, params)\n",
    "#         u = mu_star \n",
    "#         for i in range(Sigma_u.shape[0]):\n",
    "#             Sigma_u[i] = (1./D)*np.sqrt(np.sum(np.diag(sig_star[i,:,:])**2))\n",
    "\n",
    "#         # Predict RMSE on test data\n",
    "#         L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=test_lengthscales[:,[lengthscale_ind]], \n",
    "#                                      z=z, u=u, \n",
    "#                                      s=s, J=J, \n",
    "#                                      sig_eps=Sigma_eps, sig_u = Sigma_u,\n",
    "#                                      sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "#         mu_star, sig_star, K_pred = fp_predict(y_t1_rsh[:,test_inds], L, targets, params)\n",
    "#         lengthscales_perf[lengthscale_ind] = lengthscales_perf[lengthscale_ind] + rmse(y_t_rsh[:,test_inds], mu_star)\n",
    "\n",
    "# lengthscales = test_lengthscales[:, [np.argmin(lengthscales_perf)]]\n",
    "\n",
    "# lengthscales_init = lengthscales\n",
    "\n",
    "# # Initialise inducing point values to GP-autoregression values\n",
    "# L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=lengthscales_init, \n",
    "#                                      z=y_t1_rsh, u=y_t_rsh, \n",
    "#                                      s=s, J=J, \n",
    "#                                      sig_eps=Sigma_eps, sig_u = np.sqrt(np.sum(Sigma_nu**2)/D)*np.ones((y_t1_rsh.shape[1],1)),\n",
    "#                                      sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "# mu_star, sig_star, K_pred = fp_predict(z, L, targets, params)\n",
    "# u = mu_star \n",
    "# for i in range(Sigma_u.shape[0]):\n",
    "#     Sigma_u[i] = (1./D)*np.sqrt(np.sum(np.diag(sig_star[i,:,:])**2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialise inducing point values to GP-autoregression values\n",
    "# Kt1t1 = RBF(y_t1_rsh, y_t1_rsh, kernel_variance=kernel_variance, lengthscales=lengthscales)\n",
    "# Kzt1 = RBF(z, y_t1_rsh, kernel_variance=kernel_variance, lengthscales=lengthscales)\n",
    "\n",
    "# # Do GP regression between the backprojected data points to estimate the values of u\n",
    "# u = np.random.randn(*z.shape)\n",
    "# for d1 in range(D):\n",
    "#     Kpredu = np.dot(Kzt1, np.linalg.inv(Kt1t1+np.diag((Sigma_eps[d1]*np.ones((Kt1t1.shape[0],))).flatten())))\n",
    "#     u[d1,:] = np.dot(Kpredu, y_t_rsh[d1,:])\n",
    "\n",
    "# Sigma_u = kernel_variance - np.diag(np.dot(Kpredu, Kzt1.T)) # Uncertainty in inducing points\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Learn a GP, Get probability of every possible fixed point under the GP prior\n",
    "# res_points = np.int32(2000**(1./D))\n",
    "# # Get the predictions at xstar values\n",
    "# gridList = [np.linspace(np.min(z[d,:]),np.max(z[d,:]),res_points) for d in range(D)]\n",
    "# tmp = np.meshgrid(*gridList)\n",
    "# xstar = np.concatenate([tmp[d].flatten()[:,None] for d in range(D)], axis=1).T\n",
    "\n",
    "# s=np.empty((D,0))\n",
    "# Sigma_s=np.empty((0,1))\n",
    "# J=np.empty((0,D,D))\n",
    "# Sigma_J=np.empty((0,1))\n",
    "\n",
    "# for new_s in range(Ns):\n",
    "#     L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=lengthscales, z=z, u=u, s=s, J=J, \n",
    "#                                          sig_eps=Sigma_eps, sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "#     mu_star, sig_star, K_pred = fp_predict(xstar, L, targets, params)\n",
    "\n",
    "#     # Get the marginal probabilities of fixed points given mu_star and sig_star\n",
    "#     fp_probs = np.zeros((mu_star.shape[1]))\n",
    "#     s_det_pp_prior = np.ones(*fp_probs.shape)\n",
    "#     for i in range(mu_star.shape[1]):\n",
    "#         cur_nll_term, mu_t1_t1, Sigma_t1_t1 = update_t_t(mu_star[:,i:i+1], sig_star[i,:,:], C, Sigma_nu, xstar[:,i:i+1])\n",
    "#         fp_probs[i] = np.exp(-1.0*cur_nll_term)\n",
    "#         # Add a determinental point process prior to the likelihoods\n",
    "#         Kxs_xs = RBF(np.concatenate([xstar[:,[i]], params['s']], axis=1), \n",
    "#                    np.concatenate([xstar[:,[i]], params['s']], axis=1),\n",
    "#                    lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "#         if Kxs_xs.size>0:\n",
    "#             s_det_pp_prior[i] = np.exp(np.linalg.slogdet(Kxs_xs)[1]*(1.0/(params['s'].shape[1]+1)))\n",
    "        \n",
    "    \n",
    "#     if D == 1:\n",
    "#         plt([plt_type.Scatter(x=np.squeeze(xstar),y=np.squeeze(fp_probs*s_det_pp_prior))])\n",
    "#     if D == 2:\n",
    "#         plt([plt_type.Contour(z=np.reshape(fp_probs*s_det_pp_prior, (res_points,res_points)))])\n",
    "    \n",
    "#     # Add the newly defined fixed point\n",
    "#     f_ind = np.argmax(fp_probs*s_det_pp_prior)\n",
    "#     s = np.append(s, xstar[:,[f_ind]], axis=1)\n",
    "#     Sigma_s = np.append(Sigma_s, \n",
    "#                         np.atleast_2d(np.mean(np.dot(np.abs(xstar[:,[f_ind]] - mu_star[:,[f_ind]]).T,np.linalg.inv(sig_star[i,:,:])))).T, \n",
    "#                         axis=0)    \n",
    "    \n",
    "#     # Get predictive information on the derivatives at that fixed point\n",
    "#     D = targets.shape[1]\n",
    "\n",
    "#     # Compute the kstar kernels:\n",
    "#     Kdx_u = dRBF(s[:,-1:], params['z'], lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "#     Kdx_s = dRBF(s[:,-1:], params['s'], lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "#     Kdx_J = ddRBF(params['s'], s[:,-1:], lengthscales=params['lengthscales'], kernel_variance=params['eta']).T\n",
    "#     Kdx_dx = ddRBF(s[:,-1:], s[:,-1:], lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "\n",
    "#     Kdx_pred = np.concatenate([Kdx_u, Kdx_s, Kdx_J], axis=1).T\n",
    "\n",
    "\n",
    "#     Kinv = np.linalg.inv(np.dot(L,L.T))\n",
    "#     Mu_star = np.atleast_2d(np.squeeze(np.dot(Kdx_pred.T, np.dot(Kinv, targets)))).T\n",
    "\n",
    "#     # Get the expected variance at each data point\n",
    "#     Sig_star = Kdx_dx - np.dot(Kdx_pred.T, np.dot(Kinv, Kdx_pred))\n",
    "#     Sig_star = np.atleast_2d(np.squeeze(np.diag(Sig_star))).T\n",
    "\n",
    "#     # Store them\n",
    "#     J = np.append(J, np.expand_dims(Mu_star, axis=0), axis=0)\n",
    "#     Sigma_J = np.append(Sigma_J, Sig_star, axis=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def rmse(predictions, targets, axis=None):\n",
    "    return np.sqrt(np.mean(((predictions - targets) ** 2), axis=axis))\n",
    "def init_params(y, D, Nz, Ns, grad_sigma=True):\n",
    "    \"\"\"\n",
    "    Initialise the model parameters given \n",
    "    - y (Dy x T x N): observations\n",
    "    - D (scalar): latent dimensionality\n",
    "    - Nz (scalar): number of inducing points\n",
    "    - Ns (scalar): number of fixed points\n",
    "    \n",
    "    Return them as a dictionary\n",
    "    \"\"\"    \n",
    "    \n",
    "    Dy = y.shape[0]\n",
    "    \n",
    "    # If Dy = D, then just set \n",
    "    if D==Dy:\n",
    "        C = np.eye(D, Dy)\n",
    "    else:\n",
    "        # Do PCA and choose the first to find initial C (todo)\n",
    "        #C = np.eye(D, Dy)\n",
    "        \n",
    "        # Fix C to the True C for now\n",
    "        C = np.zeros((Dy, D))\n",
    "        C[:int(Dy/2),0] = 1.\n",
    "        C[int(Dy/2):,1] = 1.\n",
    "        \n",
    "    # Estimate the noisyness of y via simple autoregressive AR(1) linear regression predictability, per dim\n",
    "    y_t = y[:,1:,:]\n",
    "    y_t1 = y[:,:-1,:]\n",
    "\n",
    "    y_t_rsh = np.reshape(y_t, (Dy, -1)).T\n",
    "    y_t1_rsh = np.reshape(y_t1, (Dy, -1)).T\n",
    "\n",
    "    # Get optimal linear estimate of y_t given the y_t-1 vector    \n",
    "    y_t_hat = np.dot(y_t1_rsh, np.dot(np.dot(np.linalg.inv(np.dot(y_t1_rsh.T, y_t1_rsh)), y_t1_rsh.T), y_t_rsh))\n",
    "\n",
    "    # Set the estimated variance of y_t\n",
    "    Sigma_nu = np.var(y_t_rsh - y_t_hat, axis = 0)[:,None].T\n",
    "    Sigma_nu = np.reshape(Sigma_nu, (-1,1))\n",
    "    \n",
    "    \n",
    "    if D==Dy:\n",
    "        Cinv = np.eye(D, Dy)\n",
    "    else:\n",
    "        Cinv = np.linalg.pinv(C)\n",
    "    \n",
    "    # Split it into transition and observation noise\n",
    "    Sigma_eps = np.dot(Cinv, Sigma_nu)/2.*np.sqrt(2.) # Transition noise\n",
    "    Sigma_nu = Sigma_nu/2.*np.sqrt(2.) # Observation noise\n",
    "    \n",
    "    # Init GP params\n",
    "    # First run basic GP regression from y_t-1 -> y_t to estimate kernel hyperparams (skip for now, do simpler things)\n",
    "    \n",
    "    # Set inducing point locations to span y backprojected through Cinv  \n",
    "    Cinvy = np.tensordot(Cinv, y, axes = ([1], [0]))\n",
    "    grid_min = np.min(Cinvy, axis=(1,2))\n",
    "    grid_max = np.max(Cinvy, axis=(1,2))\n",
    "    \n",
    "    grid_num = np.floor(Nz ** (1./D)) # Do an evenly spaced D-dim grid using floor(sqrt(Nz))**D points\n",
    "    remaining_num = Nz - grid_num**D # Initialise the other points randomly\n",
    "    \n",
    "    grid_axes = []\n",
    "    for d in range(D):\n",
    "        grid_axes.append(np.linspace(grid_min[d], grid_max[d], num=grid_num))\n",
    "\n",
    "    grid_dims = np.meshgrid(*grid_axes)\n",
    "    grid_dims_flat = []\n",
    "    for d in range(D):\n",
    "        grid_dims_flat.append(grid_dims[d].flatten())\n",
    "    gridpts = np.array(grid_dims_flat)\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    if remaining_num > 0:\n",
    "        randompts = np.random.rand(D,remaining_num.astype(int))*(grid_max-grid_min)[:,None]+grid_min[:,None]\n",
    "        z = np.concatenate([gridpts, randompts], axis=1)\n",
    "    else:\n",
    "        z = gridpts\n",
    "\n",
    "        \n",
    "    # Estimate kernel lengthscale from grid size and number of inducing points\n",
    "    kernel_variance = np.array([[1.0]])\n",
    "    lengthscales=(grid_max-grid_min)[:,None]/grid_num * (2.*np.sqrt(D*1.)/2.) # Account for dimension scaling (sqrtD) also\n",
    "    lengthscales = lengthscales\n",
    "        \n",
    "        \n",
    "    # Initialise an empty inducing and fixed point arrays for now\n",
    "    u = np.zeros(z.shape)\n",
    "    Sigma_u = np.zeros((z.shape[1],1))\n",
    "    s = np.zeros((D,0))\n",
    "    J = np.zeros((0,D,D))\n",
    "    Sigma_s = np.zeros((0,1))\n",
    "    Sigma_J = np.zeros((0,1))    \n",
    "    \n",
    "    # Gather backprojected data for AR coefficients\n",
    "    y_t = Cinvy[:,1:,:]\n",
    "    y_t1 = Cinvy[:,:-1,:]\n",
    "\n",
    "    y_t_rsh = np.reshape(y_t, (D, -1))\n",
    "    y_t1_rsh = np.reshape(y_t1, (D, -1))\n",
    "    \n",
    "    # Run cross-validation on the lengthscales\n",
    "    test_lengthscales = np.zeros((D, 7))\n",
    "    for d in range(D):\n",
    "        test_lengthscales[d,:] = np.array([1e-1, 5e-1, 1e0, 1.5e0, 2e0, 4e0, 8e0])*lengthscales[d]\n",
    "    lengthscales_perf = np.zeros((test_lengthscales.shape[1],))\n",
    "    for cv_folds in range(4):\n",
    "        train_inds = np.array(random.sample(range(y_t1_rsh.shape[1]), np.int32(y_t1_rsh.shape[1]*3./4.)))\n",
    "        test_inds = np.setdiff1d(np.arange(y_t1_rsh.shape[1]), train_inds)\n",
    "        \n",
    "        for lengthscale_ind in range(test_lengthscales.shape[1]):\n",
    "            # Learn u and Sigma_u from training data\n",
    "            L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=test_lengthscales[:,[lengthscale_ind]], \n",
    "                                         z=y_t1_rsh[:,train_inds], u=y_t_rsh[:,train_inds], \n",
    "                                         s=s, J=J, \n",
    "                                         sig_eps=Sigma_eps, sig_u = np.sqrt(np.sum(Sigma_nu**2)/D)*np.ones((y_t1_rsh[:,train_inds].shape[1],1)),\n",
    "                                         sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "            mu_star, sig_star, K_pred = fp_predict(z, L, targets, params)\n",
    "            u = mu_star \n",
    "            for i in range(Sigma_u.shape[0]):\n",
    "                Sigma_u[i] = (1./D)*np.sqrt(np.sum(np.diag(sig_star[i,:,:])**2))\n",
    "                \n",
    "            # Predict RMSE on test data\n",
    "            L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=test_lengthscales[:,[lengthscale_ind]], \n",
    "                                         z=z, u=u, \n",
    "                                         s=s, J=J, \n",
    "                                         sig_eps=Sigma_eps, sig_u = Sigma_u,\n",
    "                                         sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "            mu_star, sig_star, K_pred = fp_predict(y_t1_rsh[:,test_inds], L, targets, params)\n",
    "            lengthscales_perf[lengthscale_ind] = lengthscales_perf[lengthscale_ind] + rmse(y_t_rsh[:,test_inds], mu_star)\n",
    "            \n",
    "    lengthscales = test_lengthscales[:, [np.argmin(lengthscales_perf)]]\n",
    "        \n",
    "    lengthscales_init = lengthscales\n",
    "    \n",
    "    # Initialise inducing point values to GP-autoregression values\n",
    "    L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=lengthscales_init, \n",
    "                                         z=y_t1_rsh, u=y_t_rsh, \n",
    "                                         s=s, J=J, \n",
    "                                         sig_eps=Sigma_eps, sig_u = np.sqrt(np.sum(Sigma_nu**2)/D)*np.ones((y_t1_rsh.shape[1],1)),\n",
    "                                         sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "    mu_star, sig_star, K_pred = fp_predict(z, L, targets, params)\n",
    "    u = mu_star \n",
    "    for i in range(Sigma_u.shape[0]):\n",
    "        Sigma_u[i] = (1./D)*np.sqrt(np.sum(np.diag(sig_star[i,:,:])**2))\n",
    "    \n",
    "    if (Ns>0):\n",
    "        # Learn a GP, Get probability of every possible fixed point under the GP prior\n",
    "        res_points = np.int32(2000**(1./D))\n",
    "        # Get the predictions at xstar values\n",
    "        gridList = [np.linspace(np.min(z[d,:]),np.max(z[d,:]),res_points) for d in range(D)]\n",
    "        tmp = np.meshgrid(*gridList)\n",
    "        xstar = np.concatenate([tmp[d].flatten()[:,None] for d in range(D)], axis=1).T\n",
    "\n",
    "        for new_s in range(Ns):\n",
    "            L, targets, params = fp_get_static_K(eta=kernel_variance, \n",
    "                                                 lengthscales=lengthscales_init, \n",
    "                                             z=y_t1_rsh, u=y_t_rsh, \n",
    "                                             s=s, J=J, \n",
    "                                             sig_eps=Sigma_eps, sig_u = np.sqrt(np.sum(Sigma_nu**2)/D)*np.ones((y_t1_rsh.shape[1],1)),\n",
    "                                             sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "            mu_star, sig_star, K_pred = fp_predict(xstar, L, targets, params)\n",
    "\n",
    "            # Get the marginal probabilities of fixed points given mu_star and sig_star\n",
    "            fp_probs = np.zeros((mu_star.shape[1]))\n",
    "            s_det_pp_prior = np.ones(*fp_probs.shape)\n",
    "            for i in range(mu_star.shape[1]):\n",
    "                cur_nll_term, mu_t1_t1, Sigma_t1_t1 = update_t_t(\n",
    "                    mu_star[:,i:i+1], sig_star[i,:,:], C, Sigma_nu, np.dot(C, xstar)[:,i:i+1])\n",
    "                \n",
    "                fp_probs[i] = np.exp(-1.0*cur_nll_term)\n",
    "                # Add a determinental point process prior to the likelihoods\n",
    "                Kxs_xs = RBF(np.concatenate([xstar[:,[i]], params['s']], axis=1), \n",
    "                           np.concatenate([xstar[:,[i]], params['s']], axis=1),\n",
    "                           lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "                if Kxs_xs.size>0:\n",
    "                    s_det_pp_prior[i] = np.exp(np.linalg.slogdet(Kxs_xs)[1]*(1.0/(params['s'].shape[1]+1)))\n",
    "\n",
    "\n",
    "            #plt([plt_type.Contour(z=np.reshape(fp_probs*s_det_pp_prior, (res_points,res_points)))])\n",
    "\n",
    "            # Add the newly defined fixed point\n",
    "            f_ind = np.argmax(fp_probs*s_det_pp_prior)\n",
    "            s = np.append(s, xstar[:,[f_ind]], axis=1)\n",
    "            Sigma_s = np.append(Sigma_s, \n",
    "                                np.atleast_2d(np.mean(np.dot(np.abs(xstar[:,[f_ind]] - mu_star[:,[f_ind]]).T,np.linalg.inv(sig_star[f_ind,:,:])))).T, \n",
    "                                axis=0)    \n",
    "\n",
    "            # Get predictive information on the derivatives at that fixed point\n",
    "            D = targets.shape[1]\n",
    "\n",
    "            # Compute the kstar kernels:\n",
    "            Kdx_u = dRBF(s[:,-1:], params['z'], lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "            Kdx_s = dRBF(s[:,-1:], params['s'], lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "            Kdx_J = ddRBF(params['s'], s[:,-1:], lengthscales=params['lengthscales'], kernel_variance=params['eta']).T\n",
    "            Kdx_dx = ddRBF(s[:,-1:], s[:,-1:], lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "\n",
    "            Kdx_pred = np.concatenate([Kdx_u, Kdx_s, Kdx_J], axis=1).T\n",
    "\n",
    "\n",
    "            Kinv = np.linalg.inv(np.dot(L,L.T))\n",
    "            Mu_star = np.atleast_2d(np.squeeze(np.dot(Kdx_pred.T, np.dot(Kinv, targets)))).T\n",
    "\n",
    "            # Get the expected variance at each data point\n",
    "            Sig_star = Kdx_dx - np.dot(Kdx_pred.T, np.dot(Kinv, Kdx_pred))\n",
    "            Sig_star = np.atleast_2d(np.squeeze(np.diag(Sig_star))).T\n",
    "\n",
    "            # Store them\n",
    "            J = np.append(J, np.expand_dims(Mu_star, axis=0), axis=0)\n",
    "            if grad_sigma:\n",
    "                Sigma_J = np.append(Sigma_J, Sig_star, axis=0) #Real predicted Sigma_J\n",
    "            else:\n",
    "                Sigma_J = np.append(Sigma_J, 0.*np.ones(Sig_star.shape), axis=0) #Alternatively use noiseless gradient values for better optimisation\n",
    "            \n",
    "        # Reinitialise the u values, taking into account the fixed points we've placed\n",
    "        L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=lengthscales_init, \n",
    "                                             z=y_t1_rsh, u=y_t_rsh, \n",
    "                                             s=s, J=J, \n",
    "                                             sig_eps=Sigma_eps, sig_u = np.sqrt(np.sum(Sigma_nu**2)/D)*np.ones((y_t1_rsh.shape[1],1)),\n",
    "                                             sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "        mu_star, sig_star, K_pred = fp_predict(z, L, targets, params)\n",
    "        u = mu_star\n",
    "        for i in range(Sigma_u.shape[0]):\n",
    "            Sigma_u[i] = (1./D)*np.sqrt(np.sum(np.diag(sig_star[i,:,:])**2))\n",
    "\n",
    "    \n",
    "            \n",
    "#         ###### OLD VERSION\n",
    "#         ## Initialise fixed point locations from k-means clustering on the back-projected values (k_medians better though)\n",
    "#         # Transform the data appropriately, and whiten it (kmeans requires) then unwhiten\n",
    "#         yrsh = (np.reshape(Cinvy, (D,-1)).T)\n",
    "#         ystd = np.std(yrsh, axis=0,)\n",
    "#         s = (scipy.cluster.vq.kmeans(yrsh / np.expand_dims(ystd, axis=0), Ns)[0]*np.expand_dims(ystd, axis=0)).T\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Sigma_u = Sigma_u * 1.5 # To aid further optimization\n",
    "    \n",
    "    \n",
    "    # Init p(x_0) as expected backprojection of first time step\n",
    "    yrsh = Cinvy[:,0,:]\n",
    "    mu_0_0 = np.mean(yrsh, axis=1)[:, None]\n",
    "    Sigma_0_0 = np.var(yrsh, axis=1)[:, None]\n",
    "#    # Subtract the contribution from Sigma_nu (the linear regression explained observation noise)\n",
    "#     Sigma_0_0 = 1./(1./Sigma_0_0 - 1./Sigma_nu) # Need more checking\n",
    "    \n",
    "    \n",
    "    paramdict = OrderedDict()\n",
    "    paramdict['Sigma_eps'] = Sigma_eps\n",
    "    paramdict['mu_0_0'] = mu_0_0\n",
    "    paramdict['Sigma_0_0'] = Sigma_0_0\n",
    "    paramdict['C'] = C\n",
    "    paramdict['Sigma_nu'] = Sigma_nu\n",
    "    paramdict['z'] = z\n",
    "    paramdict['u'] = u\n",
    "    paramdict['Sigma_u'] = Sigma_u\n",
    "    paramdict['lengthscales'] = lengthscales\n",
    "    paramdict['kernel_variance'] = kernel_variance\n",
    "    paramdict['s'] = s \n",
    "    paramdict['J'] = J\n",
    "    paramdict['Sigma_s'] = Sigma_s\n",
    "    paramdict['Sigma_J'] = Sigma_J\n",
    "    \n",
    "    return paramdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def rmse(predictions, targets, axis=None):\n",
    "    return np.sqrt(np.mean(((predictions - targets) ** 2), axis=axis))\n",
    "def init_params_lastminute(y, D, Nz, Ns, grad_sigma=True, Sigma_nu_given=None, True_s_given=None):\n",
    "    \"\"\"\n",
    "    Initialise the model parameters given \n",
    "    - y (Dy x T x N): observations\n",
    "    - D (scalar): latent dimensionality\n",
    "    - Nz (scalar): number of inducing points\n",
    "    - Ns (scalar): number of fixed points\n",
    "    \n",
    "    Return them as a dictionary\n",
    "    \"\"\"    \n",
    "    \n",
    "    Dy = y.shape[0]\n",
    "    \n",
    "    # If Dy = D, then just set \n",
    "    if D==Dy:\n",
    "        C = np.eye(D, Dy)\n",
    "    else:\n",
    "        # Do PCA and choose the first to find initial C (todo)\n",
    "        C = np.eye(D, Dy)\n",
    "        \n",
    "    # Estimate the noisyness of y via simple autoregressive AR(1) linear regression predictability, per dim\n",
    "    y_t = y[:,1:,:]\n",
    "    y_t1 = y[:,:-1,:]\n",
    "\n",
    "    y_t_rsh = np.reshape(y_t, (Dy, -1)).T\n",
    "    y_t1_rsh = np.reshape(y_t1, (Dy, -1)).T\n",
    "\n",
    "    # Get optimal linear estimate of y_t given the y_t-1 vector    \n",
    "    y_t_hat = np.dot(y_t1_rsh, np.dot(np.dot(np.linalg.inv(np.dot(y_t1_rsh.T, y_t1_rsh)), y_t1_rsh.T), y_t_rsh))\n",
    "\n",
    "    # Set the estimated variance of y_t\n",
    "    Sigma_nu = np.var(y_t_rsh - y_t_hat, axis = 0)[:,None].T\n",
    "    Sigma_nu = np.reshape(Sigma_nu, (-1,1))\n",
    "    \n",
    "    # Split it into transition and observation noise\n",
    "    if Sigma_nu_given is None:\n",
    "        Sigma_eps = Sigma_nu/2. # Transition noise\n",
    "        Sigma_nu = Sigma_nu/2. # Observation noise\n",
    "    else:\n",
    "        # Assume observed noise is transition + observation with known observation\n",
    "        Sigma_eps = Sigma_nu - Sigma_nu_given\n",
    "        Sigma_nu = Sigma_nu_given\n",
    "    \n",
    "    # Init GP params\n",
    "    # First run basic GP regression from y_t-1 -> y_t to estimate kernel hyperparams (skip for now, do simpler things)\n",
    "    \n",
    "    # Set inducing point locations to span y backprojected through Cinv\n",
    "    if D==Dy:\n",
    "        Cinv = np.eye(D, Dy)\n",
    "    else:\n",
    "        Cinv = numpy.linalg.pinv(C)\n",
    "        \n",
    "    Cinvy = np.tensordot(Cinv, y, axes = ([1], [0]))\n",
    "    grid_min = np.min(Cinvy, axis=(1,2))\n",
    "    grid_max = np.max(Cinvy, axis=(1,2))\n",
    "    \n",
    "    grid_num = np.floor(Nz ** (1./D)) # Do an evenly spaced D-dim grid using floor(sqrt(Nz))**D points\n",
    "    remaining_num = Nz - grid_num**D # Initialise the other points randomly\n",
    "    \n",
    "    grid_axes = []\n",
    "    for d in range(D):\n",
    "        grid_axes.append(np.linspace(grid_min[d], grid_max[d], num=grid_num))\n",
    "\n",
    "    grid_dims = np.meshgrid(*grid_axes)\n",
    "    grid_dims_flat = []\n",
    "    for d in range(D):\n",
    "        grid_dims_flat.append(grid_dims[d].flatten())\n",
    "    gridpts = np.array(grid_dims_flat)\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    if remaining_num > 0:\n",
    "        randompts = np.random.rand(D,remaining_num.astype(int))*(grid_max-grid_min)[:,None]+grid_min[:,None]\n",
    "        z = np.concatenate([gridpts, randompts], axis=1)\n",
    "    else:\n",
    "        z = gridpts\n",
    "\n",
    "        \n",
    "    # Estimate kernel lengthscale from grid size and number of inducing points\n",
    "    kernel_variance = np.array([[1.0]])\n",
    "    lengthscales=(grid_max-grid_min)[:,None]/grid_num * (2.*np.sqrt(D*1.)/2.) # Account for dimension scaling (sqrtD) also\n",
    "    lengthscales = lengthscales\n",
    "        \n",
    "        \n",
    "    # Initialise an empty inducing and fixed point arrays for now\n",
    "    u = np.zeros(z.shape)\n",
    "    Sigma_u = np.zeros((z.shape[1],1))\n",
    "    s = np.zeros((D,0))\n",
    "    J = np.zeros((0,D,D))\n",
    "    Sigma_s = np.zeros((0,1))\n",
    "    Sigma_J = np.zeros((0,1))    \n",
    "    \n",
    "    # Gather backprojected data for AR coefficients\n",
    "    y_t = Cinvy[:,1:,:]\n",
    "    y_t1 = Cinvy[:,:-1,:]\n",
    "\n",
    "    y_t_rsh = np.reshape(y_t, (D, -1))\n",
    "    y_t1_rsh = np.reshape(y_t1, (D, -1))\n",
    "    \n",
    "    # Run cross-validation on the lengthscales\n",
    "    test_lengthscales = np.zeros((D, 7))\n",
    "    for d in range(D):\n",
    "        test_lengthscales[d,:] = np.array([1e-1, 5e-1, 1e0, 1.5e0, 2e0, 4e0, 8e0])*lengthscales[d]\n",
    "    lengthscales_perf = np.zeros((test_lengthscales.shape[1],))\n",
    "    for cv_folds in range(4):\n",
    "        train_inds = np.array(random.sample(range(y_t1_rsh.shape[1]), np.int32(y_t1_rsh.shape[1]*3./4.)))\n",
    "        test_inds = np.setdiff1d(np.arange(y_t1_rsh.shape[1]), train_inds)\n",
    "        \n",
    "        for lengthscale_ind in range(test_lengthscales.shape[1]):\n",
    "            # Learn u and Sigma_u from training data\n",
    "            L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=test_lengthscales[:,[lengthscale_ind]], \n",
    "                                         z=y_t1_rsh[:,train_inds], u=y_t_rsh[:,train_inds], \n",
    "                                         s=s, J=J, \n",
    "                                         sig_eps=Sigma_eps, sig_u = np.sqrt(np.sum(Sigma_nu**2)/D)*np.ones((y_t1_rsh[:,train_inds].shape[1],1)),\n",
    "                                         sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "#             mu_star, sig_star, K_pred = fp_predict(z, L, targets, params)\n",
    "#             u = mu_star \n",
    "#             for i in range(Sigma_u.shape[0]):\n",
    "#                 Sigma_u[i] = (1./D)*np.sqrt(np.sum(np.diag(sig_star[i,:,:])**2))\n",
    "                \n",
    "#             # Predict RMSE on test data\n",
    "#             L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=test_lengthscales[:,[lengthscale_ind]], \n",
    "#                                          z=z, u=u, \n",
    "#                                          s=s, J=J, \n",
    "#                                          sig_eps=Sigma_eps, sig_u = Sigma_u,\n",
    "#                                          sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "\n",
    "            # Get optimal lengthscale by directly predicting using the training data\n",
    "            mu_star, sig_star, K_pred = fp_predict(y_t1_rsh[:,test_inds], L, targets, params)\n",
    "            lengthscales_perf[lengthscale_ind] = lengthscales_perf[lengthscale_ind] + rmse(y_t_rsh[:,test_inds], mu_star)\n",
    "            \n",
    "    lengthscales = test_lengthscales[:, [np.argmin(lengthscales_perf)]]\n",
    "        \n",
    "    lengthscales_init = lengthscales\n",
    "    \n",
    "    # Initialise inducing point values to GP-autoregression values\n",
    "    L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=lengthscales_init, \n",
    "                                         z=y_t1_rsh, u=y_t_rsh, \n",
    "                                         s=s, J=J, \n",
    "                                         sig_eps=Sigma_eps, sig_u = np.sqrt(np.sum(Sigma_nu**2)/D)*np.ones((y_t1_rsh.shape[1],1)),\n",
    "                                         sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "    mu_star, sig_star, K_pred = fp_predict(z, L, targets, params)\n",
    "    u = mu_star \n",
    "    for i in range(Sigma_u.shape[0]):\n",
    "        Sigma_u[i] = (1./D)*np.sqrt(np.sum(np.diag(sig_star[i,:,:])**2))\n",
    "    \n",
    "    if (Ns>0):\n",
    "        # Learn a GP, Get probability of every possible fixed point under the GP prior\n",
    "        res_points = np.int32(2000**(1./D))\n",
    "        # Get the predictions at xstar values\n",
    "        gridList = [np.linspace(np.min(z[d,:]),np.max(z[d,:]),res_points) for d in range(D)]\n",
    "        tmp = np.meshgrid(*gridList)\n",
    "        xstar = np.concatenate([tmp[d].flatten()[:,None] for d in range(D)], axis=1).T\n",
    "\n",
    "        \n",
    "        L, targets, params = fp_get_static_K(eta=kernel_variance, \n",
    "                                         lengthscales=lengthscales_init, \n",
    "                                         z=y_t1_rsh, u=y_t_rsh, \n",
    "                                         s=s, J=J, \n",
    "                                         sig_eps=Sigma_eps, sig_u = np.sqrt(np.sum(Sigma_nu**2)/D)*np.ones((y_t1_rsh.shape[1],1)),\n",
    "                                         sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "        mu_star, sig_star, K_pred = fp_predict(xstar, L, targets, params)\n",
    "        for new_s in range(Ns):\n",
    "\n",
    "            # Get the marginal probabilities of fixed points given mu_star and sig_star\n",
    "            fp_probs = np.zeros((mu_star.shape[1]))\n",
    "            s_det_pp_prior = np.ones(*fp_probs.shape)\n",
    "            for i in range(mu_star.shape[1]):\n",
    "                cur_nll_term, mu_t1_t1, Sigma_t1_t1 = update_t_t(mu_star[:,i:i+1], sig_star[i,:,:], C, Sigma_nu, xstar[:,i:i+1])\n",
    "                fp_probs[i] = np.exp(-1.0*cur_nll_term)\n",
    "                # Add a determinental point process prior to the likelihoods\n",
    "                Kxs_xs = RBF(np.concatenate([xstar[:,[i]], params['s']], axis=1), \n",
    "                           np.concatenate([xstar[:,[i]], params['s']], axis=1),\n",
    "                           lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "                if Kxs_xs.size>0:\n",
    "                    s_det_pp_prior[i] = np.exp(np.linalg.slogdet(Kxs_xs)[1]*(1.0/(params['s'].shape[1]+1)))\n",
    "\n",
    "\n",
    "            #plt([plt_type.Contour(z=np.reshape(fp_probs*s_det_pp_prior, (res_points,res_points)))])\n",
    "\n",
    "            # Add the newly defined fixed point\n",
    "            f_ind = np.argmax(fp_probs*s_det_pp_prior)\n",
    "            \n",
    "            if True_s_given is not None:\n",
    "                s = np.append(s, True_s_given[:,[new_s]])\n",
    "                Sigma_s = np.append(Sigma_s, 1e-12*np.ones((1,1)), axis=0)\n",
    "            else:\n",
    "                s = np.append(s, xstar[:,[f_ind]], axis=1)\n",
    "                Sigma_s = np.append(Sigma_s, \n",
    "                                    np.atleast_2d(np.mean(np.dot(np.abs(xstar[:,[f_ind]] - mu_star[:,[f_ind]]).T,np.linalg.inv(sig_star[f_ind,:,:])))).T, \n",
    "                                    axis=0)    \n",
    "\n",
    "            # Get predictive information on the derivatives at that fixed point\n",
    "            D = targets.shape[1]\n",
    "\n",
    "            # Compute the kstar kernels:\n",
    "            Kdx_u = dRBF(s[:,-1:], params['z'], lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "            Kdx_s = dRBF(s[:,-1:], params['s'], lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "            Kdx_J = ddRBF(params['s'], s[:,-1:], lengthscales=params['lengthscales'], kernel_variance=params['eta']).T\n",
    "            Kdx_dx = ddRBF(s[:,-1:], s[:,-1:], lengthscales=params['lengthscales'], kernel_variance=params['eta'])\n",
    "\n",
    "            Kdx_pred = np.concatenate([Kdx_u, Kdx_s, Kdx_J], axis=1).T\n",
    "\n",
    "\n",
    "            Kinv = np.linalg.inv(np.dot(L,L.T))\n",
    "            Mu_star = np.atleast_2d(np.squeeze(np.dot(Kdx_pred.T, np.dot(Kinv, targets)))).T\n",
    "\n",
    "            # Get the expected variance at each data point\n",
    "            Sig_star = Kdx_dx - np.dot(Kdx_pred.T, np.dot(Kinv, Kdx_pred))\n",
    "            Sig_star = np.atleast_2d(np.squeeze(np.diag(Sig_star))).T\n",
    "\n",
    "            # Store them\n",
    "            J = np.append(J, np.expand_dims(Mu_star, axis=0), axis=0)\n",
    "            if grad_sigma:\n",
    "                Sigma_J = np.append(Sigma_J, Sig_star, axis=0) #Real predicted Sigma_J\n",
    "            else:\n",
    "                Sigma_J = np.append(Sigma_J, 0.*np.ones(Sig_star.shape), axis=0) #Alternatively use noiseless gradient values for better optimisation\n",
    "            \n",
    "        # Reinitialise the u values, taking into account the fixed points we've placed\n",
    "        L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=lengthscales_init, \n",
    "                                             z=y_t1_rsh, u=y_t_rsh, \n",
    "                                             s=s, J=J, \n",
    "                                             sig_eps=Sigma_eps, sig_u = np.sqrt(np.sum(Sigma_nu**2)/D)*np.ones((y_t1_rsh.shape[1],1)),\n",
    "                                             sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "        mu_star, sig_star, K_pred = fp_predict(z, L, targets, params)\n",
    "        u = mu_star\n",
    "        for i in range(Sigma_u.shape[0]):\n",
    "            Sigma_u[i] = (1./D)*np.sqrt(np.sum(np.diag(sig_star[i,:,:])**2))\n",
    "\n",
    "    \n",
    "            \n",
    "#         ###### OLD VERSION\n",
    "#         ## Initialise fixed point locations from k-means clustering on the back-projected values (k_medians better though)\n",
    "#         # Transform the data appropriately, and whiten it (kmeans requires) then unwhiten\n",
    "#         yrsh = (np.reshape(Cinvy, (D,-1)).T)\n",
    "#         ystd = np.std(yrsh, axis=0,)\n",
    "#         s = (scipy.cluster.vq.kmeans(yrsh / np.expand_dims(ystd, axis=0), Ns)[0]*np.expand_dims(ystd, axis=0)).T\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Sigma_u = Sigma_u * 1.5 # To aid further optimization\n",
    "    \n",
    "    \n",
    "    # Init p(x_0) as expected backprojection of first time step\n",
    "    yrsh = Cinvy[:,0,:]\n",
    "    mu_0_0 = np.mean(yrsh, axis=1)[:, None]\n",
    "    Sigma_0_0 = np.var(yrsh, axis=1)[:, None]\n",
    "#    # Subtract the contribution from Sigma_nu (the linear regression explained observation noise)\n",
    "#     Sigma_0_0 = 1./(1./Sigma_0_0 - 1./Sigma_nu) # Need more checking\n",
    "    \n",
    "    \n",
    "    paramdict = OrderedDict()\n",
    "    paramdict['Sigma_eps'] = Sigma_eps\n",
    "    paramdict['mu_0_0'] = mu_0_0\n",
    "    paramdict['Sigma_0_0'] = Sigma_0_0\n",
    "    paramdict['C'] = C\n",
    "    paramdict['Sigma_nu'] = Sigma_nu\n",
    "    paramdict['z'] = z\n",
    "    paramdict['u'] = u\n",
    "    paramdict['Sigma_u'] = Sigma_u\n",
    "    paramdict['lengthscales'] = lengthscales\n",
    "    paramdict['kernel_variance'] = kernel_variance\n",
    "    paramdict['s'] = s \n",
    "    paramdict['J'] = J\n",
    "    paramdict['Sigma_s'] = Sigma_s\n",
    "    paramdict['Sigma_J'] = Sigma_J\n",
    "    \n",
    "    return paramdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter transformations\n",
    "def transform(paramvec, transform_type, transform_inds=None):\n",
    "    if transform_inds is None:\n",
    "        transform_inds = np.arange(len(paramvec))\n",
    "    \n",
    "    inds = (np.arange(paramvec.shape[0]))\n",
    "    inds = np.setdiff1d(inds,transform_inds)\n",
    "    extra_params = 0\n",
    "    \n",
    "    # Do the transformation from optimised parameter space to actual parameter space\n",
    "    if transform_type == \"Exp\":\n",
    "        out_transformed = np.exp(paramvec[transform_inds])\n",
    "    elif transform_type == \"LogExp\":\n",
    "        out_transformed = np.log1p(np.exp(paramvec[transform_inds]))\n",
    "    elif transform_type == \"Square\":\n",
    "        out_transformed = paramvec[transform_inds]**2\n",
    "    elif transform_type == \"SquareMatrix\":\n",
    "        # Cholesky representation has n_params = (N**2.-N)/2. + N free parameters, \n",
    "        # so N**2 + N - 2*n_params=0, so N = 1/2*(-1 + np.sqrt(1+8*n_params))\n",
    "        # Afterwards we fill in a matrix's lower triangle (in weird autograd compatible way) with our params\n",
    "        # Then do lowerTriMatrix * lowerTriMatrix.T\n",
    "        n_params = len(transform_inds)\n",
    "        N =np.int64(np.round(0.5*(-1 + np.sqrt(1+8*n_params))))\n",
    "        inds_nonzero = np.ravel_multi_index(np.tril_indices(N),(N,N))\n",
    "        inds_zero = np.arange(N**2)\n",
    "        inds_zero = np.setdiff1d(inds_zero,inds_nonzero)\n",
    "        lowerTriMatrix = np.reshape(\n",
    "            np.concatenate([paramvec[transform_inds], \n",
    "                           np.zeros((N**2-n_params,))]\n",
    "                          )[np.argsort(np.concatenate([inds_nonzero, inds_zero]))],\n",
    "                   (N,N))\n",
    "        out_transformed = np.dot(lowerTriMatrix, lowerTriMatrix.T).flatten()\n",
    "        extra_params = out_transformed.size - paramvec[transform_inds].size\n",
    " \n",
    "    # Fill in the parameter vector accordingly\n",
    "    out = np.concatenate([out_transformed, paramvec[inds]])\n",
    "    \n",
    "    # Make space for the extra parameters\n",
    "    inds = np.concatenate([inds[inds<np.min(transform_inds)], inds[inds>np.max(transform_inds)]+extra_params])\n",
    "    if extra_params > 0:\n",
    "        transform_inds_out = np.concatenate([transform_inds, np.arange(extra_params)+np.max(transform_inds)])\n",
    "    elif extra_params < 0:\n",
    "        transform_inds_out = transform_inds[np.arange(len(transform_inds)+extra_params)]\n",
    "    else:\n",
    "        transform_inds_out = transform_inds\n",
    "        \n",
    "    # Return the modified parameter vector\n",
    "    out = out[np.argsort(np.concatenate([transform_inds_out, inds]))]\n",
    "    \n",
    "    return (out, transform_inds_out)\n",
    "\n",
    "def transform_inv(paramvec, transform_type, transform_inds=None):     \n",
    "    if transform_inds is None:\n",
    "        transform_inds = np.arange(len(paramvec))\n",
    "    \n",
    "    inds = (np.arange(paramvec.shape[0]))\n",
    "    inds = np.setdiff1d(inds, transform_inds)\n",
    "    extra_params = 0\n",
    "    \n",
    "    # Do the transformation from optimised parameter space to actual parameter space\n",
    "    if transform_type == \"Exp\":\n",
    "        out_inv_transformed = np.log(paramvec[transform_inds])\n",
    "    elif transform_type == \"LogExp\":\n",
    "        out_inv_transformed = np.log(np.expm1(paramvec[transform_inds]))\n",
    "    elif transform_type == \"Square\":\n",
    "        out_inv_transformed = np.sqrt(paramvec[transform_inds])\n",
    "    elif transform_type == \"SquareMatrix\":\n",
    "        # During the inverse transform, we just have to do Cholesky, then extract the lower triangle\n",
    "        N = np.int64(np.sqrt(len(transform_inds)))\n",
    "        fullMatrix = np.reshape(paramvec[transform_inds],(N,-1))\n",
    "        L = np.linalg.cholesky(fullMatrix)\n",
    "        out_inv_transformed = L.flatten()[np.ravel_multi_index(np.tril_indices(N),(N,N))]\n",
    "        extra_params = out_inv_transformed.size - len(transform_inds)\n",
    " \n",
    "    # Fill in the parameter vector accordingly\n",
    "    out = np.concatenate([out_inv_transformed, paramvec[inds]])\n",
    "    \n",
    "    # Make space for the extra parameters\n",
    "    inds = np.concatenate([inds[inds<np.min(transform_inds)], inds[inds>np.max(transform_inds)]+extra_params])\n",
    "    if extra_params > 0:\n",
    "        transform_inds_out = np.concatenate([transform_inds, np.arange(extra_params)+np.max(transform_inds)])\n",
    "    elif extra_params < 0:\n",
    "        transform_inds_out = transform_inds[np.arange(len(transform_inds)+extra_params)]\n",
    "    else:\n",
    "        transform_inds_out = transform_inds\n",
    "    \n",
    "    out = out[np.argsort(np.concatenate([transform_inds_out, inds]))]\n",
    "    \n",
    "    return (out, transform_inds_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise and transform parameters for optimiser\n",
    "def params_to_vec(paramdict, transforms={}):\n",
    "    \"\"\"\n",
    "    Return all the parameters vectorised into a flattened vector, but also a dictionary of parameter lengths\n",
    "    Transforms the parameters to optimiser representation, whose names are in the transforms dictionary\n",
    "    \"\"\"\n",
    "    paramvec = []\n",
    "    paramdict_ind = OrderedDict()\n",
    "    paramdict_shape = OrderedDict()\n",
    "    \n",
    "    cur_ind = 0\n",
    "    \n",
    "    # Add \"unexpected\" extra parameters via kwargs at the end of the parameter vector \n",
    "    for arg_name in paramdict:\n",
    "        paramdict_shape[arg_name] = paramdict[arg_name].shape\n",
    "        if np.prod(paramdict_shape[arg_name]) > 0:\n",
    "            if arg_name in transforms:\n",
    "                if \"inds\" in transforms[arg_name]:\n",
    "                    transform_inds = transforms[arg_name]['inds']\n",
    "                else:\n",
    "                    transform_inds = None\n",
    "                cur_arg = transform_inv(paramdict[arg_name], transforms[arg_name]['type'])[0]\n",
    "            else:\n",
    "                cur_arg = paramdict[arg_name]\n",
    "        else:\n",
    "            cur_arg = paramdict[arg_name]\n",
    "            \n",
    "        paramdict_ind[arg_name] = np.arange(cur_ind, cur_ind+cur_arg.size)\n",
    "        cur_ind = cur_ind+cur_arg.size\n",
    "        paramvec.append(np.reshape(cur_arg,(-1,1)))\n",
    "        \n",
    "    paramvec = np.concatenate(paramvec, axis=0).flatten()\n",
    "    \n",
    "    return (paramvec, paramdict_ind, paramdict_shape)\n",
    "\n",
    "def vec_to_params(paramvec, paramdict_ind, paramdict_shape, transforms={}):\n",
    "    \"\"\"\n",
    "    Return all the parameters in their original forms as a dictionary\n",
    "    Transforms the parameters to natural representation, whose names are in the transforms dictionary\n",
    "    \"\"\"\n",
    "    out = OrderedDict()\n",
    "    \n",
    "    cur_ind = 0\n",
    "    for arg_name in paramdict_shape.keys():\n",
    "        if np.prod(paramdict_shape[arg_name]) > 0:\n",
    "            if arg_name in transforms:\n",
    "                if \"inds\" in transforms[arg_name]:\n",
    "                    transform_inds = transforms[arg_name]['inds']\n",
    "                else:\n",
    "                    transform_inds = None\n",
    "                cur_arg = transform(paramvec[paramdict_ind[arg_name]], transforms[arg_name]['type'])[0]\n",
    "            else:\n",
    "                cur_arg = paramvec[paramdict_ind[arg_name]]\n",
    "        else:\n",
    "            cur_arg = np.array([])\n",
    "        \n",
    "        out[arg_name] = np.reshape(cur_arg, paramdict_shape[arg_name])\n",
    "        \n",
    "    return out\n",
    "\n",
    "# def params_to_vec(Sigma_eps, mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_u, lengthscales, kernel_variance, s, J, **kwargs):\n",
    "#     \"\"\"\n",
    "#     Return all the parameters vectorised into a flattened vector, but also a dictionary of parameter lengths\n",
    "#     \"\"\"\n",
    "#     paramvec = []\n",
    "#     paramdict_ind = OrderedDict()\n",
    "#     paramdict_shape = OrderedDict()\n",
    "    \n",
    "#     paramdict_shape['Sigma_eps'] = Sigma_eps.shape\n",
    "#     paramdict_shape['mu_0_0'] = mu_0_0.shape\n",
    "#     paramdict_shape['Sigma_0_0'] = Sigma_0_0.shape\n",
    "#     paramdict_shape['C'] = C.shape\n",
    "#     paramdict_shape['Sigma_nu'] = Sigma_nu.shape\n",
    "#     paramdict_shape['z'] = z.shape\n",
    "#     paramdict_shape['u'] = u.shape\n",
    "#     paramdict_shape['Sigma_u'] = Sigma_u.shape\n",
    "#     paramdict_shape['lengthscales'] = lengthscales.shape\n",
    "#     paramdict_shape['kernel_variance'] = kernel_variance.shape\n",
    "#     paramdict_shape['s'] = s.shape \n",
    "#     paramdict_shape['J'] = J.shape\n",
    "    \n",
    "#     cur_ind = 0\n",
    "#     for key in paramdict_shape.keys():\n",
    "#         paramdict_ind[key] = np.arange(cur_ind, cur_ind+np.prod(paramdict_shape[key]))\n",
    "#         cur_ind = cur_ind+np.prod(paramdict_shape[key])\n",
    "#         paramvec.append(np.reshape(eval(key),(-1,1)))\n",
    "    \n",
    "#     # Add \"unexpected\" extra parameters via kwargs at the end of the parameter vector \n",
    "#     for arg_name in kwargs:\n",
    "#         paramdict_shape[arg_name] = kwargs[arg_name].shape\n",
    "#         paramdict_ind[arg_name] = np.arange(cur_ind, cur_ind+np.prod(paramdict_shape[arg_name]))\n",
    "#         cur_ind = cur_ind+np.prod(paramdict_shape[arg_name])\n",
    "#         paramvec.append(np.reshape(kwargs[arg_name],(-1,1)))\n",
    "        \n",
    "    \n",
    "#     paramvec = np.concatenate(paramvec, axis=0).flatten()\n",
    "    \n",
    "#     return (paramvec, paramdict_ind, paramdict_shape)\n",
    "\n",
    "# def vec_to_params(paramvec, paramdict_ind, paramdict_shape):\n",
    "#     out = []\n",
    "    \n",
    "#     cur_ind = 0\n",
    "#     for key in paramdict_shape.keys():\n",
    "#         out.append(np.reshape(paramvec[paramdict_ind[key]], paramdict_shape[key]))\n",
    "        \n",
    "#     return tuple(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parameter substitution to enable optimising only a subset of params\n",
    "\n",
    "def replace_params(pvec, opt_params, paramvec):\n",
    "    inds = (np.arange(paramvec.shape[0]))\n",
    "    inds = np.setdiff1d(inds,opt_params)\n",
    "    out = np.concatenate([pvec, paramvec[inds]])\n",
    "    out = out[np.argsort(np.concatenate([opt_params, inds]))]\n",
    "    return out\n",
    "\n",
    "# def transform(paramvec, transform_type, transform_inds):\n",
    "#     inds = (np.arange(paramvec.shape[0]))\n",
    "#     inds = np.setdiff1d(inds,transform_inds)\n",
    "#     extra_params = 0\n",
    "    \n",
    "#     # Do the transformation from optimised parameter space to actual parameter space\n",
    "#     if transform_type == \"Exp\":\n",
    "#         out_transformed = np.exp(paramvec[transform_inds])\n",
    "#     elif transform_type == \"LogExp\":\n",
    "#         out_transformed = np.log1p(np.exp(paramvec[transform_inds]))\n",
    "#     elif transform_type == \"Square\":\n",
    "#         out_transformed = paramvec[transform_inds]**2\n",
    "#     elif transform_type == \"SquareMatrix\":\n",
    "#         # Cholesky representation has n_params = (N**2.-N)/2. + N free parameters, \n",
    "#         # so N**2 + N - 2*n_params=0, so N = 1/2*(-1 + np.sqrt(1+8*n_params))\n",
    "#         # Afterwards we fill in a matrix's lower triangle (in weird autograd compatible way) with our params\n",
    "#         # Then do lowerTriMatrix * lowerTriMatrix.T\n",
    "#         n_params = len(transform_inds)\n",
    "#         N =np.int64(np.round(0.5*(-1 + np.sqrt(1+8*n_params))))\n",
    "#         inds_nonzero = np.ravel_multi_index(np.tril_indices(N),(N,N))\n",
    "#         inds_zero = np.arange(N**2)\n",
    "#         inds_zero = np.setdiff1d(inds_zero,inds_nonzero)\n",
    "#         lowerTriMatrix = np.reshape(\n",
    "#             np.concatenate([paramvec[transform_inds], \n",
    "#                            np.zeros((N**2-n_params,))]\n",
    "#                           )[np.argsort(np.concatenate([inds_nonzero, inds_zero]))],\n",
    "#                    (N,N))\n",
    "#         out_transformed = np.dot(lowerTriMatrix, lowerTriMatrix.T).flatten()\n",
    "#         extra_params = out_transformed.size - paramvec[transform_inds].size\n",
    " \n",
    "#     # Fill in the parameter vector accordingly\n",
    "#     out = np.concatenate([out_transformed, paramvec[inds]])\n",
    "    \n",
    "#     # Make space for the extra parameters\n",
    "#     inds = np.concatenate([inds[inds<np.min(transform_inds)], inds[inds>np.max(transform_inds)]+extra_params])\n",
    "#     if extra_params > 0:\n",
    "#         transform_inds_out = np.concatenate([transform_inds, np.arange(extra_params)+np.max(transform_inds)])\n",
    "#     elif extra_params < 0:\n",
    "#         transform_inds_out = transform_inds[np.arange(len(transform_inds)+extra_params)]\n",
    "#     else:\n",
    "#         transform_inds_out = transform_inds\n",
    "        \n",
    "#     # Return the modified parameter vector\n",
    "#     out = out[np.argsort(np.concatenate([transform_inds_out, inds]))]\n",
    "    \n",
    "#     return (out, transform_inds_out)\n",
    "\n",
    "# def transform_inv(paramvec, transform_type, transform_inds):        \n",
    "#     inds = (np.arange(paramvec.shape[0]))\n",
    "#     inds = np.setdiff1d(inds, transform_inds)\n",
    "#     extra_params = 0\n",
    "    \n",
    "#     # Do the transformation from optimised parameter space to actual parameter space\n",
    "#     if transform_type == \"Exp\":\n",
    "#         out_inv_transformed = np.log(paramvec[transform_inds])\n",
    "#     elif transform_type == \"LogExp\":\n",
    "#         out_inv_transformed = np.log(np.expm1(paramvec[transform_inds]))\n",
    "#     elif transform_type == \"Square\":\n",
    "#         out_inv_transformed = np.sqrt(paramvec[transform_inds])\n",
    "#     elif transform_type == \"SquareMatrix\":\n",
    "#         # During the inverse transform, we just have to do Cholesky, then extract the lower triangle\n",
    "#         N = np.int64(np.sqrt(len(transform_inds)))\n",
    "#         fullMatrix = np.reshape(paramvec[transform_inds],(N,-1))\n",
    "#         L = np.linalg.cholesky(fullMatrix)\n",
    "#         out_inv_transformed = L.flatten()[np.ravel_multi_index(np.tril_indices(N),(N,N))]\n",
    "#         extra_params = out_inv_transformed.size - len(transform_inds)\n",
    " \n",
    "#     # Fill in the parameter vector accordingly\n",
    "#     out = np.concatenate([out_inv_transformed, paramvec[inds]])\n",
    "    \n",
    "#     # Make space for the extra parameters\n",
    "#     inds = np.concatenate([inds[inds<np.min(transform_inds)], inds[inds>np.max(transform_inds)]+extra_params])\n",
    "#     if extra_params > 0:\n",
    "#         transform_inds_out = np.concatenate([transform_inds, np.arange(extra_params)+np.max(transform_inds)])\n",
    "#     elif extra_params < 0:\n",
    "#         transform_inds_out = transform_inds[np.arange(len(transform_inds)+extra_params)]\n",
    "#     else:\n",
    "#         transform_inds_out = transform_inds\n",
    "    \n",
    "#     out = out[np.argsort(np.concatenate([transform_inds_out, inds]))]\n",
    "    \n",
    "#     return (out, transform_inds_out)\n",
    "\n",
    "# def do_inv_transforms(paramvec, transforms):\n",
    "#     # Change from \"natural\" parameter space into the \"optimised\" parameter space\n",
    "#     for tf in transforms:\n",
    "#         paramvec = transform_inv(paramvec, tf['type'], tf['inds'])[0]\n",
    "        \n",
    "#     return paramvec\n",
    "\n",
    "# def do_transforms(paramvec, transforms):\n",
    "#     # Change from \"optimised\" parameter space into the \"natural\" parameter space\n",
    "#     # Transforms have to be done in reverse order than the transform_inds were determined during the inverse transforms\n",
    "#     for tf in reversed(transforms):\n",
    "#         paramvec = transform(paramvec, tf['type'], tf['inds'])[0]\n",
    "    \n",
    "#     return paramvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prior types and derivatives and compute contribution\n",
    "# from autograd.extend import primitive, defvjp\n",
    "\n",
    "def add_prior(paramdict, prior_type, prior_metadata={}):\n",
    "    \"\"\"\n",
    "    Compute a the negative LL of the current paramdict, under a prior of given type and metadata,\n",
    "    \"\"\"\n",
    "    if prior_type==\"LogGamma\":\n",
    "        \"\"\"\n",
    "            Gamma distribution over log(x) with parameters [shape, scale, log10loc]\n",
    "        \"\"\"\n",
    "        prior_params = prior_metadata['hyperparams']\n",
    "        out = 0\n",
    "        for key in prior_metadata['names']:\n",
    "            for x in paramdict[key].flatten():\n",
    "                shape = prior_params[0]\n",
    "                scale = prior_params[1]\n",
    "                log10loc = prior_params[2]\n",
    "\n",
    "                # Rescale and shift x\n",
    "                x = (np.log10(x)-log10loc)/scale\n",
    "                \n",
    "                # Compute logpdf of standardised gamma (scale = 1, loc =0)            \n",
    "                cur_out = (shape-1.0) *np.log(x) - x - scipy.special.gammaln(shape)\n",
    "                \n",
    "                # Compute the negative logpdf for the scaled variable\n",
    "                cur_out = -1.*(cur_out - scale)\n",
    "                \n",
    "                # Add to running sum\n",
    "                out = out + cur_out\n",
    "                \n",
    "    elif prior_type==\"Exponential\":\n",
    "        \"\"\"\n",
    "            Exponential distribution over x with parameters [scale]\n",
    "        \"\"\"\n",
    "        prior_params = prior_metadata['hyperparams']\n",
    "        out = 0\n",
    "        for key in prior_metadata['names']:\n",
    "            for x in paramdict[key].flatten():\n",
    "                scale = prior_params[0]\n",
    "\n",
    "                # Compute the negative logpdf for the scaled variable\n",
    "                cur_out = -1.*(-x/scale - scale)\n",
    "                \n",
    "                # Add to running sum\n",
    "                out = out + cur_out\n",
    "        \n",
    "    elif prior_type==\"InducingDPP\":\n",
    "        \"\"\"\n",
    "        Assume x is built from parts: [u,z,Sigma_u, kernelparams] = \"partition\"(x)\n",
    "        Places a prior on the values u, such that if they are \"close\" in location z \n",
    "        (with closeness defined by a kernel in metadata['kernel_func'](z,z | kernelparams) ),\n",
    "        then the values of u should be close as well:\n",
    "        P(z) = DeterminentalPointProcessPrior\n",
    "        \"\"\"\n",
    "        \n",
    "        [u,z,Sigma_u,kernelparams] = prior_metadata['unpack_dict'](paramdict)\n",
    "        Kzz = prior_metadata['kernel_func'](z,z, **kernelparams)\n",
    "        curcovm = Kzz+np.diag(Sigma_u.flatten())\n",
    "        curcovm_inv = np.linalg.inv(curcovm)\n",
    "        ldet = np.linalg.slogdet(curcovm)[1]\n",
    "        out = - ldet*(1.0/np.max([1.0, z.shape[1]]))*prior_metadata['prior_weight']\n",
    "            \n",
    "    \n",
    "    elif prior_type==\"InducingSmooth\":\n",
    "        \"\"\"\n",
    "        Assume x is built from parts: [u,z,Sigma_u, kernelparams] = metadata['unpack_x'](x)\n",
    "        Places a prior on the values u, such that if they are \"close\" in location z \n",
    "        (with closeness defined by a kernel in metadata['kernel_func'](z,z | kernelparams) ),\n",
    "        then the values of u should be close as well:\n",
    "        P(u | z, Sigma_u, kernelparams) = Normal( 0, K_zz + diag(Sigma_u) )\n",
    "        \"\"\"\n",
    "\n",
    "        [u,z,Sigma_u,kernelparams] = prior_metadata['unpack_dict'](paramdict)\n",
    "\n",
    "        Kzz = prior_metadata['kernel_func'](z,z, **kernelparams)\n",
    "\n",
    "        out = 0\n",
    "        try:\n",
    "            curcovm = Kzz+np.diag(Sigma_u.flatten())\n",
    "            curcovm_inv = np.linalg.inv(curcovm)\n",
    "            ldet = np.linalg.slogdet(curcovm)[1]\n",
    "            for d1 in range(u.shape[0]):\n",
    "                curlogpdf = - 1.0/2.0*ldet - 1.0/2.0*np.dot(np.dot(u[d1:(d1+1),:], curcovm_inv), u[d1:(d1+1),:].T)\n",
    "                out = out - curlogpdf*prior_metadata['prior_weight']\n",
    "\n",
    "        except:\n",
    "            set_trace()\n",
    "            \n",
    "    elif prior_type==\"InducingSmooth_and_DPP\":\n",
    "        \"\"\"\n",
    "        Combining smoothness and DPP priors to avoid unnecessary computations\n",
    "        \"\"\"\n",
    "\n",
    "        [u,z,Sigma_u,kernelparams] = prior_metadata['unpack_dict'](paramdict)\n",
    "\n",
    "        Kzz = prior_metadata['kernel_func'](z,z, **kernelparams)\n",
    "\n",
    "        out = 0\n",
    "        try:\n",
    "            curcovm = Kzz+np.diag(Sigma_u.flatten())\n",
    "            curcovm_inv = np.linalg.inv(curcovm)\n",
    "            ldet = np.linalg.slogdet(curcovm)[1]\n",
    "            for d1 in range(u.shape[0]):\n",
    "                curlogpdf = - 1.0/2.0*ldet - 1.0/2.0*np.dot(np.dot(u[d1:(d1+1),:], curcovm_inv), u[d1:(d1+1),:].T)\n",
    "                out = out - curlogpdf*prior_metadata['prior_weight_Smooth']\n",
    "\n",
    "            out = out - ldet*(1.0/np.max([1.0, z.shape[1]]))*prior_metadata['prior_weight_DPP']\n",
    "        except:\n",
    "            set_trace()\n",
    "        \n",
    "    else:\n",
    "        out = 0\n",
    "        \n",
    "    return out\n",
    "            \n",
    "    \n",
    "# # # Examples\n",
    "\n",
    "# # # LogGamma prior\n",
    "# pf = create_prior(\"LogGamma\", [2., 1.5, -6.])\n",
    "# x = np.logspace(-6,2,100)\n",
    "# plt(plt_type.Figure(data=[plt_type.Scatter(x=x, y=np.exp(-pf(x)))], layout=plt_type.Layout(xaxis=dict(type= \"log\"))))\n",
    "# plt(plt_type.Figure(data=[plt_type.Scatter(x=x, y=pf(x))], layout=plt_type.Layout(xaxis=dict(type= \"log\"))))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run optimisation algorithm\n",
    "\n",
    "See McHutchon thesis 3.6 Direct method, Algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a single complete iteration for vectorised parameter collection\n",
    "def time_full_iter(paramvec, y, paramdict_ind, paramdict_shape, \n",
    "                   transforms={}, priors={}, ret_smoothed = False, cutoff = None):\n",
    "    \"\"\"\n",
    "    Run a full iteration filtering forward in time, returning the total negative log likelihood (the objective)\n",
    "    as a function of a vectorised collection of all our parameters, so we can take easy gradient steps\n",
    "    \"\"\"\n",
    "    \n",
    "    # The objective is to minimise the negative log likelihood\n",
    "    neg_log_likelihood = 0\n",
    "    \n",
    "    Dy = y.shape[0]\n",
    "    T = y.shape[1]\n",
    "    Ny = y.shape[2]\n",
    "        \n",
    "    paramvec_orig = paramvec\n",
    "    \n",
    "    # Get the original representation of parameters\n",
    "    paramdict = vec_to_params(paramvec, paramdict_ind, paramdict_shape, transforms)\n",
    "    \n",
    "    # Compute contributions of the priors if they exist\n",
    "    nprior_contrib = 0\n",
    "    for prior in priors:\n",
    "        nprior_contrib = nprior_contrib + add_prior(paramdict, prior['type'], prior['metadata'])\n",
    "    \n",
    "    # Unpack the usual parameters\n",
    "    (Sigma_eps, mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_u, lengthscales, kernel_variance, s, J)  = \\\n",
    "        list(paramdict.values())[:12]\n",
    "    \n",
    "    if np.any(np.isnan(lengthscales)):\n",
    "        set_trace()\n",
    "    \n",
    "    # Deal with the extra possible parameters\n",
    "    Sigma_s = None; Sigma_J=None;\n",
    "    if 'Sigma_s' in paramdict.keys():\n",
    "        Sigma_s = paramdict['Sigma_s']\n",
    "    if 'Sigma_J' in paramdict.keys():\n",
    "        Sigma_J = paramdict['Sigma_J']\n",
    "        \n",
    "    D = mu_0_0.shape[0]\n",
    "    \n",
    "    # Collect smoothed latent trajectories\n",
    "    x_all_t1 = np.zeros((D, T, Ny))\n",
    "    x_all_t = np.zeros((D, T, Ny))\n",
    "    sig_all_t1 = np.zeros((D*D, T, Ny))\n",
    "    sig_all_t = np.zeros((D, T, Ny))\n",
    "    neg_log_likelihood_all = np.zeros((T, Ny))\n",
    "    \n",
    "\n",
    "    \n",
    "    L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=lengthscales, z=z, u=u, s=s, J=J, \n",
    "                                         sig_eps=Sigma_eps, sig_u = Sigma_u, sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "\n",
    "    for n in range(Ny):\n",
    "        \n",
    "        mu_t1_t1 = mu_0_0\n",
    "        Sigma_t1_t1 = Sigma_0_0\n",
    "        \n",
    "        for t in range(T):\n",
    "            mu_t_t1, Sigma_t_t1 = update_t_t1(mu_t1_t1, Sigma_t1_t1, L, targets, kernel_variance, \n",
    "                                              Sigma_eps, z, u, lengthscales, s, J)\n",
    "\n",
    "#             # Check \"condition number\" and add diag term to correct if needed\n",
    "#             if (np.min(np.diag(Sigma_t_t1))<1e-6):\n",
    "#                 Sigma_t_t1 = Sigma_t_t1 + 1e-6*np.eye(Sigma_t_t1.shape[0])\n",
    "            \n",
    "            if ret_smoothed:\n",
    "                x_all_t1[:,t,n] = mu_t_t1.flatten()            \n",
    "                sig_all_t1[:,t,n] = np.reshape(Sigma_t_t1,(1,-1))\n",
    "            \n",
    "            # cur_nll_term = nlog_marg_ll(mu_t_t1, Sigma_t_t1, C, Sigma_nu, y[:,t:(t+1),n])\n",
    "            \n",
    "\n",
    "            cur_nll_term, mu_t1_t1, Sigma_t1_t1 = update_t_t(mu_t_t1, Sigma_t_t1, C, Sigma_nu, y[:,t:(t+1),n])\n",
    "            neg_log_likelihood = neg_log_likelihood + cur_nll_term\n",
    "                        \n",
    "            Sigma_t1_t1 = np.diag(Sigma_t1_t1)[:,None]\n",
    "            \n",
    "            if cutoff is not None:\n",
    "                if t>cutoff:\n",
    "                    # Integrate the first Tau data points, but then no more data available (ignore mu_t1_t1 and Sigma_t1_t1)\n",
    "                    mu_t1_t1 = mu_t_t1\n",
    "                    Sigma_t1_t1 = np.diag(Sigma_t_t1)[:,None]\n",
    "            \n",
    "            # Check \"condition number\" and add diag term to correct if needed            \n",
    "            if (np.min(Sigma_t1_t1)<1e-6):\n",
    "                #set_trace()\n",
    "                # Sigma_t1_t1 = Sigma_nu # This doesn't work where Dx != Dy\n",
    "                Sigma_t1_t1 = Sigma_t1_t1 + 1e-6\n",
    "                # print(\"Warning, Sigma_t1_t1 is not pos def, resetting it\")\n",
    "                \n",
    "            \n",
    "            \n",
    "            if ret_smoothed:\n",
    "                x_all_t[:,t,n] = mu_t1_t1.flatten()\n",
    "                sig_all_t[:,t,n] = np.diag(Sigma_t1_t1).flatten()\n",
    "                neg_log_likelihood_all[t,n] = cur_nll_term\n",
    "\n",
    "            #print(np.concatenate([y[:,t:(t+1),0], mu_t_t1, mu_t1_t1], axis=1))\n",
    "            \n",
    "    return (neg_log_likelihood + nprior_contrib, \n",
    "            x_all_t1, x_all_t, sig_all_t1, \n",
    "            sig_all_t, neg_log_likelihood_all, nprior_contrib)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_iter(paramvec, y, *args, **kwargs):\n",
    "    if \"minibatch_func\" in kwargs:\n",
    "        minibatch_func = kwargs.pop(\"minibatch_func\")\n",
    "    elif \"minibatch_size\" in kwargs:\n",
    "        minibatch_size = kwargs.pop(\"minibatch_size\")\n",
    "        def minibatch_func(data):\n",
    "            return data[:,:,random.sample(range(data.shape[2]),minibatch_size)]\n",
    "    else:\n",
    "        def minibatch_func(data):\n",
    "            return data\n",
    "    \n",
    "    return time_full_iter(paramvec, minibatch_func(y), *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the algorithm\n",
    "\n",
    "# Dy = 2\n",
    "# T = 7\n",
    "# Ny = 20\n",
    "# D = Dy\n",
    "# Nz = 2\n",
    "# Ns = 2\n",
    "\n",
    "# np.random.seed(123)\n",
    "# y = 0.3*np.random.randn(Dy,T,Ny)+0.5\n",
    "\n",
    "# (mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_eps, lengthscales, kernel_variance, s, J) = \\\n",
    "#     init_params(y, D, Nz, Ns)\n",
    "\n",
    "\n",
    "# paramvec = params_to_vec(mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_eps, lengthscales, kernel_variance, s, J)\n",
    "    \n",
    "# # Optimise only certain elements of paramvec\n",
    "# cur_pvec = paramvec[0:2]\n",
    "# objective_with_grad = value_and_grad(lambda pvec: \n",
    "#                                          time_full_iter(np.concatenate([pvec, paramvec[2:]]), \n",
    "#                                                         y, D, Nz, Ns), argnum=0)\n",
    "\n",
    "\n",
    "# result = scipy.optimize.minimize(objective_with_grad, cur_pvec, jac=True, method='CG',\n",
    "#                       options={'maxiter':3, 'disp':True})\n",
    "\n",
    "# # \n",
    "# # for it in range(1):\n",
    "# #     a = objective_with_grad(cur_pvec)\n",
    "# #     b = elem_grad(cur_pvec)\n",
    "    \n",
    "# #     print np.concatenate([a[1], b], axis=1)\n",
    "    \n",
    "# #     print a[0]\n",
    "# #     cur_pvec = cur_pvec - 1e-5*a[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Optimizers\n",
    "\n",
    "1. ADAM optimizer (Kingma 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "def adamOptimize(fval, fgrad, theta, options={},\n",
    "                 callback_func=None, callback_params=[]):\n",
    "    \n",
    "    default_options = {\n",
    "        'alpha':1e-3,\n",
    "        'beta1':0.9,\n",
    "        'beta2':0.999,\n",
    "        'epsilon':1e-8,\n",
    "        'maxiter':1000,\n",
    "        'ma_iters':20,\n",
    "        'obj_eval_iters':1 # Evaluate the objective every this many iterations\n",
    "    }\n",
    "    \n",
    "    for key in default_options:\n",
    "        if key not in options:\n",
    "            options[key] = default_options[key]\n",
    "    \n",
    "    alpha=options['alpha']\n",
    "    beta1=options['beta1']\n",
    "    beta2=options['beta2']\n",
    "    epsilon=options['epsilon']\n",
    "    maxiter=options['maxiter']\n",
    "    \n",
    "    t = 0\n",
    "    \n",
    "    # Create same optimizeResult object as scipy\n",
    "    result = scipy.optimize.OptimizeResult()\n",
    "    \n",
    "    # Store information for subsequent diagnostics\n",
    "    init_time = time.time()\n",
    "    all_t = []\n",
    "    all_theta = []\n",
    "    all_times = []\n",
    "    all_objs = [] # Iter and obj value\n",
    "    \n",
    "    # Compute the function to get the initial objective and gradient\n",
    "    converged = False\n",
    "    message = \"Not converged\"\n",
    "    \n",
    "    # Store a moving average of the objective as a stopping condition\n",
    "    obj_ma = list((np.inf,)*options['ma_iters'])\n",
    "    obj_new = np.inf\n",
    "    \n",
    "    while (not converged) and (t < options['maxiter']):\n",
    "        \n",
    "        all_t.append(t)\n",
    "        all_theta.append(theta)\n",
    "        all_times.append(time.time()-init_time)\n",
    "        \n",
    "        \n",
    "        if not np.mod(t, options['obj_eval_iters']):\n",
    "            obj_new = fval(theta) # This may be on full data\n",
    "        gr = fgrad(theta) # This may be on minibatch\n",
    "        if t == 0:\n",
    "            m = np.zeros(*gr.shape)\n",
    "            v = np.zeros(*gr.shape)            \n",
    "            obj_orig = obj_new # This may be on full data\n",
    "            obj = obj_orig\n",
    "            \n",
    "        t = t+1\n",
    "        \n",
    "        m = beta1*m + (1-beta1)*gr\n",
    "        v = beta2*v + (1-beta2)*(gr**2)\n",
    "        \n",
    "        mhat = m/(1-beta1**t)\n",
    "        vhat = v/(1-beta2**t)\n",
    "        theta_new = theta - alpha*mhat/(np.sqrt(vhat)+epsilon)\n",
    "        \n",
    "        # Check for convergence in theta\n",
    "        if (np.sqrt(np.sum((theta_new-theta)**2))<1e-6*np.sqrt(np.sum(theta**2))):\n",
    "            converged = True\n",
    "            message = \"Theta converged\"\n",
    "            \n",
    "        # Objective convergence criterion (based on moving average of true objective)\n",
    "        if np.mean(np.array(obj_ma))<obj_new:\n",
    "            converged = True\n",
    "            message = \"Objective converged\"\n",
    "        else:        \n",
    "            theta = theta_new\n",
    "            obj = obj_new\n",
    "            obj_ma.pop(0)\n",
    "            obj_ma.append(obj_new)\n",
    "            all_objs.append(np.array([t-1, obj])) # Iter and obj value\n",
    "        \n",
    "        if (t < 11) or (not np.mod(t-1,10)):\n",
    "#             obj = fval(theta)\n",
    "#             all_objs.append(np.array([t-1, obj])) # Iter and obj value\n",
    "            print_str = \"Iter %3d     Objective: %6f      |grad| %4f\\n\" % (t-1, obj, np.sqrt(np.sum((mhat/(np.sqrt(vhat)+epsilon))**2)))\n",
    "            os.write(1, print_str.encode())\n",
    "                \n",
    "        result.x = theta\n",
    "        result.fun = obj\n",
    "        result.converged = converged\n",
    "        result.iters = all_t\n",
    "        result.objs = all_objs\n",
    "        result.theta_hist = all_theta\n",
    "        result.times = all_times\n",
    "        result.options = options\n",
    "        result.message = message\n",
    "    \n",
    "        if callback_func is not None:\n",
    "            callback_func(result, *callback_params)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return result\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics\n",
    "\n",
    "This section implements the following evaluation metrics for a fit on training data, given held out test data\n",
    "\n",
    "1. One time point ahead prediction - RMSE and likelihood of observed data under GP posterior\n",
    "2. Full trace prediction - (weighted) RMSE and likelihood of observed data under GP posterior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_lin_AR1(y_test, y, cutoff=None):\n",
    "    # Perform linear AR(1) on y as baseline\n",
    "    Dy = y_test.shape[0]\n",
    "    y_t = y[:,1:,:]\n",
    "    y_t1 = y[:,:-1,:]\n",
    "\n",
    "    y_t_rsh = np.reshape(y_t, (y.shape[0], -1)).T\n",
    "    y_t1_rsh = np.reshape(y_t1, (y.shape[0], -1)).T\n",
    "\n",
    "    # Get least squares linear regression weights from original dataset\n",
    "    weights_AR = np.dot(np.dot(np.linalg.inv(np.dot(y_t1_rsh.T, y_t1_rsh)), y_t1_rsh.T), y_t_rsh)\n",
    "\n",
    "    # Get optimal linear estimate of y_tt given the y_t-1 vector\n",
    "    if cutoff is None:\n",
    "        y_test_t1_rsh = np.reshape(y_test[:,:-1,:], (y.shape[0], -1)).T\n",
    "        y_test_t_hat = np.dot(y_test_t1_rsh, weights_AR)\n",
    "        y_test_t_hat = np.reshape(y_test_t_hat.T, (y_test.shape[0], y_test.shape[1]-1, y_test.shape[2]))\n",
    "    else:\n",
    "        # Until cutoff do the same\n",
    "        y_test_t1_rsh = y_test[:,0:min(cutoff+1,y_test.shape[1]-1),:]\n",
    "        y_test_t_hat = np.einsum('ed,dtn->etn', weights_AR, y_test_t1_rsh)\n",
    "        # From cutoff do successive one step ahead predicitons\n",
    "        for t in range(cutoff+1, y_test.shape[1]-1):\n",
    "            y_test_t_hat = np.concatenate(\n",
    "                [y_test_t_hat, \n",
    "                 np.einsum('ed,dtn->etn', weights_AR, y_test_t_hat[:,-1:,:])],\n",
    "                axis = 1)\n",
    "            \n",
    "    \n",
    "    # Set first time point prediction as the mean of observed first time points\n",
    "    y_test_t_hat0 = np.tile(np.mean(y[:,0:1,:],axis=2),[1,1,y_test.shape[2]])\n",
    "    \n",
    "    # Concatenate\n",
    "    y_test_t_hat = np.concatenate([y_test_t_hat0, y_test_t_hat], axis=1)\n",
    "\n",
    "    return y_test_t_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_paramvec = log_transform_inv(replace_params(all_results[-1].x, opt_params, init_paramvec), log_transformed)\n",
    "\n",
    "def pred_GP(y_test, final_paramvec, transforms, dict_ind, dict_shape, priors={}, cutoff = None):\n",
    "    \"\"\"\n",
    "    Returns a list, with elements:\n",
    "    0 - Objective function value (nll + nlogprior) [scalar]\n",
    "    1 - Latent mean at t, given data up to t-1 [DxTxN]\n",
    "    2 - Latent mean at t, given data up to t [DxTxN]\n",
    "    3 - Latent covariance at t, given data up to t-1 (square matrix) [(D*D)xTxN]\n",
    "    4 - Latent covariance at t, given data up to t (vector of the diagonal) [DxTxN]\n",
    "    5 - Negative log likelihood of each data point [TxN]\n",
    "    6 - The negative log likelihood of the paramaters under the priors [scalar]\n",
    "    7 - The mean of the prediction in observed data space [DyxTxN]\n",
    "    \"\"\"\n",
    "    #     time_full_iter returns these:\n",
    "    #     (neg_log_likelihood + nprior_contrib, \n",
    "    #             x_all_t1, x_all_t, sig_all_t1, \n",
    "    #             sig_all_t, neg_log_likelihood_all, nprior_contrib)\n",
    "    \n",
    "    results_GP = time_full_iter(final_paramvec, \n",
    "                          y_test, dict_ind, dict_shape, transforms=transforms, priors=priors,\n",
    "                          ret_smoothed=True, cutoff = cutoff)\n",
    "    \n",
    "    # Get the y mean predictions via C' * x_t1\n",
    "    y_test_pred = np.einsum('ed,dtn->etn', np.reshape(final_paramvec[dict_ind['C']], dict_shape['C']), results_GP[1])\n",
    "    \n",
    "    results_GP = list(results_GP)\n",
    "    results_GP.append(y_test_pred)\n",
    "    \n",
    "    return results_GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets, axis=None):\n",
    "    return np.sqrt(np.mean(((predictions - targets) ** 2), axis=axis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training data in 2D, /w derivative observations\n",
    "# # z = np.array([[-2, 0, 2], [1, 0, 1]])\n",
    "# # u = np.array([[-1, 1, -3], [-1, 1, -3]])\n",
    "# z = np.array([[-3.0], [-3.0]])\n",
    "# u = np.array([[2.0], [1.0]])\n",
    "# s = np.array([[-1.0, 2.0], \n",
    "#               [-1.0, 3.0]])\n",
    "# # Set the derivates to be N fixed point x DxD Jacobians\n",
    "# J = np.concatenate(\n",
    "#         [np.array([[[-1, 0], \n",
    "#                    [0, -1]]]),\n",
    "#          np.array([[[2, -1], \n",
    "#                    [0, 0]]])\n",
    "#          ],\n",
    "#     axis = 0)*1.0\n",
    "\n",
    "# which_fixed_point = 1\n",
    "# xtmp, ytmp = np.meshgrid(np.concatenate([np.arange(-5,5,0.5), np.arange(s[0,which_fixed_point]-0.1,s[0,which_fixed_point]+0.1,0.01)]),\n",
    "#                          np.concatenate([np.arange(-5,5,0.5), np.arange(s[1,which_fixed_point]-0.1,s[1,which_fixed_point]+0.1,0.01)]))\n",
    "# # xtmp, ytmp = np.meshgrid(np.concatenate([np.arange(s[0,which_fixed_point]-0.1,s[0,which_fixed_point]+0.1,0.01)]),\n",
    "# #                          np.concatenate([np.arange(s[1,which_fixed_point]-0.1,s[1,which_fixed_point]+0.1,0.01)]))\n",
    "# # xtmp, ytmp = np.meshgrid(np.arange(1.9,2.1,0.005),\n",
    "# #                          np.arange(2.9,3.1,0.005))\n",
    "\n",
    "# xstar = np.concatenate([xtmp.flatten()[:,None], ytmp.flatten()[:,None]], axis=1).T\n",
    "\n",
    "\n",
    "# # # Training data in 1D, /w derivative observations\n",
    "# # z = np.array([[1.0]])\n",
    "# # u = np.array([[1.0]])\n",
    "\n",
    "# # s = np.array([[0.0, 2.0]])\n",
    "# # J = np.array([[[-0.5, 0.5]]])\n",
    "\n",
    "# # xstar = np.array([np.concatenate([np.arange(-5,5,0.5), \n",
    "# #                                   np.arange(s[0,0]-0.1,s[0,0]+0.1,0.01), \n",
    "# #                                   np.arange(s[0,1]-0.1,s[0,1]+0.1,0.01)])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the GP functions\n",
    "# fp_get_static_K, fp_predict = create_fp_gp_funcs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute the params\n",
    "# L, targets, params = fp_get_static_K(eta=1.0, lengthscales=1.5*np.ones((z.shape[0], 1)), z=z, u=u, s=s, J=J, sig_eps=1e-1)\n",
    "# #For debug\n",
    "# #K_full = fp_get_static_K(eta=1.0, lengthscales=None, z=z, u=u, s=s, J=J, sig_eps=1e-1)\n",
    "        \n",
    "# mu_star, sig_star, K_pred = fp_predict(xstar, L, targets, params)\n",
    "# #mu_star, sig_star, K_pred = fp_predict(np.array([[0,1]]), L, targets, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt([plt_type.Scatter(x=np.squeeze(xstar), y=np.squeeze(mu_star), mode='markers'),\n",
    "#      plt_type.Scatter(x=np.squeeze(s), y=np.squeeze(s), mode='markers') \\\n",
    "#     ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt([plt_type.Scatter3d(x=np.squeeze(xstar[0,:]), \n",
    "#                         y=np.squeeze(xstar[1,:]), \n",
    "#                         z=np.squeeze(mu_star[:,1]), mode='markers')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2D input\n",
    "\n",
    "# [mu_t_t1, Sigma_t_t1] = update_t_t1(np.array([[1.0], [[0.5]]]),np.array([[1e-2], [1e-2]]), \n",
    "#             L, targets, \n",
    "#             params[\"eta\"], params[\"z\"], params[\"u\"], params[\"lengthscales\"],\n",
    "#             params[\"s\"], params[\"J\"])\n",
    "\n",
    "# print([mu_t_t1, Sigma_t_t1])\n",
    "\n",
    "# [mu_star, sig_star, Kx_pred] = fp_predict(np.array([[1.0], [[0.5]]]), L, targets, params)\n",
    "\n",
    "# print([mu_star, sig_star])\n",
    "\n",
    "# # # 1 D input\n",
    "\n",
    "# # [mu_t_t1, Sigma_t_t1] = update_t_t1(np.array([[-2.0]]),np.array([[1e-2]]), \n",
    "# #             L, targets, \n",
    "# #             [], params[\"eta\"], params[\"z\"], params[\"u\"], params[\"sig_eps\"], params[\"lengthscales\"],\n",
    "# #             params[\"s\"], params[\"J\"])\n",
    "\n",
    "# # print([mu_t_t1, Sigma_t_t1])\n",
    "\n",
    "# # [mu_star, sig_star, Kx_pred] = fp_predict(np.array([[-2.0]]), L, targets, params)\n",
    "\n",
    "# # print([mu_star, sig_star])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate example data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D,  1 well example\n",
    "\n",
    "1 D transition function, static plus noise, except for pull towards +1.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def draw_trial(well_loc, T, Sigma_eps, Sigma_nu, mu_0_0, Sigma_0_0, well_width):\n",
    "#     x0 = mu_0_0 + np.sqrt(Sigma_0_0)*np.random.randn(1)\n",
    "#     x = np.zeros((1,T))\n",
    "#     y = np.zeros((1,T))\n",
    "#     for t in range(T):\n",
    "#         if t==0:\n",
    "#             xprev = x0\n",
    "#         else:\n",
    "#             xprev = x[:,t-1]\n",
    "\n",
    "#         if ((xprev<(well_loc - np.sqrt(Sigma_eps)*well_width)) or (xprev>(well_loc + np.sqrt(Sigma_eps)*well_width))):\n",
    "#             x[:,t] = xprev + np.sqrt(Sigma_eps)*np.random.randn(1)\n",
    "#         else:\n",
    "#             x[:,t] = well_loc + np.sqrt(Sigma_eps)*np.random.randn(1)\n",
    "        \n",
    "#         y[:,t] = x[:,t] + np.sqrt(Sigma_nu)*np.random.randn(1)\n",
    "        \n",
    "#     return (x,y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ny = 13\n",
    "# well_loc = 1.41\n",
    "# T = 10\n",
    "# Sigma_eps = 1e-1\n",
    "# Sigma_nu = 1e-1\n",
    "# mu_0_0 = 0.7\n",
    "# Sigma_0_0 = 1e-1\n",
    "# well_width = 3\n",
    "\n",
    "# # Collect trials into runnable object\n",
    "# all_trials_x = []\n",
    "# all_trials_y = []\n",
    "# for n in range(Ny):\n",
    "#     x,y = draw_trial(well_loc, T, Sigma_eps, Sigma_nu, mu_0_0, Sigma_0_0, well_width)\n",
    "    \n",
    "#     all_trials_x.append(x[:,:,None])\n",
    "#     all_trials_y.append(y[:,:,None])\n",
    "\n",
    "\n",
    "# x = np.concatenate(all_trials_x, axis=2)\n",
    "# y = np.concatenate(all_trials_y, axis=2)\n",
    "\n",
    "# plots_by_run = []\n",
    "# for v in range(Ny):\n",
    "#     plots_by_run.append(\n",
    "#         plt_type.Scatter(x=np.squeeze(np.arange(T)), \n",
    "#                       y=np.squeeze(x[:,:,v]), \n",
    "#                       mode='lines')\n",
    "#     )\n",
    "    \n",
    "# plt(plots_by_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a plotting function for callback that shows the current transition function estimate\n",
    "# def callback_plot(pvec):\n",
    "#     (mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_eps, lengthscales, kernel_variance, s, J) = \\\n",
    "#         vec_to_params(pvec, y.shape[0], D, Nz, Ns)\n",
    "\n",
    "#     # Plot transition function\n",
    "#     xstar = np.arange(-2.0,4.0,0.05)\n",
    "\n",
    "#     fp_get_static_K, fp_predict = create_fp_gp_funcs()\n",
    "#     L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=lengthscales, z=z, u=u, s=s, J=J, sig_eps=Sigma_eps)\n",
    "#     mu_star, sig_star, K_pred = fp_predict(xstar, L, targets, params)\n",
    "\n",
    "#     plt([plt_type.Scatter(x=np.squeeze(xstar), y=np.squeeze(mu_star), mode='markers'),\n",
    "#          plt_type.Scatter(x=np.squeeze(z), y=np.squeeze(-2.0*np.ones_like(z)), mode='markers') \\\n",
    "#         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try to solve the full problem\n",
    "# D = 1\n",
    "# Nz = 8\n",
    "# Ns = 1\n",
    "\n",
    "# (mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_eps, lengthscales, kernel_variance, s, J) = \\\n",
    "#     init_params(y, D, Nz, Ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (init_paramvec, dict_ind, dict_shape) = params_to_vec(mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_eps, lengthscales, kernel_variance, s, J)\n",
    "    \n",
    "# # # Optimise only certain elements of paramvec\n",
    "# #opt_params = np.concatenate([dict_ind['u'], dict_ind['s']])\n",
    "# #cur_pvec = paramvec[opt_params] + np.random.randn(*(paramvec[opt_params].shape))\n",
    "\n",
    "# tmp_func = lambda pvec: (time_full_iter(pvec, y, D, Nz, Ns)[0])\n",
    "# objective_with_grad = value_and_grad(tmp_func, argnum=0)\n",
    "\n",
    "# cur_pvec = init_paramvec\n",
    "\n",
    "# # Add bounds to ensure positive variances\n",
    "# bnds = list(((None, None),) * paramvec.shape[0])\n",
    "# for i in np.concatenate([dict_ind['Sigma_0_0'], dict_ind['Sigma_nu'], dict_ind['Sigma_eps']]):\n",
    "#     bnds[i] = (1e-6, None)\n",
    "# bnds = tuple(bnds)\n",
    "    \n",
    "# result = scipy.optimize.minimize(objective_with_grad, cur_pvec, jac=True, method='L-BFGS-B', bounds=bnds, callback=callback_plot,\n",
    "#                       options={'maxiter':50, 'disp':True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot likelihood wrt to fix point location only (other true parameters are given)\n",
    "# D = 1\n",
    "# Nz = 10\n",
    "# Ns = 1\n",
    "\n",
    "# # (mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_eps, lengthscales, kernel_variance, s, J) = \\\n",
    "# #     init_params(y, D, Nz, Ns)\n",
    "  \n",
    "\n",
    "# mu_0_0 = np.array([[mu_0_0]])\n",
    "# Sigma_0_0 = np.array([[Sigma_0_0]])\n",
    "# C = np.array([[1]])\n",
    "# Sigma_nu = np.array([[Sigma_nu]])\n",
    "# z = np.concatenate(\n",
    "#      [np.linspace(np.min(y),0.3,10)[:,None].T,\n",
    "#       np.linspace(2.2,np.max(y),10)[:,None].T],\n",
    "#     axis=1)\n",
    "# u = z\n",
    "# #Sigma_eps = np.array([[1e-10]])\n",
    "# Sigma_eps = np.array([[Sigma_eps]])\n",
    "# lengthscales = 0.6*np.ones((1,1))\n",
    "# kernel_variance = 1.0*np.ones((1,1))\n",
    "# J = np.zeros((1,1,1))\n",
    "# s = np.array([[well_loc]])\n",
    "\n",
    "# Nz = z.shape[1]\n",
    "      \n",
    "# # est_locs = np.linspace(0.8, 1.7, 10)\n",
    "# # #est_locs = [1.32]\n",
    "# # results = []\n",
    "# # for est_well_loc in est_locs:\n",
    "# #     s = np.array([[est_well_loc]])\n",
    "# #     paramvec = params_to_vec(mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_eps, lengthscales, kernel_variance, s, J)\n",
    "    \n",
    "# #     a = np.asscalar(time_full_iter(paramvec, y, D, Nz, Ns)[0])\n",
    "# #     results.append(a)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well_loc_plot = [plt_type.Scatter(\n",
    "#                       x=np.squeeze(est_locs), \n",
    "#                       y=np.squeeze(np.array(results)), \n",
    "#                       mode='markers')]\n",
    "\n",
    "# plt(well_loc_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = np.array([[well_loc]])\n",
    "\n",
    "# mu_0_0 = np.array([[-0.2]]) # Give wrong initial param\n",
    "# paramvec = params_to_vec(mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_eps, lengthscales, kernel_variance, s, J)[0]\n",
    "\n",
    "# # Try to run proper optimisation on mu_0_0\n",
    "# cur_pvec = paramvec[0:1]\n",
    "# objective_with_grad = value_and_grad(lambda pvec: \n",
    "#                                          time_full_iter(np.concatenate([pvec, paramvec[1:]]), \n",
    "#                                                         y, D, Nz, Ns)[0], argnum=0)\n",
    "\n",
    "\n",
    "# # # Test at various locations\n",
    "# # est_mus = np.linspace(-0.3, 1.3, 40)\n",
    "# # results = []\n",
    "# # for est_mu in est_mus:\n",
    "# #     mu_0_0 = np.array([[est_mu]])\n",
    "# #     paramvec = params_to_vec(mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_eps, lengthscales, kernel_variance, s, J)[0]\n",
    "    \n",
    "# #     a = np.asscalar(time_full_iter(paramvec, y, D, Nz, Ns)[0])\n",
    "# #     results.append(a)\n",
    "\n",
    "\n",
    "# # Optimise via manual gradient descent\n",
    "# for it in range(10):\n",
    "#     a = objective_with_grad(cur_pvec)\n",
    "    \n",
    "#     print [a, cur_pvec]\n",
    "#     cur_pvec = (cur_pvec \n",
    "#                 - 1e-3*a[1]\n",
    "#                 )\n",
    "\n",
    "\n",
    "# # # Optimise via fancy gradient descent\n",
    "# # result = scipy.optimize.minimize(objective_with_grad, cur_pvec, jac=True, method='CG',\n",
    "# #                       options={'maxiter': 2, 'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well_loc_plot = [plt_type.Scatter(\n",
    "#                       x=np.squeeze(est_mus), \n",
    "#                       y=np.squeeze(np.array(results)), \n",
    "#                       mode='markers')]\n",
    "\n",
    "# plt(well_loc_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot smoothed trajectories\n",
    "# obj, x_t1, x_t, sig_t1, sig_t = time_full_iter(paramvec, y, D, Nz, Ns, ret_smoothed=True)\n",
    "\n",
    "\n",
    "# plots_by_run = []\n",
    "# for v in [1]:\n",
    "#     plots_by_run.append(\n",
    "#         plt_type.Scatter(x=np.squeeze(np.arange(T)), \n",
    "#                       y=np.squeeze(y[:,:,v]), \n",
    "#                       mode='lines')\n",
    "#     )\n",
    "#     plots_by_run.append(\n",
    "#         plt_type.Scatter(x=np.squeeze(np.arange(T)), \n",
    "#                       y=np.squeeze(x[:,:,v]), \n",
    "#                       mode='lines')\n",
    "#     )\n",
    "#     plots_by_run.append(\n",
    "#         plt_type.Scatter(x=np.squeeze(np.arange(T)), \n",
    "#                       y=np.squeeze(x_t[:,:,v]), \n",
    "#                       mode='lines')\n",
    "#     )\n",
    "    \n",
    "#     plots_by_run.append(\n",
    "#         plt_type.Scatter(x=np.squeeze(np.arange(T)), \n",
    "#                       y=np.squeeze(x_t1[:,:,v]), \n",
    "#                       mode='lines')\n",
    "#     )\n",
    "    \n",
    "# plt(plots_by_run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_eps, lengthscales, kernel_variance, s, J) = \\\n",
    "#     vec_to_params(result.x, y.shape[0], D, Nz, Ns)\n",
    "\n",
    "# # Plot transition function\n",
    "# xstar = np.arange(-1.0,3.0,0.01)\n",
    "\n",
    "# fp_get_static_K, fp_predict = create_fp_gp_funcs()\n",
    "# L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=lengthscales, z=z, u=u, s=s, J=J, sig_eps=Sigma_eps)\n",
    "# mu_star, sig_star, K_pred = fp_predict(xstar, L, targets, params)\n",
    "\n",
    "# plt([plt_type.Scatter(x=np.squeeze(xstar), y=np.squeeze(mu_star), mode='markers') #,\n",
    "#      #plt_type.Scatter(x=np.squeeze(s), y=np.squeeze(s), mode='markers') \\\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis_env]",
   "language": "python",
   "name": "conda-env-thesis_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
